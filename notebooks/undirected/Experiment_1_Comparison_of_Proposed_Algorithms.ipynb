{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import eigs\n",
    "import logging\n",
    "\n",
    "from anonymigraph.anonymization.method_private_colors import (\n",
    "    SamplingFreeEvaluator,\n",
    "    LocalSearchColorOptimizer,\n",
    "    Optimal1dColorOptimizer,\n",
    "    RandomColorSampler,\n",
    ")\n",
    "\n",
    "from anonymigraph.anonymization.method_private_colors_soft_assignment import SoftColorOptimizer\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "logging.getLogger('randcolorgraphs').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49975\n"
     ]
    }
   ],
   "source": [
    "print(nx.barabasi_albert_graph(10000, 5).number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 07:37:53,620 - INFO - Evaluating with parameters: w=100, rel_alpha=0.9, G_name=Erdos-Renyi, G=Graph with 200 nodes and 1025 edges, seed=42\n",
      "2024-12-22 07:38:24,594 - INFO - Optimal 1d: k=1,  total_loss=8128.1646, loss_U=1666.4085, loss_P=64.6176\n",
      "2024-12-22 07:38:24,600 - INFO - Optimal 1d: k=2,  total_loss=8226.6103, loss_U=621.0867, loss_P=76.0552\n",
      "2024-12-22 07:38:24,603 - INFO - Optimal 1d: k_best=1,  total_loss=8128.1646, loss_U=1666.4085, loss_P=64.6176\n",
      "2024-12-22 07:38:24,637 - INFO - Optimal 1D         : num_clusters=1, sub_opt_gap=1666.4, total_loss=8128.2, loss_U=1666.4, loss_P=64.618 \n",
      "2024-12-22 07:38:24,685 - INFO - Eager Local Search : num_clusters=2, sub_opt_gap=1251.7, total_loss=7713.4, loss_U=824.89, loss_P=68.885 \n",
      "2024-12-22 07:38:24,753 - INFO - Greedy Local Search: num_clusters=2, sub_opt_gap=1255.7, total_loss=7717.5, loss_U=782.75, loss_P=69.347 \n",
      "C:\\Users\\jmeis\\Desktop\\Data\\Professional\\master\\AnonymiGraph\\src\\anonymigraph\\anonymization\\method_private_colors_soft_assignment.py:111: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\SparseCsrTensorImpl.cpp:55.)\n",
      "  self.A_pytorch = torch.sparse_csr_tensor(crow_indices, col_indices, values, size=A_scipy.shape)\n",
      "2024-12-22 07:38:27,389 - INFO - Epoch 0, Total Loss: 9021.9512, Loss U: 1660.0713, Loss P: 73.6188, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:27,438 - INFO - Epoch 10, Total Loss: 8196.3115, Loss U: 1645.6035, Loss P: 65.5071, Entropy: 751.7524, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:27,491 - INFO - Epoch 20, Total Loss: 8080.4834, Loss U: 1530.6992, Loss P: 65.4978, Entropy: 745.9058, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:27,547 - INFO - Epoch 30, Total Loss: 7881.6948, Loss U: 1249.6191, Loss P: 66.3208, Entropy: 715.7130, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:27,607 - INFO - Epoch 40, Total Loss: 7736.1450, Loss U: 1032.1641, Loss P: 67.0398, Entropy: 680.3648, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:27,658 - INFO - Epoch 50, Total Loss: 7670.5283, Loss U: 933.2207, Loss P: 67.3731, Entropy: 657.0978, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:27,715 - INFO - Epoch 60, Total Loss: 7641.6313, Loss U: 889.3594, Loss P: 67.5227, Entropy: 644.3770, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:27,768 - INFO - Epoch 70, Total Loss: 7626.7773, Loss U: 867.4512, Loss P: 67.5933, Entropy: 637.0746, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:27,831 - INFO - Epoch 80, Total Loss: 7618.4209, Loss U: 853.4473, Loss P: 67.6497, Entropy: 631.3766, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:27,898 - INFO - Epoch 90, Total Loss: 7613.0674, Loss U: 844.7637, Loss P: 67.6830, Entropy: 626.3468, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:27,961 - INFO - Epoch 100, Total Loss: 7609.4331, Loss U: 838.1738, Loss P: 67.7126, Entropy: 621.6508, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,026 - INFO - Epoch 110, Total Loss: 7606.8149, Loss U: 833.6172, Loss P: 67.7320, Entropy: 617.1176, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,078 - INFO - Epoch 120, Total Loss: 7604.8306, Loss U: 829.7871, Loss P: 67.7504, Entropy: 612.6129, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,142 - INFO - Epoch 130, Total Loss: 7603.3330, Loss U: 827.2832, Loss P: 67.7605, Entropy: 607.8737, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,209 - INFO - Epoch 140, Total Loss: 7602.1724, Loss U: 824.8438, Loss P: 67.7733, Entropy: 602.9832, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,280 - INFO - Epoch 150, Total Loss: 7601.1968, Loss U: 822.7168, Loss P: 67.7848, Entropy: 598.2242, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,344 - INFO - Epoch 160, Total Loss: 7600.3726, Loss U: 821.2441, Loss P: 67.7913, Entropy: 593.6656, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,423 - INFO - Epoch 170, Total Loss: 7599.6738, Loss U: 820.0410, Loss P: 67.7963, Entropy: 589.2020, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,484 - INFO - Epoch 180, Total Loss: 7599.0742, Loss U: 818.9727, Loss P: 67.8010, Entropy: 584.8740, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,541 - INFO - Epoch 190, Total Loss: 7598.5342, Loss U: 817.9434, Loss P: 67.8059, Entropy: 580.7626, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,598 - INFO - Epoch 200, Total Loss: 7598.0693, Loss U: 817.2910, Loss P: 67.8078, Entropy: 576.4562, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,651 - INFO - Epoch 210, Total Loss: 7597.6738, Loss U: 816.6270, Loss P: 67.8105, Entropy: 571.8731, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,703 - INFO - Epoch 220, Total Loss: 7597.3506, Loss U: 815.8535, Loss P: 67.8150, Entropy: 567.3118, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,758 - INFO - Epoch 230, Total Loss: 7597.0142, Loss U: 815.2637, Loss P: 67.8175, Entropy: 563.0610, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,811 - INFO - Epoch 240, Total Loss: 7596.7231, Loss U: 814.7148, Loss P: 67.8201, Entropy: 558.9402, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,864 - INFO - Epoch 250, Total Loss: 7596.4702, Loss U: 814.1934, Loss P: 67.8228, Entropy: 554.8522, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,923 - INFO - Epoch 260, Total Loss: 7596.2559, Loss U: 813.6973, Loss P: 67.8256, Entropy: 550.9304, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:28,979 - INFO - Epoch 270, Total Loss: 7596.0498, Loss U: 813.2188, Loss P: 67.8283, Entropy: 547.2779, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,040 - INFO - Epoch 280, Total Loss: 7595.8701, Loss U: 812.7812, Loss P: 67.8309, Entropy: 543.8165, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,091 - INFO - Epoch 290, Total Loss: 7595.7036, Loss U: 812.3516, Loss P: 67.8335, Entropy: 540.5437, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,149 - INFO - Epoch 300, Total Loss: 7595.5781, Loss U: 812.0059, Loss P: 67.8357, Entropy: 537.4407, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,202 - INFO - Epoch 310, Total Loss: 7595.4287, Loss U: 811.6953, Loss P: 67.8373, Entropy: 534.5516, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,262 - INFO - Epoch 320, Total Loss: 7595.3008, Loss U: 811.4141, Loss P: 67.8389, Entropy: 531.8458, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,323 - INFO - Epoch 330, Total Loss: 7595.1909, Loss U: 811.1621, Loss P: 67.8403, Entropy: 529.3040, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,387 - INFO - Epoch 340, Total Loss: 7595.0845, Loss U: 810.9336, Loss P: 67.8415, Entropy: 526.9097, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,449 - INFO - Epoch 350, Total Loss: 7595.0332, Loss U: 810.6035, Loss P: 67.8443, Entropy: 524.6499, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,514 - INFO - Epoch 360, Total Loss: 7594.9087, Loss U: 810.4766, Loss P: 67.8443, Entropy: 522.5860, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,566 - INFO - Epoch 370, Total Loss: 7594.8223, Loss U: 810.2598, Loss P: 67.8456, Entropy: 520.5738, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,617 - INFO - Epoch 380, Total Loss: 7594.7461, Loss U: 810.0547, Loss P: 67.8469, Entropy: 518.6212, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,670 - INFO - Epoch 390, Total Loss: 7594.6787, Loss U: 809.8750, Loss P: 67.8480, Entropy: 516.7339, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,729 - INFO - Epoch 400, Total Loss: 7594.6113, Loss U: 809.7051, Loss P: 67.8491, Entropy: 514.9338, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,786 - INFO - Epoch 410, Total Loss: 7594.5527, Loss U: 809.5605, Loss P: 67.8499, Entropy: 513.2150, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,839 - INFO - Epoch 420, Total Loss: 7594.5024, Loss U: 809.3613, Loss P: 67.8514, Entropy: 511.6080, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,891 - INFO - Epoch 430, Total Loss: 7594.4487, Loss U: 809.2480, Loss P: 67.8520, Entropy: 510.0754, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:29,948 - INFO - Epoch 440, Total Loss: 7594.3906, Loss U: 809.1133, Loss P: 67.8528, Entropy: 508.5994, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:30,007 - INFO - Epoch 450, Total Loss: 7594.3477, Loss U: 809.0059, Loss P: 67.8534, Entropy: 507.1668, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:30,088 - INFO - Epoch 460, Total Loss: 7594.3008, Loss U: 808.8945, Loss P: 67.8541, Entropy: 505.7735, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:30,178 - INFO - Epoch 470, Total Loss: 7594.2837, Loss U: 808.7988, Loss P: 67.8549, Entropy: 504.4157, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:30,262 - INFO - Epoch 480, Total Loss: 7594.2285, Loss U: 808.6602, Loss P: 67.8557, Entropy: 503.0998, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:30,333 - INFO - Epoch 490, Total Loss: 7594.1914, Loss U: 808.6230, Loss P: 67.8557, Entropy: 501.7910, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:30,600 - INFO - Epoch 500, Total Loss: 7594.1489, Loss U: 808.5273, Loss P: 67.8562, Entropy: 500.4805, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:30,690 - INFO - Epoch 510, Total Loss: 7594.1099, Loss U: 808.5371, Loss P: 67.8557, Entropy: 499.1360, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:30,756 - INFO - Epoch 520, Total Loss: 7594.0771, Loss U: 808.5684, Loss P: 67.8551, Entropy: 497.7113, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:30,828 - INFO - Epoch 530, Total Loss: 7594.0454, Loss U: 808.5918, Loss P: 67.8545, Entropy: 496.2628, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:30,969 - INFO - Epoch 540, Total Loss: 7594.0244, Loss U: 808.5332, Loss P: 67.8549, Entropy: 494.9714, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,021 - INFO - Epoch 550, Total Loss: 7593.9873, Loss U: 808.4785, Loss P: 67.8551, Entropy: 493.7973, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,088 - INFO - Epoch 560, Total Loss: 7593.9512, Loss U: 808.3965, Loss P: 67.8555, Entropy: 492.6669, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,158 - INFO - Epoch 570, Total Loss: 7593.9224, Loss U: 808.3320, Loss P: 67.8559, Entropy: 491.5351, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,254 - INFO - Epoch 580, Total Loss: 7593.8999, Loss U: 808.2988, Loss P: 67.8560, Entropy: 490.3953, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,346 - INFO - Epoch 590, Total Loss: 7593.8765, Loss U: 808.2617, Loss P: 67.8561, Entropy: 489.2660, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,437 - INFO - Epoch 600, Total Loss: 7593.8472, Loss U: 808.1758, Loss P: 67.8567, Entropy: 488.1689, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,568 - INFO - Epoch 610, Total Loss: 7593.8511, Loss U: 808.0996, Loss P: 67.8575, Entropy: 487.0946, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,646 - INFO - Epoch 620, Total Loss: 7593.8198, Loss U: 808.1211, Loss P: 67.8570, Entropy: 486.0541, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,711 - INFO - Epoch 630, Total Loss: 7593.7896, Loss U: 807.9863, Loss P: 67.8580, Entropy: 485.0066, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,791 - INFO - Epoch 640, Total Loss: 7593.7817, Loss U: 807.9434, Loss P: 67.8584, Entropy: 483.9812, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,849 - INFO - Epoch 650, Total Loss: 7593.7588, Loss U: 807.9160, Loss P: 67.8584, Entropy: 482.9881, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,903 - INFO - Epoch 660, Total Loss: 7593.7378, Loss U: 807.8535, Loss P: 67.8588, Entropy: 482.0097, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:31,958 - INFO - Epoch 670, Total Loss: 7593.7241, Loss U: 807.8262, Loss P: 67.8590, Entropy: 481.0448, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,018 - INFO - Epoch 680, Total Loss: 7593.7109, Loss U: 807.7637, Loss P: 67.8595, Entropy: 480.1027, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,078 - INFO - Epoch 690, Total Loss: 7593.6875, Loss U: 807.6777, Loss P: 67.8601, Entropy: 479.1667, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,136 - INFO - Epoch 700, Total Loss: 7593.6748, Loss U: 807.6426, Loss P: 67.8603, Entropy: 478.2527, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,190 - INFO - Epoch 710, Total Loss: 7593.6577, Loss U: 807.5859, Loss P: 67.8607, Entropy: 477.3604, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,245 - INFO - Epoch 720, Total Loss: 7593.6460, Loss U: 807.5566, Loss P: 67.8609, Entropy: 476.4777, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,301 - INFO - Epoch 730, Total Loss: 7593.6328, Loss U: 807.4727, Loss P: 67.8616, Entropy: 475.6069, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,357 - INFO - Epoch 740, Total Loss: 7593.6440, Loss U: 807.3730, Loss P: 67.8627, Entropy: 474.7446, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,442 - INFO - Epoch 750, Total Loss: 7593.6064, Loss U: 807.4902, Loss P: 67.8612, Entropy: 473.8880, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,496 - INFO - Epoch 760, Total Loss: 7593.5869, Loss U: 807.3809, Loss P: 67.8621, Entropy: 473.0647, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,551 - INFO - Epoch 770, Total Loss: 7593.5767, Loss U: 807.3652, Loss P: 67.8621, Entropy: 472.2408, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,600 - INFO - Epoch 780, Total Loss: 7593.5874, Loss U: 807.3027, Loss P: 67.8628, Entropy: 471.4292, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,654 - INFO - Epoch 790, Total Loss: 7593.5610, Loss U: 807.3262, Loss P: 67.8624, Entropy: 470.6365, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,709 - INFO - Epoch 800, Total Loss: 7593.5474, Loss U: 807.2773, Loss P: 67.8627, Entropy: 469.8553, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,761 - INFO - Epoch 810, Total Loss: 7593.5317, Loss U: 807.2676, Loss P: 67.8626, Entropy: 469.0817, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,812 - INFO - Epoch 820, Total Loss: 7593.5176, Loss U: 807.2148, Loss P: 67.8630, Entropy: 468.3172, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,876 - INFO - Epoch 830, Total Loss: 7593.5127, Loss U: 807.1953, Loss P: 67.8632, Entropy: 467.5593, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,931 - INFO - Epoch 840, Total Loss: 7593.5518, Loss U: 807.1719, Loss P: 67.8638, Entropy: 466.7995, Entropy Lam: 1e-07\n",
      "2024-12-22 07:38:32,992 - INFO - Epoch 850, Total Loss: 7593.5127, Loss U: 807.1602, Loss P: 67.8635, Entropy: 466.0656, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,054 - INFO - Epoch 860, Total Loss: 7593.4946, Loss U: 807.1270, Loss P: 67.8637, Entropy: 465.3432, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,113 - INFO - Epoch 870, Total Loss: 7593.4785, Loss U: 807.1074, Loss P: 67.8637, Entropy: 464.6389, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,164 - INFO - Epoch 880, Total Loss: 7593.4688, Loss U: 807.0820, Loss P: 67.8639, Entropy: 463.9426, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,222 - INFO - Epoch 890, Total Loss: 7593.4785, Loss U: 807.1016, Loss P: 67.8638, Entropy: 463.2606, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,274 - INFO - Epoch 900, Total Loss: 7593.4565, Loss U: 807.0117, Loss P: 67.8644, Entropy: 462.5781, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,327 - INFO - Epoch 910, Total Loss: 7593.4487, Loss U: 806.9941, Loss P: 67.8645, Entropy: 461.9127, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,384 - INFO - Epoch 920, Total Loss: 7593.4458, Loss U: 807.0000, Loss P: 67.8645, Entropy: 461.2454, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,443 - INFO - Epoch 930, Total Loss: 7593.4546, Loss U: 806.9531, Loss P: 67.8650, Entropy: 460.5782, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,503 - INFO - Epoch 940, Total Loss: 7593.4258, Loss U: 806.9160, Loss P: 67.8651, Entropy: 459.9242, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,562 - INFO - Epoch 950, Total Loss: 7593.4214, Loss U: 806.9141, Loss P: 67.8651, Entropy: 459.2807, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,615 - INFO - Epoch 960, Total Loss: 7593.4077, Loss U: 806.8887, Loss P: 67.8652, Entropy: 458.6421, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,670 - INFO - Epoch 970, Total Loss: 7593.4019, Loss U: 806.8555, Loss P: 67.8655, Entropy: 458.0015, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,727 - INFO - Epoch 980, Total Loss: 7593.4224, Loss U: 806.8457, Loss P: 67.8658, Entropy: 457.3683, Entropy Lam: 1.1e-07\n",
      "2024-12-22 07:38:33,788 - INFO - Epoch 990, Total Loss: 7593.4263, Loss U: 806.7715, Loss P: 67.8665, Entropy: 456.7615, Entropy Lam: 1.21e-07\n",
      "2024-12-22 07:38:33,854 - INFO - Epoch 1000, Total Loss: 7593.3882, Loss U: 806.8555, Loss P: 67.8653, Entropy: 456.1282, Entropy Lam: 1.21e-07\n",
      "2024-12-22 07:38:33,914 - INFO - Epoch 1010, Total Loss: 7593.3765, Loss U: 806.7812, Loss P: 67.8660, Entropy: 455.5114, Entropy Lam: 1.21e-07\n",
      "2024-12-22 07:38:33,978 - INFO - Epoch 1020, Total Loss: 7593.3657, Loss U: 806.7637, Loss P: 67.8660, Entropy: 454.8984, Entropy Lam: 1.21e-07\n",
      "2024-12-22 07:38:34,039 - INFO - Epoch 1030, Total Loss: 7593.3628, Loss U: 806.7617, Loss P: 67.8660, Entropy: 454.2903, Entropy Lam: 1.21e-07\n",
      "2024-12-22 07:38:34,097 - INFO - Epoch 1040, Total Loss: 7593.3730, Loss U: 806.7168, Loss P: 67.8666, Entropy: 453.6863, Entropy Lam: 1.21e-07\n",
      "2024-12-22 07:38:34,152 - INFO - Epoch 1050, Total Loss: 7593.3833, Loss U: 806.7637, Loss P: 67.8662, Entropy: 453.1140, Entropy Lam: 1.3310000000000002e-07\n",
      "2024-12-22 07:38:34,208 - INFO - Epoch 1060, Total Loss: 7593.3501, Loss U: 806.7578, Loss P: 67.8659, Entropy: 452.5488, Entropy Lam: 1.3310000000000002e-07\n",
      "2024-12-22 07:38:34,262 - INFO - Epoch 1070, Total Loss: 7593.3452, Loss U: 806.7324, Loss P: 67.8661, Entropy: 451.9926, Entropy Lam: 1.3310000000000002e-07\n",
      "2024-12-22 07:38:34,314 - INFO - Epoch 1080, Total Loss: 7593.3335, Loss U: 806.7109, Loss P: 67.8662, Entropy: 451.4517, Entropy Lam: 1.3310000000000002e-07\n",
      "2024-12-22 07:38:34,366 - INFO - Epoch 1090, Total Loss: 7593.3271, Loss U: 806.7090, Loss P: 67.8662, Entropy: 450.9189, Entropy Lam: 1.3310000000000002e-07\n",
      "2024-12-22 07:38:34,417 - INFO - Epoch 1100, Total Loss: 7593.3433, Loss U: 806.6543, Loss P: 67.8669, Entropy: 450.3970, Entropy Lam: 1.3310000000000002e-07\n",
      "2024-12-22 07:38:34,469 - INFO - Epoch 1110, Total Loss: 7593.3442, Loss U: 806.6758, Loss P: 67.8667, Entropy: 449.8749, Entropy Lam: 1.4641000000000003e-07\n",
      "2024-12-22 07:38:34,532 - INFO - Epoch 1120, Total Loss: 7593.3198, Loss U: 806.7246, Loss P: 67.8660, Entropy: 449.3963, Entropy Lam: 1.4641000000000003e-07\n",
      "2024-12-22 07:38:34,588 - INFO - Epoch 1130, Total Loss: 7593.3174, Loss U: 806.6855, Loss P: 67.8663, Entropy: 448.8985, Entropy Lam: 1.4641000000000003e-07\n",
      "2024-12-22 07:38:34,737 - INFO - Epoch 1140, Total Loss: 7593.3096, Loss U: 806.6992, Loss P: 67.8661, Entropy: 448.4036, Entropy Lam: 1.4641000000000003e-07\n",
      "2024-12-22 07:38:34,791 - INFO - Epoch 1150, Total Loss: 7593.3057, Loss U: 806.6738, Loss P: 67.8663, Entropy: 447.9037, Entropy Lam: 1.4641000000000003e-07\n",
      "2024-12-22 07:38:34,842 - INFO - Epoch 1160, Total Loss: 7593.3242, Loss U: 806.6406, Loss P: 67.8668, Entropy: 447.3956, Entropy Lam: 1.4641000000000003e-07\n",
      "2024-12-22 07:38:34,895 - INFO - Epoch 1170, Total Loss: 7593.3140, Loss U: 806.6484, Loss P: 67.8667, Entropy: 446.8905, Entropy Lam: 1.6105100000000003e-07\n",
      "2024-12-22 07:38:34,947 - INFO - Epoch 1180, Total Loss: 7593.2979, Loss U: 806.6387, Loss P: 67.8666, Entropy: 446.4004, Entropy Lam: 1.6105100000000003e-07\n",
      "2024-12-22 07:38:34,998 - INFO - Epoch 1190, Total Loss: 7593.2847, Loss U: 806.6348, Loss P: 67.8665, Entropy: 445.9359, Entropy Lam: 1.6105100000000003e-07\n",
      "2024-12-22 07:38:35,055 - INFO - Epoch 1200, Total Loss: 7593.2778, Loss U: 806.6211, Loss P: 67.8666, Entropy: 445.4684, Entropy Lam: 1.6105100000000003e-07\n",
      "2024-12-22 07:38:35,116 - INFO - Epoch 1210, Total Loss: 7593.2769, Loss U: 806.6191, Loss P: 67.8666, Entropy: 445.0026, Entropy Lam: 1.6105100000000003e-07\n",
      "2024-12-22 07:38:35,176 - INFO - Epoch 1220, Total Loss: 7593.2896, Loss U: 806.6172, Loss P: 67.8667, Entropy: 444.5550, Entropy Lam: 1.6105100000000003e-07\n",
      "2024-12-22 07:38:35,232 - INFO - Epoch 1230, Total Loss: 7593.2998, Loss U: 806.6406, Loss P: 67.8666, Entropy: 444.0991, Entropy Lam: 1.7715610000000005e-07\n",
      "2024-12-22 07:38:35,291 - INFO - Epoch 1240, Total Loss: 7593.2793, Loss U: 806.5645, Loss P: 67.8671, Entropy: 443.6410, Entropy Lam: 1.7715610000000005e-07\n",
      "2024-12-22 07:38:35,348 - INFO - Epoch 1250, Total Loss: 7593.2627, Loss U: 806.5625, Loss P: 67.8670, Entropy: 443.2115, Entropy Lam: 1.7715610000000005e-07\n",
      "2024-12-22 07:38:35,399 - INFO - Epoch 1260, Total Loss: 7593.2559, Loss U: 806.5645, Loss P: 67.8669, Entropy: 442.7820, Entropy Lam: 1.7715610000000005e-07\n",
      "2024-12-22 07:38:35,449 - INFO - Epoch 1270, Total Loss: 7593.2598, Loss U: 806.5684, Loss P: 67.8669, Entropy: 442.3456, Entropy Lam: 1.7715610000000005e-07\n",
      "2024-12-22 07:38:35,499 - INFO - Epoch 1280, Total Loss: 7593.2700, Loss U: 806.5664, Loss P: 67.8670, Entropy: 441.9128, Entropy Lam: 1.7715610000000005e-07\n",
      "2024-12-22 07:38:35,554 - INFO - Epoch 1290, Total Loss: 7593.2554, Loss U: 806.4863, Loss P: 67.8677, Entropy: 441.4780, Entropy Lam: 1.9487171000000006e-07\n",
      "2024-12-22 07:38:35,600 - INFO - Epoch 1300, Total Loss: 7593.2524, Loss U: 806.5391, Loss P: 67.8671, Entropy: 441.0609, Entropy Lam: 1.9487171000000006e-07\n",
      "2024-12-22 07:38:35,654 - INFO - Epoch 1310, Total Loss: 7593.2358, Loss U: 806.4844, Loss P: 67.8675, Entropy: 440.6701, Entropy Lam: 1.9487171000000006e-07\n",
      "2024-12-22 07:38:35,710 - INFO - Epoch 1320, Total Loss: 7593.2329, Loss U: 806.5098, Loss P: 67.8672, Entropy: 440.2632, Entropy Lam: 1.9487171000000006e-07\n",
      "2024-12-22 07:38:35,763 - INFO - Epoch 1330, Total Loss: 7593.2490, Loss U: 806.5195, Loss P: 67.8673, Entropy: 439.8561, Entropy Lam: 1.9487171000000006e-07\n",
      "2024-12-22 07:38:35,820 - INFO - Epoch 1340, Total Loss: 7593.2246, Loss U: 806.4883, Loss P: 67.8674, Entropy: 439.4588, Entropy Lam: 1.9487171000000006e-07\n",
      "2024-12-22 07:38:35,881 - INFO - Epoch 1350, Total Loss: 7593.2104, Loss U: 806.4414, Loss P: 67.8677, Entropy: 439.0658, Entropy Lam: 1.9487171000000006e-07\n",
      "2024-12-22 07:38:35,934 - INFO - Epoch 1360, Total Loss: 7593.2188, Loss U: 806.4727, Loss P: 67.8675, Entropy: 438.6757, Entropy Lam: 1.9487171000000006e-07\n",
      "2024-12-22 07:38:35,994 - INFO - Epoch 1370, Total Loss: 7593.2241, Loss U: 806.4648, Loss P: 67.8676, Entropy: 438.3070, Entropy Lam: 2.143588810000001e-07\n",
      "2024-12-22 07:38:36,060 - INFO - Epoch 1380, Total Loss: 7593.2041, Loss U: 806.4199, Loss P: 67.8678, Entropy: 437.8899, Entropy Lam: 2.143588810000001e-07\n",
      "2024-12-22 07:38:36,124 - INFO - Epoch 1390, Total Loss: 7593.1953, Loss U: 806.4805, Loss P: 67.8671, Entropy: 437.5057, Entropy Lam: 2.143588810000001e-07\n",
      "2024-12-22 07:38:36,189 - INFO - Epoch 1400, Total Loss: 7593.1938, Loss U: 806.4316, Loss P: 67.8676, Entropy: 437.1414, Entropy Lam: 2.143588810000001e-07\n",
      "2024-12-22 07:38:36,254 - INFO - Epoch 1410, Total Loss: 7593.1836, Loss U: 806.4238, Loss P: 67.8676, Entropy: 436.7798, Entropy Lam: 2.143588810000001e-07\n",
      "2024-12-22 07:38:36,310 - INFO - Epoch 1420, Total Loss: 7593.1914, Loss U: 806.4297, Loss P: 67.8676, Entropy: 436.4142, Entropy Lam: 2.143588810000001e-07\n",
      "2024-12-22 07:38:36,367 - INFO - Epoch 1430, Total Loss: 7593.2109, Loss U: 806.3984, Loss P: 67.8681, Entropy: 436.0689, Entropy Lam: 2.143588810000001e-07\n",
      "2024-12-22 07:38:36,424 - INFO - Epoch 1440, Total Loss: 7593.2007, Loss U: 806.3711, Loss P: 67.8683, Entropy: 435.6936, Entropy Lam: 2.357947691000001e-07\n",
      "2024-12-22 07:38:36,483 - INFO - Epoch 1450, Total Loss: 7593.1841, Loss U: 806.3789, Loss P: 67.8680, Entropy: 435.3101, Entropy Lam: 2.357947691000001e-07\n",
      "2024-12-22 07:38:36,533 - INFO - Epoch 1460, Total Loss: 7593.1709, Loss U: 806.3516, Loss P: 67.8682, Entropy: 434.9718, Entropy Lam: 2.357947691000001e-07\n",
      "2024-12-22 07:38:36,584 - INFO - Epoch 1470, Total Loss: 7593.1719, Loss U: 806.3496, Loss P: 67.8682, Entropy: 434.6191, Entropy Lam: 2.357947691000001e-07\n",
      "2024-12-22 07:38:36,662 - INFO - Epoch 1480, Total Loss: 7593.1880, Loss U: 806.3184, Loss P: 67.8687, Entropy: 434.2796, Entropy Lam: 2.357947691000001e-07\n",
      "2024-12-22 07:38:36,732 - INFO - Epoch 1490, Total Loss: 7593.1807, Loss U: 806.2734, Loss P: 67.8691, Entropy: 433.9368, Entropy Lam: 2.5937424601000015e-07\n",
      "2024-12-22 07:38:36,813 - INFO - Epoch 1500, Total Loss: 7593.1719, Loss U: 806.3340, Loss P: 67.8684, Entropy: 433.5772, Entropy Lam: 2.5937424601000015e-07\n",
      "2024-12-22 07:38:36,884 - INFO - Epoch 1510, Total Loss: 7593.1592, Loss U: 806.3262, Loss P: 67.8683, Entropy: 433.2303, Entropy Lam: 2.5937424601000015e-07\n",
      "2024-12-22 07:38:36,949 - INFO - Epoch 1520, Total Loss: 7593.1528, Loss U: 806.3027, Loss P: 67.8685, Entropy: 432.8808, Entropy Lam: 2.5937424601000015e-07\n",
      "2024-12-22 07:38:37,009 - INFO - Epoch 1530, Total Loss: 7593.1494, Loss U: 806.3184, Loss P: 67.8683, Entropy: 432.5271, Entropy Lam: 2.5937424601000015e-07\n",
      "2024-12-22 07:38:37,064 - INFO - Epoch 1540, Total Loss: 7593.1743, Loss U: 806.3594, Loss P: 67.8681, Entropy: 432.1600, Entropy Lam: 2.5937424601000015e-07\n",
      "2024-12-22 07:38:37,148 - INFO - Epoch 1550, Total Loss: 7593.1655, Loss U: 806.3125, Loss P: 67.8685, Entropy: 431.8068, Entropy Lam: 2.853116706110002e-07\n",
      "2024-12-22 07:38:37,209 - INFO - Epoch 1560, Total Loss: 7593.1729, Loss U: 806.3145, Loss P: 67.8686, Entropy: 431.4789, Entropy Lam: 2.853116706110002e-07\n",
      "2024-12-22 07:38:37,265 - INFO - Epoch 1570, Total Loss: 7593.1577, Loss U: 806.2344, Loss P: 67.8692, Entropy: 431.1456, Entropy Lam: 2.853116706110002e-07\n",
      "2024-12-22 07:38:37,329 - INFO - Epoch 1580, Total Loss: 7593.1460, Loss U: 806.2891, Loss P: 67.8686, Entropy: 430.8154, Entropy Lam: 2.853116706110002e-07\n",
      "2024-12-22 07:38:37,393 - INFO - Epoch 1590, Total Loss: 7593.1465, Loss U: 806.2598, Loss P: 67.8689, Entropy: 430.4869, Entropy Lam: 2.853116706110002e-07\n",
      "2024-12-22 07:38:37,458 - INFO - Epoch 1600, Total Loss: 7593.1509, Loss U: 806.2461, Loss P: 67.8690, Entropy: 430.1431, Entropy Lam: 2.853116706110002e-07\n",
      "2024-12-22 07:38:37,523 - INFO - Epoch 1610, Total Loss: 7593.1543, Loss U: 806.2148, Loss P: 67.8694, Entropy: 429.8018, Entropy Lam: 3.1384283767210024e-07\n",
      "2024-12-22 07:38:37,584 - INFO - Epoch 1620, Total Loss: 7593.1426, Loss U: 806.2676, Loss P: 67.8688, Entropy: 429.4689, Entropy Lam: 3.1384283767210024e-07\n",
      "2024-12-22 07:38:37,636 - INFO - Epoch 1630, Total Loss: 7593.1367, Loss U: 806.1797, Loss P: 67.8696, Entropy: 429.1550, Entropy Lam: 3.1384283767210024e-07\n",
      "2024-12-22 07:38:37,692 - INFO - Epoch 1640, Total Loss: 7593.1353, Loss U: 806.1992, Loss P: 67.8694, Entropy: 428.8330, Entropy Lam: 3.1384283767210024e-07\n",
      "2024-12-22 07:38:37,745 - INFO - Epoch 1650, Total Loss: 7593.1465, Loss U: 806.1348, Loss P: 67.8701, Entropy: 428.5065, Entropy Lam: 3.452271214393103e-07\n",
      "2024-12-22 07:38:37,799 - INFO - Epoch 1660, Total Loss: 7593.1582, Loss U: 806.1953, Loss P: 67.8696, Entropy: 428.1817, Entropy Lam: 3.452271214393103e-07\n",
      "2024-12-22 07:38:37,850 - INFO - Epoch 1670, Total Loss: 7593.1431, Loss U: 806.2695, Loss P: 67.8687, Entropy: 427.8875, Entropy Lam: 3.452271214393103e-07\n",
      "2024-12-22 07:38:37,979 - INFO - Epoch 1680, Total Loss: 7593.1377, Loss U: 806.2207, Loss P: 67.8692, Entropy: 427.5599, Entropy Lam: 3.452271214393103e-07\n",
      "2024-12-22 07:38:38,033 - INFO - Epoch 1690, Total Loss: 7593.1299, Loss U: 806.1836, Loss P: 67.8695, Entropy: 427.2528, Entropy Lam: 3.452271214393103e-07\n",
      "2024-12-22 07:38:38,089 - INFO - Epoch 1700, Total Loss: 7593.1279, Loss U: 806.1758, Loss P: 67.8695, Entropy: 426.9511, Entropy Lam: 3.452271214393103e-07\n",
      "2024-12-22 07:38:38,148 - INFO - Epoch 1710, Total Loss: 7593.1255, Loss U: 806.1738, Loss P: 67.8695, Entropy: 426.6493, Entropy Lam: 3.797498335832414e-07\n",
      "2024-12-22 07:38:38,205 - INFO - Epoch 1720, Total Loss: 7593.1489, Loss U: 806.1172, Loss P: 67.8703, Entropy: 426.3521, Entropy Lam: 3.797498335832414e-07\n",
      "2024-12-22 07:38:38,264 - INFO - Epoch 1730, Total Loss: 7593.1187, Loss U: 806.1992, Loss P: 67.8692, Entropy: 426.0525, Entropy Lam: 3.797498335832414e-07\n",
      "2024-12-22 07:38:38,329 - INFO - Epoch 1740, Total Loss: 7593.1372, Loss U: 806.2188, Loss P: 67.8692, Entropy: 425.7448, Entropy Lam: 3.797498335832414e-07\n",
      "2024-12-22 07:38:38,392 - INFO - Epoch 1750, Total Loss: 7593.1304, Loss U: 806.1758, Loss P: 67.8695, Entropy: 425.4295, Entropy Lam: 3.797498335832414e-07\n",
      "2024-12-22 07:38:38,454 - INFO - Epoch 1760, Total Loss: 7593.1172, Loss U: 806.1133, Loss P: 67.8700, Entropy: 425.1415, Entropy Lam: 4.177248169415656e-07\n",
      "2024-12-22 07:38:38,515 - INFO - Epoch 1770, Total Loss: 7593.1157, Loss U: 806.1328, Loss P: 67.8698, Entropy: 424.8492, Entropy Lam: 4.177248169415656e-07\n",
      "2024-12-22 07:38:38,569 - INFO - Epoch 1780, Total Loss: 7593.1104, Loss U: 806.1484, Loss P: 67.8696, Entropy: 424.5610, Entropy Lam: 4.177248169415656e-07\n",
      "2024-12-22 07:38:38,623 - INFO - Epoch 1790, Total Loss: 7593.1025, Loss U: 806.1211, Loss P: 67.8698, Entropy: 424.2717, Entropy Lam: 4.177248169415656e-07\n",
      "2024-12-22 07:38:38,683 - INFO - Epoch 1800, Total Loss: 7593.1431, Loss U: 806.1445, Loss P: 67.8700, Entropy: 423.9922, Entropy Lam: 4.177248169415656e-07\n",
      "2024-12-22 07:38:38,733 - INFO - Epoch 1810, Total Loss: 7593.1265, Loss U: 806.0820, Loss P: 67.8704, Entropy: 423.7315, Entropy Lam: 4.594972986357222e-07\n",
      "2024-12-22 07:38:38,778 - INFO - Epoch 1820, Total Loss: 7593.1143, Loss U: 806.1230, Loss P: 67.8699, Entropy: 423.4368, Entropy Lam: 4.594972986357222e-07\n",
      "2024-12-22 07:38:38,823 - INFO - Epoch 1830, Total Loss: 7593.1362, Loss U: 806.0879, Loss P: 67.8705, Entropy: 423.1434, Entropy Lam: 4.594972986357222e-07\n",
      "2024-12-22 07:38:38,866 - INFO - Epoch 1840, Total Loss: 7593.1123, Loss U: 806.0977, Loss P: 67.8701, Entropy: 422.8707, Entropy Lam: 5.054470284992945e-07\n",
      "2024-12-22 07:38:38,911 - INFO - Epoch 1850, Total Loss: 7593.0986, Loss U: 806.1660, Loss P: 67.8693, Entropy: 422.5995, Entropy Lam: 5.054470284992945e-07\n",
      "2024-12-22 07:38:38,957 - INFO - Epoch 1860, Total Loss: 7593.1006, Loss U: 806.1191, Loss P: 67.8698, Entropy: 422.3375, Entropy Lam: 5.054470284992945e-07\n",
      "2024-12-22 07:38:39,008 - INFO - Epoch 1870, Total Loss: 7593.1138, Loss U: 806.1641, Loss P: 67.8695, Entropy: 422.0802, Entropy Lam: 5.054470284992945e-07\n",
      "2024-12-22 07:38:39,060 - INFO - Epoch 1880, Total Loss: 7593.1255, Loss U: 806.1777, Loss P: 67.8695, Entropy: 421.7980, Entropy Lam: 5.55991731349224e-07\n",
      "2024-12-22 07:38:39,115 - INFO - Epoch 1890, Total Loss: 7593.0977, Loss U: 806.0586, Loss P: 67.8704, Entropy: 421.5338, Entropy Lam: 5.55991731349224e-07\n",
      "2024-12-22 07:38:39,181 - INFO - Epoch 1900, Total Loss: 7593.0977, Loss U: 806.1270, Loss P: 67.8697, Entropy: 421.2685, Entropy Lam: 5.55991731349224e-07\n",
      "2024-12-22 07:38:39,246 - INFO - Epoch 1910, Total Loss: 7593.0962, Loss U: 806.0859, Loss P: 67.8701, Entropy: 421.0077, Entropy Lam: 5.55991731349224e-07\n",
      "2024-12-22 07:38:39,311 - INFO - Epoch 1920, Total Loss: 7593.1216, Loss U: 806.0391, Loss P: 67.8708, Entropy: 420.7596, Entropy Lam: 6.115909044841464e-07\n",
      "2024-12-22 07:38:39,501 - INFO - Epoch 1930, Total Loss: 7593.0947, Loss U: 806.1602, Loss P: 67.8693, Entropy: 420.5189, Entropy Lam: 6.115909044841464e-07\n",
      "2024-12-22 07:38:39,573 - INFO - Epoch 1940, Total Loss: 7593.0874, Loss U: 806.1582, Loss P: 67.8693, Entropy: 420.2628, Entropy Lam: 6.115909044841464e-07\n",
      "2024-12-22 07:38:39,643 - INFO - Epoch 1950, Total Loss: 7593.1035, Loss U: 806.0996, Loss P: 67.8700, Entropy: 420.0132, Entropy Lam: 6.115909044841464e-07\n",
      "2024-12-22 07:38:39,716 - INFO - Epoch 1960, Total Loss: 7593.0942, Loss U: 806.1230, Loss P: 67.8697, Entropy: 419.7697, Entropy Lam: 6.115909044841464e-07\n",
      "2024-12-22 07:38:39,784 - INFO - Epoch 1970, Total Loss: 7593.0811, Loss U: 806.1191, Loss P: 67.8696, Entropy: 419.5338, Entropy Lam: 6.727499949325611e-07\n",
      "2024-12-22 07:38:39,845 - INFO - Epoch 1980, Total Loss: 7593.0928, Loss U: 806.1270, Loss P: 67.8697, Entropy: 419.2645, Entropy Lam: 6.727499949325611e-07\n",
      "2024-12-22 07:38:39,900 - INFO - Epoch 1990, Total Loss: 7593.1021, Loss U: 806.1113, Loss P: 67.8699, Entropy: 418.9560, Entropy Lam: 6.727499949325611e-07\n",
      "2024-12-22 07:38:39,954 - INFO - Epoch 2000, Total Loss: 7593.1001, Loss U: 806.0527, Loss P: 67.8705, Entropy: 418.6761, Entropy Lam: 7.400249944258173e-07\n",
      "2024-12-22 07:38:40,008 - INFO - Epoch 2010, Total Loss: 7593.0903, Loss U: 806.1836, Loss P: 67.8691, Entropy: 418.3965, Entropy Lam: 7.400249944258173e-07\n",
      "2024-12-22 07:38:40,064 - INFO - Epoch 2020, Total Loss: 7593.0947, Loss U: 806.0918, Loss P: 67.8700, Entropy: 418.1646, Entropy Lam: 7.400249944258173e-07\n",
      "2024-12-22 07:38:40,113 - INFO - Epoch 2030, Total Loss: 7593.1050, Loss U: 806.0586, Loss P: 67.8705, Entropy: 417.9304, Entropy Lam: 8.140274938683991e-07\n",
      "2024-12-22 07:38:40,170 - INFO - Epoch 2040, Total Loss: 7593.0791, Loss U: 806.1523, Loss P: 67.8693, Entropy: 417.6223, Entropy Lam: 8.140274938683991e-07\n",
      "2024-12-22 07:38:40,219 - INFO - Epoch 2050, Total Loss: 7593.0737, Loss U: 806.1094, Loss P: 67.8696, Entropy: 417.3840, Entropy Lam: 8.140274938683991e-07\n",
      "2024-12-22 07:38:40,275 - INFO - Epoch 2060, Total Loss: 7593.0703, Loss U: 806.1543, Loss P: 67.8692, Entropy: 417.1289, Entropy Lam: 8.140274938683991e-07\n",
      "2024-12-22 07:38:40,334 - INFO - Epoch 2070, Total Loss: 7593.0811, Loss U: 806.1465, Loss P: 67.8693, Entropy: 416.8806, Entropy Lam: 8.140274938683991e-07\n",
      "2024-12-22 07:38:40,394 - INFO - Epoch 2080, Total Loss: 7593.0952, Loss U: 806.1484, Loss P: 67.8695, Entropy: 416.6363, Entropy Lam: 8.95430243255239e-07\n",
      "2024-12-22 07:38:40,449 - INFO - Epoch 2090, Total Loss: 7593.1035, Loss U: 806.0801, Loss P: 67.8702, Entropy: 416.3582, Entropy Lam: 8.95430243255239e-07\n",
      "2024-12-22 07:38:40,511 - INFO - Epoch 2100, Total Loss: 7593.0708, Loss U: 806.1484, Loss P: 67.8692, Entropy: 416.0893, Entropy Lam: 8.95430243255239e-07\n",
      "2024-12-22 07:38:40,594 - INFO - Epoch 2110, Total Loss: 7593.0713, Loss U: 806.1230, Loss P: 67.8695, Entropy: 415.8296, Entropy Lam: 8.95430243255239e-07\n",
      "2024-12-22 07:38:40,658 - INFO - Epoch 2120, Total Loss: 7593.0693, Loss U: 806.1680, Loss P: 67.8690, Entropy: 415.5651, Entropy Lam: 8.95430243255239e-07\n",
      "2024-12-22 07:38:40,729 - INFO - Epoch 2130, Total Loss: 7593.0669, Loss U: 806.1270, Loss P: 67.8694, Entropy: 415.3010, Entropy Lam: 8.95430243255239e-07\n",
      "2024-12-22 07:38:40,792 - INFO - Epoch 2140, Total Loss: 7593.0625, Loss U: 806.1562, Loss P: 67.8691, Entropy: 415.0388, Entropy Lam: 8.95430243255239e-07\n",
      "2024-12-22 07:38:40,849 - INFO - Epoch 2150, Total Loss: 7593.0806, Loss U: 806.1934, Loss P: 67.8689, Entropy: 414.7730, Entropy Lam: 8.95430243255239e-07\n",
      "2024-12-22 07:38:40,909 - INFO - Epoch 2160, Total Loss: 7593.1001, Loss U: 806.2695, Loss P: 67.8683, Entropy: 414.5126, Entropy Lam: 8.95430243255239e-07\n",
      "2024-12-22 07:38:40,963 - INFO - Epoch 2170, Total Loss: 7593.0723, Loss U: 806.0996, Loss P: 67.8697, Entropy: 414.2746, Entropy Lam: 9.84973267580763e-07\n",
      "2024-12-22 07:38:41,013 - INFO - Epoch 2180, Total Loss: 7593.0796, Loss U: 806.1934, Loss P: 67.8689, Entropy: 414.0352, Entropy Lam: 9.84973267580763e-07\n",
      "2024-12-22 07:38:41,066 - INFO - Epoch 2190, Total Loss: 7593.0728, Loss U: 806.1172, Loss P: 67.8696, Entropy: 413.7844, Entropy Lam: 9.84973267580763e-07\n",
      "2024-12-22 07:38:41,113 - INFO - Epoch 2200, Total Loss: 7593.0596, Loss U: 806.1152, Loss P: 67.8694, Entropy: 413.5457, Entropy Lam: 1.0834705943388394e-06\n",
      "2024-12-22 07:38:41,167 - INFO - Epoch 2210, Total Loss: 7593.0693, Loss U: 806.0723, Loss P: 67.8700, Entropy: 413.3034, Entropy Lam: 1.0834705943388394e-06\n",
      "2024-12-22 07:38:41,214 - INFO - Epoch 2220, Total Loss: 7593.0674, Loss U: 806.1328, Loss P: 67.8693, Entropy: 413.0465, Entropy Lam: 1.1918176537727233e-06\n",
      "2024-12-22 07:38:41,270 - INFO - Epoch 2230, Total Loss: 7593.0698, Loss U: 806.1348, Loss P: 67.8693, Entropy: 412.8164, Entropy Lam: 1.1918176537727233e-06\n",
      "2024-12-22 07:38:41,322 - INFO - Epoch 2240, Total Loss: 7593.0601, Loss U: 806.1309, Loss P: 67.8693, Entropy: 412.5804, Entropy Lam: 1.1918176537727233e-06\n",
      "2024-12-22 07:38:41,373 - INFO - Epoch 2250, Total Loss: 7593.0718, Loss U: 806.1055, Loss P: 67.8697, Entropy: 412.3280, Entropy Lam: 1.1918176537727233e-06\n",
      "2024-12-22 07:38:41,489 - INFO - Epoch 2260, Total Loss: 7593.0757, Loss U: 806.1094, Loss P: 67.8697, Entropy: 412.0583, Entropy Lam: 1.1918176537727233e-06\n",
      "2024-12-22 07:38:41,539 - INFO - Epoch 2270, Total Loss: 7593.0815, Loss U: 806.1289, Loss P: 67.8695, Entropy: 411.8382, Entropy Lam: 1.3109994191499957e-06\n",
      "2024-12-22 07:38:41,589 - INFO - Epoch 2280, Total Loss: 7593.0630, Loss U: 806.1309, Loss P: 67.8693, Entropy: 411.6293, Entropy Lam: 1.3109994191499957e-06\n",
      "2024-12-22 07:38:41,637 - INFO - Epoch 2290, Total Loss: 7593.0693, Loss U: 806.0352, Loss P: 67.8703, Entropy: 411.4032, Entropy Lam: 1.3109994191499957e-06\n",
      "2024-12-22 07:38:41,687 - INFO - Epoch 2300, Total Loss: 7593.0806, Loss U: 806.0859, Loss P: 67.8699, Entropy: 411.1570, Entropy Lam: 1.4420993610649954e-06\n",
      "2024-12-22 07:38:41,733 - INFO - Epoch 2310, Total Loss: 7593.0703, Loss U: 806.1602, Loss P: 67.8691, Entropy: 410.8967, Entropy Lam: 1.4420993610649954e-06\n",
      "2024-12-22 07:38:41,782 - INFO - Epoch 2320, Total Loss: 7593.0601, Loss U: 806.1270, Loss P: 67.8693, Entropy: 410.7174, Entropy Lam: 1.4420993610649954e-06\n",
      "2024-12-22 07:38:41,843 - INFO - Epoch 2330, Total Loss: 7593.0488, Loss U: 806.1250, Loss P: 67.8692, Entropy: 410.5055, Entropy Lam: 1.4420993610649954e-06\n",
      "2024-12-22 07:38:41,904 - INFO - Epoch 2340, Total Loss: 7593.0386, Loss U: 806.1133, Loss P: 67.8692, Entropy: 410.2874, Entropy Lam: 1.4420993610649954e-06\n",
      "2024-12-22 07:38:41,963 - INFO - Epoch 2350, Total Loss: 7593.0449, Loss U: 806.1191, Loss P: 67.8693, Entropy: 410.0612, Entropy Lam: 1.4420993610649954e-06\n",
      "2024-12-22 07:38:42,026 - INFO - Epoch 2360, Total Loss: 7593.0522, Loss U: 806.1094, Loss P: 67.8694, Entropy: 409.8344, Entropy Lam: 1.4420993610649954e-06\n",
      "2024-12-22 07:38:42,084 - INFO - Epoch 2370, Total Loss: 7593.0835, Loss U: 806.1094, Loss P: 67.8697, Entropy: 409.5937, Entropy Lam: 1.586309297171495e-06\n",
      "2024-12-22 07:38:42,136 - INFO - Epoch 2380, Total Loss: 7593.0649, Loss U: 806.1289, Loss P: 67.8694, Entropy: 409.3401, Entropy Lam: 1.586309297171495e-06\n",
      "2024-12-22 07:38:42,201 - INFO - Epoch 2390, Total Loss: 7593.0591, Loss U: 806.1367, Loss P: 67.8692, Entropy: 409.1351, Entropy Lam: 1.7449402268886447e-06\n",
      "2024-12-22 07:38:42,269 - INFO - Epoch 2400, Total Loss: 7593.0659, Loss U: 806.0684, Loss P: 67.8700, Entropy: 408.9673, Entropy Lam: 1.7449402268886447e-06\n",
      "2024-12-22 07:38:42,346 - INFO - Epoch 2410, Total Loss: 7593.0562, Loss U: 806.0938, Loss P: 67.8696, Entropy: 408.7667, Entropy Lam: 1.7449402268886447e-06\n",
      "2024-12-22 07:38:42,415 - INFO - Epoch 2420, Total Loss: 7593.0503, Loss U: 806.1387, Loss P: 67.8691, Entropy: 408.5578, Entropy Lam: 1.7449402268886447e-06\n",
      "2024-12-22 07:38:42,488 - INFO - Epoch 2430, Total Loss: 7593.1030, Loss U: 806.0586, Loss P: 67.8704, Entropy: 408.3250, Entropy Lam: 1.7449402268886447e-06\n",
      "2024-12-22 07:38:42,558 - INFO - Epoch 2440, Total Loss: 7593.0581, Loss U: 806.1094, Loss P: 67.8695, Entropy: 408.0749, Entropy Lam: 1.9194342495775094e-06\n",
      "2024-12-22 07:38:42,632 - INFO - Epoch 2450, Total Loss: 7593.0332, Loss U: 806.1328, Loss P: 67.8690, Entropy: 407.8947, Entropy Lam: 1.9194342495775094e-06\n",
      "2024-12-22 07:38:42,706 - INFO - Epoch 2460, Total Loss: 7593.0420, Loss U: 806.0996, Loss P: 67.8694, Entropy: 407.7045, Entropy Lam: 1.9194342495775094e-06\n",
      "2024-12-22 07:38:42,772 - INFO - Epoch 2470, Total Loss: 7593.0513, Loss U: 806.1211, Loss P: 67.8693, Entropy: 407.5320, Entropy Lam: 1.9194342495775094e-06\n",
      "2024-12-22 07:38:42,842 - INFO - Epoch 2480, Total Loss: 7593.0381, Loss U: 806.1055, Loss P: 67.8693, Entropy: 407.3487, Entropy Lam: 2.1113776745352604e-06\n",
      "2024-12-22 07:38:42,904 - INFO - Epoch 2490, Total Loss: 7593.0400, Loss U: 806.1250, Loss P: 67.8691, Entropy: 407.1468, Entropy Lam: 2.1113776745352604e-06\n",
      "2024-12-22 07:38:42,973 - INFO - Epoch 2500, Total Loss: 7593.0430, Loss U: 806.1934, Loss P: 67.8685, Entropy: 406.9474, Entropy Lam: 2.3225154419887867e-06\n",
      "2024-12-22 07:38:43,029 - INFO - Epoch 2510, Total Loss: 7593.1323, Loss U: 806.2539, Loss P: 67.8688, Entropy: 406.7029, Entropy Lam: 2.3225154419887867e-06\n",
      "2024-12-22 07:38:43,128 - INFO - Epoch 2520, Total Loss: 7593.0488, Loss U: 806.0547, Loss P: 67.8699, Entropy: 406.4477, Entropy Lam: 2.5547669861876656e-06\n",
      "2024-12-22 07:38:43,203 - INFO - Epoch 2530, Total Loss: 7593.0396, Loss U: 806.0703, Loss P: 67.8697, Entropy: 406.2930, Entropy Lam: 2.5547669861876656e-06\n",
      "2024-12-22 07:38:43,279 - INFO - Epoch 2540, Total Loss: 7593.0347, Loss U: 806.1309, Loss P: 67.8690, Entropy: 406.1190, Entropy Lam: 2.5547669861876656e-06\n",
      "2024-12-22 07:38:43,344 - INFO - Epoch 2550, Total Loss: 7593.0342, Loss U: 806.1133, Loss P: 67.8692, Entropy: 405.9281, Entropy Lam: 2.5547669861876656e-06\n",
      "2024-12-22 07:38:43,426 - INFO - Epoch 2560, Total Loss: 7593.0312, Loss U: 806.0996, Loss P: 67.8693, Entropy: 405.7417, Entropy Lam: 2.8102436848064324e-06\n",
      "2024-12-22 07:38:43,488 - INFO - Epoch 2570, Total Loss: 7593.0469, Loss U: 806.1289, Loss P: 67.8692, Entropy: 405.5577, Entropy Lam: 2.8102436848064324e-06\n",
      "2024-12-22 07:38:43,551 - INFO - Epoch 2580, Total Loss: 7593.0508, Loss U: 806.1328, Loss P: 67.8692, Entropy: 405.3882, Entropy Lam: 3.091268053287076e-06\n",
      "2024-12-22 07:38:43,613 - INFO - Epoch 2590, Total Loss: 7593.0601, Loss U: 806.1699, Loss P: 67.8689, Entropy: 405.1959, Entropy Lam: 3.091268053287076e-06\n",
      "2024-12-22 07:38:43,676 - INFO - Epoch 2600, Total Loss: 7593.0459, Loss U: 806.1289, Loss P: 67.8692, Entropy: 404.9774, Entropy Lam: 3.400394858615784e-06\n",
      "2024-12-22 07:38:43,730 - INFO - Epoch 2610, Total Loss: 7593.0273, Loss U: 806.0957, Loss P: 67.8693, Entropy: 404.7893, Entropy Lam: 3.400394858615784e-06\n",
      "2024-12-22 07:38:43,784 - INFO - Epoch 2620, Total Loss: 7593.0278, Loss U: 806.0977, Loss P: 67.8693, Entropy: 404.6101, Entropy Lam: 3.400394858615784e-06\n",
      "2024-12-22 07:38:43,835 - INFO - Epoch 2630, Total Loss: 7593.0503, Loss U: 806.0430, Loss P: 67.8701, Entropy: 404.4107, Entropy Lam: 3.400394858615784e-06\n",
      "2024-12-22 07:38:43,884 - INFO - Epoch 2640, Total Loss: 7593.0430, Loss U: 806.0879, Loss P: 67.8695, Entropy: 404.1989, Entropy Lam: 3.7404343444773626e-06\n",
      "2024-12-22 07:38:43,932 - INFO - Epoch 2650, Total Loss: 7593.0264, Loss U: 806.1406, Loss P: 67.8688, Entropy: 404.0308, Entropy Lam: 3.7404343444773626e-06\n",
      "2024-12-22 07:38:43,978 - INFO - Epoch 2660, Total Loss: 7593.0298, Loss U: 806.0645, Loss P: 67.8696, Entropy: 403.8505, Entropy Lam: 3.7404343444773626e-06\n",
      "2024-12-22 07:38:44,029 - INFO - Epoch 2670, Total Loss: 7593.0645, Loss U: 806.1289, Loss P: 67.8693, Entropy: 403.6688, Entropy Lam: 3.7404343444773626e-06\n",
      "2024-12-22 07:38:44,083 - INFO - Epoch 2680, Total Loss: 7593.0425, Loss U: 806.1758, Loss P: 67.8687, Entropy: 403.4950, Entropy Lam: 4.1144777789250995e-06\n",
      "2024-12-22 07:38:44,137 - INFO - Epoch 2690, Total Loss: 7593.0352, Loss U: 806.0859, Loss P: 67.8695, Entropy: 403.3530, Entropy Lam: 4.1144777789250995e-06\n",
      "2024-12-22 07:38:44,192 - INFO - Epoch 2700, Total Loss: 7593.0410, Loss U: 806.1348, Loss P: 67.8690, Entropy: 403.1730, Entropy Lam: 4.52592555681761e-06\n",
      "2024-12-22 07:38:44,251 - INFO - Epoch 2710, Total Loss: 7593.0337, Loss U: 806.1465, Loss P: 67.8689, Entropy: 402.9957, Entropy Lam: 4.52592555681761e-06\n",
      "2024-12-22 07:38:44,301 - INFO - Epoch 2720, Total Loss: 7593.0205, Loss U: 806.1152, Loss P: 67.8690, Entropy: 402.8463, Entropy Lam: 4.52592555681761e-06\n",
      "2024-12-22 07:38:44,348 - INFO - Epoch 2730, Total Loss: 7593.0273, Loss U: 806.1543, Loss P: 67.8687, Entropy: 402.6805, Entropy Lam: 4.52592555681761e-06\n",
      "2024-12-22 07:38:44,395 - INFO - Epoch 2740, Total Loss: 7593.0386, Loss U: 806.2285, Loss P: 67.8681, Entropy: 402.5282, Entropy Lam: 4.52592555681761e-06\n",
      "2024-12-22 07:38:44,442 - INFO - Epoch 2750, Total Loss: 7593.0444, Loss U: 806.2031, Loss P: 67.8684, Entropy: 402.3659, Entropy Lam: 4.978518112499371e-06\n",
      "2024-12-22 07:38:44,480 - INFO - Epoch 2760, Total Loss: 7593.0371, Loss U: 806.1270, Loss P: 67.8691, Entropy: 402.2348, Entropy Lam: 4.978518112499371e-06\n",
      "2024-12-22 07:38:44,529 - INFO - Epoch 2770, Total Loss: 7593.0249, Loss U: 806.1914, Loss P: 67.8683, Entropy: 402.0880, Entropy Lam: 4.978518112499371e-06\n",
      "2024-12-22 07:38:44,576 - INFO - Epoch 2780, Total Loss: 7593.0288, Loss U: 806.2012, Loss P: 67.8683, Entropy: 401.9235, Entropy Lam: 4.978518112499371e-06\n",
      "2024-12-22 07:38:44,624 - INFO - Epoch 2790, Total Loss: 7593.0767, Loss U: 806.1289, Loss P: 67.8695, Entropy: 401.7201, Entropy Lam: 5.476369923749309e-06\n",
      "2024-12-22 07:38:44,668 - INFO - Epoch 2800, Total Loss: 7593.0498, Loss U: 806.1074, Loss P: 67.8694, Entropy: 401.5069, Entropy Lam: 5.476369923749309e-06\n",
      "2024-12-22 07:38:44,716 - INFO - Epoch 2810, Total Loss: 7593.0239, Loss U: 806.2051, Loss P: 67.8682, Entropy: 401.3476, Entropy Lam: 5.476369923749309e-06\n",
      "2024-12-22 07:38:44,759 - INFO - Epoch 2820, Total Loss: 7593.0225, Loss U: 806.1230, Loss P: 67.8690, Entropy: 401.1772, Entropy Lam: 5.476369923749309e-06\n",
      "2024-12-22 07:38:44,816 - INFO - Epoch 2830, Total Loss: 7593.0161, Loss U: 806.1406, Loss P: 67.8687, Entropy: 401.0185, Entropy Lam: 5.476369923749309e-06\n",
      "2024-12-22 07:38:44,931 - INFO - Epoch 2840, Total Loss: 7593.0322, Loss U: 806.1016, Loss P: 67.8693, Entropy: 400.8463, Entropy Lam: 5.476369923749309e-06\n",
      "2024-12-22 07:38:44,985 - INFO - Epoch 2850, Total Loss: 7593.0581, Loss U: 806.0840, Loss P: 67.8697, Entropy: 400.6244, Entropy Lam: 5.476369923749309e-06\n",
      "2024-12-22 07:38:45,047 - INFO - Epoch 2860, Total Loss: 7593.0312, Loss U: 806.1797, Loss P: 67.8685, Entropy: 400.4711, Entropy Lam: 6.024006916124241e-06\n",
      "2024-12-22 07:38:45,102 - INFO - Epoch 2870, Total Loss: 7593.0254, Loss U: 806.1309, Loss P: 67.8689, Entropy: 400.3454, Entropy Lam: 6.024006916124241e-06\n",
      "2024-12-22 07:38:45,155 - INFO - Epoch 2880, Total Loss: 7593.0151, Loss U: 806.1680, Loss P: 67.8684, Entropy: 400.1869, Entropy Lam: 6.024006916124241e-06\n",
      "2024-12-22 07:38:45,210 - INFO - Epoch 2890, Total Loss: 7593.0454, Loss U: 806.1367, Loss P: 67.8691, Entropy: 400.0230, Entropy Lam: 6.024006916124241e-06\n",
      "2024-12-22 07:38:45,258 - INFO - Epoch 2900, Total Loss: 7593.0391, Loss U: 806.0859, Loss P: 67.8695, Entropy: 399.8358, Entropy Lam: 6.024006916124241e-06\n",
      "2024-12-22 07:38:45,309 - INFO - Epoch 2910, Total Loss: 7593.0283, Loss U: 806.1699, Loss P: 67.8686, Entropy: 399.6903, Entropy Lam: 6.626407607736665e-06\n",
      "2024-12-22 07:38:45,351 - INFO - Epoch 2920, Total Loss: 7593.0156, Loss U: 806.1602, Loss P: 67.8685, Entropy: 399.5610, Entropy Lam: 6.626407607736665e-06\n",
      "2024-12-22 07:38:45,392 - INFO - Epoch 2930, Total Loss: 7593.0186, Loss U: 806.1523, Loss P: 67.8686, Entropy: 399.4081, Entropy Lam: 6.626407607736665e-06\n",
      "2024-12-22 07:38:45,434 - INFO - Epoch 2940, Total Loss: 7593.0518, Loss U: 806.1348, Loss P: 67.8691, Entropy: 399.2476, Entropy Lam: 7.289048368510332e-06\n",
      "2024-12-22 07:38:45,475 - INFO - Epoch 2950, Total Loss: 7593.0425, Loss U: 806.1270, Loss P: 67.8691, Entropy: 399.0620, Entropy Lam: 7.289048368510332e-06\n",
      "2024-12-22 07:38:45,538 - INFO - Epoch 2960, Total Loss: 7593.0347, Loss U: 806.1602, Loss P: 67.8687, Entropy: 398.9204, Entropy Lam: 8.017953205361366e-06\n",
      "2024-12-22 07:38:45,584 - INFO - Epoch 2970, Total Loss: 7593.0239, Loss U: 806.1445, Loss P: 67.8688, Entropy: 398.7739, Entropy Lam: 8.017953205361366e-06\n",
      "2024-12-22 07:38:45,627 - INFO - Epoch 2980, Total Loss: 7593.0190, Loss U: 806.1406, Loss P: 67.8688, Entropy: 398.6319, Entropy Lam: 8.017953205361366e-06\n",
      "2024-12-22 07:38:45,671 - INFO - Epoch 2990, Total Loss: 7593.0132, Loss U: 806.1172, Loss P: 67.8689, Entropy: 398.4867, Entropy Lam: 8.819748525897503e-06\n",
      "2024-12-22 07:38:45,721 - INFO - Epoch 3000, Total Loss: 7593.0186, Loss U: 806.0918, Loss P: 67.8692, Entropy: 398.3347, Entropy Lam: 8.819748525897503e-06\n",
      "2024-12-22 07:38:45,773 - INFO - Epoch 3010, Total Loss: 7593.0508, Loss U: 806.0449, Loss P: 67.8700, Entropy: 398.1591, Entropy Lam: 9.701723378487254e-06\n",
      "2024-12-22 07:38:45,827 - INFO - Epoch 3020, Total Loss: 7593.0259, Loss U: 806.2500, Loss P: 67.8677, Entropy: 397.9938, Entropy Lam: 9.701723378487254e-06\n",
      "2024-12-22 07:38:45,882 - INFO - Epoch 3030, Total Loss: 7593.0396, Loss U: 806.2812, Loss P: 67.8675, Entropy: 397.8266, Entropy Lam: 9.701723378487254e-06\n",
      "2024-12-22 07:38:45,929 - INFO - Epoch 3040, Total Loss: 7593.0142, Loss U: 806.1445, Loss P: 67.8687, Entropy: 397.6656, Entropy Lam: 1.067189571633598e-05\n",
      "2024-12-22 07:38:45,976 - INFO - Epoch 3050, Total Loss: 7593.0229, Loss U: 806.1191, Loss P: 67.8690, Entropy: 397.5499, Entropy Lam: 1.067189571633598e-05\n",
      "2024-12-22 07:38:46,024 - INFO - Epoch 3060, Total Loss: 7593.0273, Loss U: 806.1738, Loss P: 67.8685, Entropy: 397.4342, Entropy Lam: 1.067189571633598e-05\n",
      "2024-12-22 07:38:46,073 - INFO - Epoch 3070, Total Loss: 7593.0249, Loss U: 806.1758, Loss P: 67.8684, Entropy: 397.3095, Entropy Lam: 1.067189571633598e-05\n",
      "2024-12-22 07:38:46,118 - INFO - Epoch 3080, Total Loss: 7593.0249, Loss U: 806.1387, Loss P: 67.8688, Entropy: 397.1481, Entropy Lam: 1.1739085287969579e-05\n",
      "2024-12-22 07:38:46,168 - INFO - Epoch 3090, Total Loss: 7593.0483, Loss U: 806.1738, Loss P: 67.8687, Entropy: 396.9448, Entropy Lam: 1.1739085287969579e-05\n",
      "2024-12-22 07:38:46,212 - INFO - Epoch 3100, Total Loss: 7593.0225, Loss U: 806.1582, Loss P: 67.8686, Entropy: 396.8248, Entropy Lam: 1.2912993816766538e-05\n",
      "2024-12-22 07:38:46,271 - INFO - Epoch 3110, Total Loss: 7593.0142, Loss U: 806.1777, Loss P: 67.8683, Entropy: 396.6982, Entropy Lam: 1.2912993816766538e-05\n",
      "2024-12-22 07:38:46,322 - INFO - Epoch 3120, Total Loss: 7593.0078, Loss U: 806.1445, Loss P: 67.8686, Entropy: 396.5549, Entropy Lam: 1.2912993816766538e-05\n",
      "2024-12-22 07:38:46,376 - INFO - Epoch 3130, Total Loss: 7593.0078, Loss U: 806.1484, Loss P: 67.8685, Entropy: 396.3932, Entropy Lam: 1.2912993816766538e-05\n",
      "2024-12-22 07:38:46,429 - INFO - Epoch 3140, Total Loss: 7593.0225, Loss U: 806.1738, Loss P: 67.8684, Entropy: 396.2313, Entropy Lam: 1.2912993816766538e-05\n",
      "2024-12-22 07:38:46,481 - INFO - Epoch 3150, Total Loss: 7593.0513, Loss U: 806.1582, Loss P: 67.8689, Entropy: 396.0832, Entropy Lam: 1.4204293198443194e-05\n",
      "2024-12-22 07:38:46,526 - INFO - Epoch 3160, Total Loss: 7593.0425, Loss U: 806.2051, Loss P: 67.8683, Entropy: 395.9016, Entropy Lam: 1.4204293198443194e-05\n",
      "2024-12-22 07:38:46,568 - INFO - Epoch 3170, Total Loss: 7593.0234, Loss U: 806.2500, Loss P: 67.8677, Entropy: 395.7166, Entropy Lam: 1.4204293198443194e-05\n",
      "2024-12-22 07:38:46,618 - INFO - Epoch 3180, Total Loss: 7593.0093, Loss U: 806.1738, Loss P: 67.8683, Entropy: 395.5633, Entropy Lam: 1.4204293198443194e-05\n",
      "2024-12-22 07:38:46,665 - INFO - Epoch 3190, Total Loss: 7593.0190, Loss U: 806.1602, Loss P: 67.8685, Entropy: 395.4353, Entropy Lam: 1.4204293198443194e-05\n",
      "2024-12-22 07:38:46,713 - INFO - Epoch 3200, Total Loss: 7593.0063, Loss U: 806.1797, Loss P: 67.8682, Entropy: 395.3038, Entropy Lam: 1.5624722518287514e-05\n",
      "2024-12-22 07:38:46,756 - INFO - Epoch 3210, Total Loss: 7593.0078, Loss U: 806.1484, Loss P: 67.8685, Entropy: 395.1629, Entropy Lam: 1.5624722518287514e-05\n",
      "2024-12-22 07:38:46,824 - INFO - Epoch 3220, Total Loss: 7593.0288, Loss U: 806.0801, Loss P: 67.8694, Entropy: 395.0053, Entropy Lam: 1.5624722518287514e-05\n",
      "2024-12-22 07:38:46,869 - INFO - Epoch 3230, Total Loss: 7593.0293, Loss U: 806.0840, Loss P: 67.8694, Entropy: 394.8088, Entropy Lam: 1.7187194770116268e-05\n",
      "2024-12-22 07:38:46,918 - INFO - Epoch 3240, Total Loss: 7593.0166, Loss U: 806.2148, Loss P: 67.8680, Entropy: 394.6830, Entropy Lam: 1.7187194770116268e-05\n",
      "2024-12-22 07:38:46,967 - INFO - Epoch 3250, Total Loss: 7593.0098, Loss U: 806.1582, Loss P: 67.8684, Entropy: 394.5760, Entropy Lam: 1.7187194770116268e-05\n",
      "2024-12-22 07:38:47,020 - INFO - Epoch 3260, Total Loss: 7593.0063, Loss U: 806.1914, Loss P: 67.8681, Entropy: 394.4518, Entropy Lam: 1.7187194770116268e-05\n",
      "2024-12-22 07:38:47,069 - INFO - Epoch 3270, Total Loss: 7593.0225, Loss U: 806.2070, Loss P: 67.8681, Entropy: 394.3000, Entropy Lam: 1.7187194770116268e-05\n",
      "2024-12-22 07:38:47,122 - INFO - Epoch 3280, Total Loss: 7593.0278, Loss U: 806.2012, Loss P: 67.8682, Entropy: 394.1165, Entropy Lam: 1.8905914247127896e-05\n",
      "2024-12-22 07:38:47,174 - INFO - Epoch 3290, Total Loss: 7593.0127, Loss U: 806.1035, Loss P: 67.8690, Entropy: 393.9920, Entropy Lam: 1.8905914247127896e-05\n",
      "2024-12-22 07:38:47,228 - INFO - Epoch 3300, Total Loss: 7593.0039, Loss U: 806.1602, Loss P: 67.8684, Entropy: 393.8428, Entropy Lam: 1.8905914247127896e-05\n",
      "2024-12-22 07:38:47,287 - INFO - Epoch 3310, Total Loss: 7593.0356, Loss U: 806.1484, Loss P: 67.8688, Entropy: 393.7119, Entropy Lam: 2.0796505671840686e-05\n",
      "2024-12-22 07:38:47,345 - INFO - Epoch 3320, Total Loss: 7593.0200, Loss U: 806.1523, Loss P: 67.8686, Entropy: 393.5976, Entropy Lam: 2.0796505671840686e-05\n",
      "2024-12-22 07:38:47,399 - INFO - Epoch 3330, Total Loss: 7593.0146, Loss U: 806.0938, Loss P: 67.8691, Entropy: 393.4261, Entropy Lam: 2.0796505671840686e-05\n",
      "2024-12-22 07:38:47,449 - INFO - Epoch 3340, Total Loss: 7593.0586, Loss U: 806.0234, Loss P: 67.8703, Entropy: 393.2162, Entropy Lam: 2.0796505671840686e-05\n",
      "2024-12-22 07:38:47,509 - INFO - Epoch 3350, Total Loss: 7593.0220, Loss U: 806.3164, Loss P: 67.8670, Entropy: 393.0231, Entropy Lam: 2.2876156239024755e-05\n",
      "2024-12-22 07:38:47,566 - INFO - Epoch 3360, Total Loss: 7593.0020, Loss U: 806.0684, Loss P: 67.8692, Entropy: 392.8745, Entropy Lam: 2.2876156239024755e-05\n",
      "2024-12-22 07:38:47,618 - INFO - Epoch 3370, Total Loss: 7592.9961, Loss U: 806.1816, Loss P: 67.8681, Entropy: 392.7392, Entropy Lam: 2.2876156239024755e-05\n",
      "2024-12-22 07:38:47,682 - INFO - Epoch 3380, Total Loss: 7593.0146, Loss U: 806.1484, Loss P: 67.8686, Entropy: 392.5935, Entropy Lam: 2.2876156239024755e-05\n",
      "2024-12-22 07:38:47,736 - INFO - Epoch 3390, Total Loss: 7593.0103, Loss U: 806.2031, Loss P: 67.8680, Entropy: 392.4601, Entropy Lam: 2.516377186292723e-05\n",
      "2024-12-22 07:38:47,788 - INFO - Epoch 3400, Total Loss: 7593.0337, Loss U: 806.2031, Loss P: 67.8682, Entropy: 392.3302, Entropy Lam: 2.516377186292723e-05\n",
      "2024-12-22 07:38:47,838 - INFO - Epoch 3410, Total Loss: 7593.0332, Loss U: 806.0859, Loss P: 67.8694, Entropy: 392.1743, Entropy Lam: 2.7680149049219957e-05\n",
      "2024-12-22 07:38:47,948 - INFO - Epoch 3420, Total Loss: 7593.0098, Loss U: 806.1367, Loss P: 67.8686, Entropy: 392.0321, Entropy Lam: 2.7680149049219957e-05\n",
      "2024-12-22 07:38:48,005 - INFO - Epoch 3430, Total Loss: 7593.0093, Loss U: 806.0938, Loss P: 67.8690, Entropy: 391.8994, Entropy Lam: 2.7680149049219957e-05\n",
      "2024-12-22 07:38:48,061 - INFO - Epoch 3440, Total Loss: 7593.0029, Loss U: 806.1270, Loss P: 67.8687, Entropy: 391.7302, Entropy Lam: 2.7680149049219957e-05\n",
      "2024-12-22 07:38:48,124 - INFO - Epoch 3450, Total Loss: 7592.9956, Loss U: 806.1289, Loss P: 67.8686, Entropy: 391.5627, Entropy Lam: 2.7680149049219957e-05\n",
      "2024-12-22 07:38:48,181 - INFO - Epoch 3460, Total Loss: 7593.0103, Loss U: 806.1641, Loss P: 67.8684, Entropy: 391.4057, Entropy Lam: 2.7680149049219957e-05\n",
      "2024-12-22 07:38:48,243 - INFO - Epoch 3470, Total Loss: 7593.0176, Loss U: 806.1758, Loss P: 67.8683, Entropy: 391.2377, Entropy Lam: 3.0448163954141954e-05\n",
      "2024-12-22 07:38:48,292 - INFO - Epoch 3480, Total Loss: 7593.0171, Loss U: 806.2402, Loss P: 67.8677, Entropy: 391.0796, Entropy Lam: 3.0448163954141954e-05\n",
      "2024-12-22 07:38:48,343 - INFO - Epoch 3490, Total Loss: 7593.0283, Loss U: 806.2461, Loss P: 67.8677, Entropy: 390.8769, Entropy Lam: 3.0448163954141954e-05\n",
      "2024-12-22 07:38:48,395 - INFO - Epoch 3500, Total Loss: 7593.0039, Loss U: 806.1035, Loss P: 67.8689, Entropy: 390.7056, Entropy Lam: 3.0448163954141954e-05\n",
      "2024-12-22 07:38:48,454 - INFO - Epoch 3510, Total Loss: 7592.9932, Loss U: 806.1680, Loss P: 67.8681, Entropy: 390.5525, Entropy Lam: 3.0448163954141954e-05\n",
      "2024-12-22 07:38:48,514 - INFO - Epoch 3520, Total Loss: 7593.0068, Loss U: 806.1621, Loss P: 67.8683, Entropy: 390.4009, Entropy Lam: 3.0448163954141954e-05\n",
      "2024-12-22 07:38:48,569 - INFO - Epoch 3530, Total Loss: 7593.0137, Loss U: 806.0938, Loss P: 67.8691, Entropy: 390.2350, Entropy Lam: 3.0448163954141954e-05\n",
      "2024-12-22 07:38:48,649 - INFO - Epoch 3540, Total Loss: 7593.0244, Loss U: 806.1191, Loss P: 67.8689, Entropy: 390.1021, Entropy Lam: 3.349298034955615e-05\n",
      "2024-12-22 07:38:48,717 - INFO - Epoch 3550, Total Loss: 7593.0044, Loss U: 806.0938, Loss P: 67.8690, Entropy: 389.9604, Entropy Lam: 3.349298034955615e-05\n",
      "2024-12-22 07:38:48,773 - INFO - Epoch 3560, Total Loss: 7593.0020, Loss U: 806.1738, Loss P: 67.8681, Entropy: 389.8217, Entropy Lam: 3.349298034955615e-05\n",
      "2024-12-22 07:38:48,823 - INFO - Epoch 3570, Total Loss: 7593.0020, Loss U: 806.1875, Loss P: 67.8680, Entropy: 389.6745, Entropy Lam: 3.349298034955615e-05\n",
      "2024-12-22 07:38:48,874 - INFO - Epoch 3580, Total Loss: 7593.0049, Loss U: 806.1387, Loss P: 67.8685, Entropy: 389.5310, Entropy Lam: 3.684227838451177e-05\n",
      "2024-12-22 07:38:48,923 - INFO - Epoch 3590, Total Loss: 7593.0073, Loss U: 806.1250, Loss P: 67.8687, Entropy: 389.3963, Entropy Lam: 3.684227838451177e-05\n",
      "2024-12-22 07:38:48,972 - INFO - Epoch 3600, Total Loss: 7593.0347, Loss U: 806.1855, Loss P: 67.8683, Entropy: 389.2071, Entropy Lam: 3.684227838451177e-05\n",
      "2024-12-22 07:38:49,022 - INFO - Epoch 3610, Total Loss: 7593.0200, Loss U: 806.2324, Loss P: 67.8677, Entropy: 389.0177, Entropy Lam: 4.052650622296295e-05\n",
      "2024-12-22 07:38:49,068 - INFO - Epoch 3620, Total Loss: 7593.0127, Loss U: 806.2090, Loss P: 67.8679, Entropy: 388.8803, Entropy Lam: 4.052650622296295e-05\n",
      "2024-12-22 07:38:49,112 - INFO - Epoch 3630, Total Loss: 7593.0195, Loss U: 806.2090, Loss P: 67.8680, Entropy: 388.7258, Entropy Lam: 4.052650622296295e-05\n",
      "2024-12-22 07:38:49,161 - INFO - Epoch 3640, Total Loss: 7593.0146, Loss U: 806.2129, Loss P: 67.8679, Entropy: 388.5948, Entropy Lam: 4.052650622296295e-05\n",
      "2024-12-22 07:38:49,208 - INFO - Epoch 3650, Total Loss: 7593.0044, Loss U: 806.2305, Loss P: 67.8676, Entropy: 388.4482, Entropy Lam: 4.457915684525925e-05\n",
      "2024-12-22 07:38:49,261 - INFO - Epoch 3660, Total Loss: 7593.0015, Loss U: 806.2207, Loss P: 67.8676, Entropy: 388.3023, Entropy Lam: 4.457915684525925e-05\n",
      "2024-12-22 07:38:49,313 - INFO - Epoch 3670, Total Loss: 7593.0215, Loss U: 806.2598, Loss P: 67.8674, Entropy: 388.1340, Entropy Lam: 4.457915684525925e-05\n",
      "2024-12-22 07:38:49,361 - INFO - Epoch 3680, Total Loss: 7593.0215, Loss U: 806.1543, Loss P: 67.8685, Entropy: 387.9215, Entropy Lam: 4.9037072529785175e-05\n",
      "2024-12-22 07:38:49,423 - INFO - Epoch 3690, Total Loss: 7593.0210, Loss U: 806.0449, Loss P: 67.8696, Entropy: 387.7461, Entropy Lam: 4.9037072529785175e-05\n",
      "2024-12-22 07:38:49,477 - INFO - Epoch 3700, Total Loss: 7593.0127, Loss U: 806.2598, Loss P: 67.8673, Entropy: 387.6046, Entropy Lam: 4.9037072529785175e-05\n",
      "2024-12-22 07:38:49,582 - INFO - Epoch 3710, Total Loss: 7593.0112, Loss U: 806.1758, Loss P: 67.8682, Entropy: 387.4656, Entropy Lam: 4.9037072529785175e-05\n",
      "2024-12-22 07:38:50,231 - INFO - Epoch 3720, Total Loss: 7593.0029, Loss U: 806.1973, Loss P: 67.8679, Entropy: 387.3355, Entropy Lam: 4.9037072529785175e-05\n",
      "2024-12-22 07:38:50,347 - INFO - Epoch 3730, Total Loss: 7593.0059, Loss U: 806.1895, Loss P: 67.8680, Entropy: 387.1759, Entropy Lam: 4.9037072529785175e-05\n",
      "2024-12-22 07:38:50,456 - INFO - Epoch 3740, Total Loss: 7593.0186, Loss U: 806.2539, Loss P: 67.8675, Entropy: 387.0278, Entropy Lam: 5.39407797827637e-05\n",
      "2024-12-22 07:38:50,549 - INFO - Epoch 3750, Total Loss: 7593.0181, Loss U: 806.2363, Loss P: 67.8676, Entropy: 386.8891, Entropy Lam: 5.39407797827637e-05\n",
      "2024-12-22 07:38:50,643 - INFO - Epoch 3760, Total Loss: 7593.0039, Loss U: 806.1543, Loss P: 67.8683, Entropy: 386.7091, Entropy Lam: 5.39407797827637e-05\n",
      "2024-12-22 07:38:50,770 - INFO - Epoch 3770, Total Loss: 7593.0073, Loss U: 806.1562, Loss P: 67.8683, Entropy: 386.5549, Entropy Lam: 5.39407797827637e-05\n",
      "2024-12-22 07:38:50,884 - INFO - Epoch 3780, Total Loss: 7593.0112, Loss U: 806.1738, Loss P: 67.8682, Entropy: 386.4217, Entropy Lam: 5.39407797827637e-05\n",
      "2024-12-22 07:38:51,000 - INFO - Epoch 3790, Total Loss: 7593.0117, Loss U: 806.2012, Loss P: 67.8679, Entropy: 386.2918, Entropy Lam: 5.933485776104007e-05\n",
      "2024-12-22 07:38:51,086 - INFO - Epoch 3800, Total Loss: 7593.0352, Loss U: 806.1426, Loss P: 67.8687, Entropy: 386.1258, Entropy Lam: 5.933485776104007e-05\n",
      "2024-12-22 07:38:51,168 - INFO - Epoch 3810, Total Loss: 7593.0151, Loss U: 806.1328, Loss P: 67.8686, Entropy: 385.9336, Entropy Lam: 6.526834353714408e-05\n",
      "2024-12-22 07:38:51,250 - INFO - Epoch 3820, Total Loss: 7593.0088, Loss U: 806.2012, Loss P: 67.8678, Entropy: 385.7875, Entropy Lam: 6.526834353714408e-05\n",
      "2024-12-22 07:38:51,338 - INFO - Epoch 3830, Total Loss: 7593.0132, Loss U: 806.2109, Loss P: 67.8678, Entropy: 385.6610, Entropy Lam: 6.526834353714408e-05\n",
      "2024-12-22 07:38:51,396 - INFO - Epoch 3840, Total Loss: 7593.0078, Loss U: 806.1738, Loss P: 67.8681, Entropy: 385.4982, Entropy Lam: 6.526834353714408e-05\n",
      "2024-12-22 07:38:51,463 - INFO - Epoch 3850, Total Loss: 7593.0117, Loss U: 806.1836, Loss P: 67.8680, Entropy: 385.3334, Entropy Lam: 6.526834353714408e-05\n",
      "2024-12-22 07:38:51,528 - INFO - Epoch 3860, Total Loss: 7593.0122, Loss U: 806.2227, Loss P: 67.8676, Entropy: 385.1871, Entropy Lam: 7.17951778908585e-05\n",
      "2024-12-22 07:38:51,591 - INFO - Epoch 3870, Total Loss: 7593.0615, Loss U: 806.0859, Loss P: 67.8695, Entropy: 384.9922, Entropy Lam: 7.17951778908585e-05\n",
      "2024-12-22 07:38:51,650 - INFO - Epoch 3880, Total Loss: 7593.0317, Loss U: 806.1934, Loss P: 67.8681, Entropy: 384.7947, Entropy Lam: 7.897469567994435e-05\n",
      "2024-12-22 07:38:51,711 - INFO - Epoch 3890, Total Loss: 7593.0229, Loss U: 806.1562, Loss P: 67.8684, Entropy: 384.6259, Entropy Lam: 7.897469567994435e-05\n",
      "2024-12-22 07:38:51,778 - INFO - Epoch 3900, Total Loss: 7593.0054, Loss U: 806.2363, Loss P: 67.8674, Entropy: 384.5296, Entropy Lam: 7.897469567994435e-05\n",
      "2024-12-22 07:38:51,847 - INFO - Epoch 3910, Total Loss: 7593.0020, Loss U: 806.1738, Loss P: 67.8680, Entropy: 384.3722, Entropy Lam: 7.897469567994435e-05\n",
      "2024-12-22 07:38:51,910 - INFO - Epoch 3920, Total Loss: 7592.9980, Loss U: 806.2109, Loss P: 67.8676, Entropy: 384.2088, Entropy Lam: 7.897469567994435e-05\n",
      "2024-12-22 07:38:51,994 - INFO - Epoch 3930, Total Loss: 7593.0049, Loss U: 806.1797, Loss P: 67.8680, Entropy: 384.0399, Entropy Lam: 7.897469567994435e-05\n",
      "2024-12-22 07:38:52,056 - INFO - Epoch 3940, Total Loss: 7593.0249, Loss U: 806.1113, Loss P: 67.8688, Entropy: 383.8919, Entropy Lam: 8.687216524793879e-05\n",
      "2024-12-22 07:38:52,109 - INFO - Epoch 3950, Total Loss: 7593.0244, Loss U: 806.1328, Loss P: 67.8686, Entropy: 383.7408, Entropy Lam: 8.687216524793879e-05\n",
      "2024-12-22 07:38:52,162 - INFO - Epoch 3960, Total Loss: 7593.0303, Loss U: 806.1426, Loss P: 67.8685, Entropy: 383.5748, Entropy Lam: 8.687216524793879e-05\n",
      "2024-12-22 07:38:52,220 - INFO - Epoch 3970, Total Loss: 7593.0347, Loss U: 806.0977, Loss P: 67.8690, Entropy: 383.3998, Entropy Lam: 9.555938177273267e-05\n",
      "2024-12-22 07:38:52,270 - INFO - Epoch 3980, Total Loss: 7593.0112, Loss U: 806.1641, Loss P: 67.8681, Entropy: 383.2197, Entropy Lam: 9.555938177273267e-05\n",
      "2024-12-22 07:38:52,329 - INFO - Epoch 3990, Total Loss: 7593.0088, Loss U: 806.2754, Loss P: 67.8670, Entropy: 383.0637, Entropy Lam: 9.555938177273267e-05\n",
      "2024-12-22 07:38:52,384 - INFO - Epoch 4000, Total Loss: 7593.0132, Loss U: 806.2168, Loss P: 67.8676, Entropy: 382.8980, Entropy Lam: 9.555938177273267e-05\n",
      "2024-12-22 07:38:52,440 - INFO - Epoch 4010, Total Loss: 7593.0356, Loss U: 806.2461, Loss P: 67.8675, Entropy: 382.7198, Entropy Lam: 9.555938177273267e-05\n",
      "2024-12-22 07:38:52,494 - INFO - Epoch 4020, Total Loss: 7593.0161, Loss U: 806.2910, Loss P: 67.8669, Entropy: 382.5334, Entropy Lam: 0.00010511531995000595\n",
      "2024-12-22 07:38:52,547 - INFO - Epoch 4030, Total Loss: 7593.0122, Loss U: 806.1914, Loss P: 67.8678, Entropy: 382.3773, Entropy Lam: 0.00010511531995000595\n",
      "2024-12-22 07:38:52,598 - INFO - Epoch 4040, Total Loss: 7593.0166, Loss U: 806.2188, Loss P: 67.8676, Entropy: 382.2119, Entropy Lam: 0.00010511531995000595\n",
      "2024-12-22 07:38:52,655 - INFO - Epoch 4050, Total Loss: 7593.0352, Loss U: 806.2305, Loss P: 67.8676, Entropy: 382.0425, Entropy Lam: 0.00010511531995000595\n",
      "2024-12-22 07:38:52,791 - INFO - Epoch 4060, Total Loss: 7593.0366, Loss U: 806.2480, Loss P: 67.8674, Entropy: 381.8748, Entropy Lam: 0.00011562685194500656\n",
      "2024-12-22 07:38:52,850 - INFO - Epoch 4070, Total Loss: 7593.0210, Loss U: 806.3086, Loss P: 67.8667, Entropy: 381.6951, Entropy Lam: 0.00011562685194500656\n",
      "2024-12-22 07:38:52,900 - INFO - Epoch 4080, Total Loss: 7593.0205, Loss U: 806.2852, Loss P: 67.8669, Entropy: 381.5402, Entropy Lam: 0.00011562685194500656\n",
      "2024-12-22 07:38:52,960 - INFO - Epoch 4090, Total Loss: 7593.0210, Loss U: 806.2988, Loss P: 67.8668, Entropy: 381.3609, Entropy Lam: 0.00011562685194500656\n",
      "2024-12-22 07:38:53,017 - INFO - Epoch 4100, Total Loss: 7593.0713, Loss U: 806.4180, Loss P: 67.8661, Entropy: 381.1265, Entropy Lam: 0.00012718953713950723\n",
      "2024-12-22 07:38:53,105 - INFO - Epoch 4110, Total Loss: 7593.0264, Loss U: 806.1914, Loss P: 67.8679, Entropy: 380.9080, Entropy Lam: 0.00012718953713950723\n",
      "2024-12-22 07:38:53,169 - INFO - Epoch 4120, Total Loss: 7593.0103, Loss U: 806.2734, Loss P: 67.8669, Entropy: 380.7271, Entropy Lam: 0.00012718953713950723\n",
      "2024-12-22 07:38:53,240 - INFO - Epoch 4130, Total Loss: 7593.0195, Loss U: 806.2539, Loss P: 67.8672, Entropy: 380.5495, Entropy Lam: 0.00012718953713950723\n",
      "2024-12-22 07:38:53,317 - INFO - Epoch 4140, Total Loss: 7593.0210, Loss U: 806.2676, Loss P: 67.8671, Entropy: 380.3784, Entropy Lam: 0.00012718953713950723\n",
      "2024-12-22 07:38:53,372 - INFO - Epoch 4150, Total Loss: 7593.0347, Loss U: 806.2812, Loss P: 67.8670, Entropy: 380.1781, Entropy Lam: 0.00013990849085345798\n",
      "2024-12-22 07:38:53,438 - INFO - Epoch 4160, Total Loss: 7593.0576, Loss U: 806.2500, Loss P: 67.8675, Entropy: 379.9167, Entropy Lam: 0.00013990849085345798\n",
      "2024-12-22 07:38:53,497 - INFO - Epoch 4170, Total Loss: 7593.0444, Loss U: 806.3008, Loss P: 67.8669, Entropy: 379.6970, Entropy Lam: 0.0001538993399388038\n",
      "2024-12-22 07:38:53,564 - INFO - Epoch 4180, Total Loss: 7593.0254, Loss U: 806.2129, Loss P: 67.8675, Entropy: 379.4734, Entropy Lam: 0.0001538993399388038\n",
      "2024-12-22 07:38:53,625 - INFO - Epoch 4190, Total Loss: 7593.0269, Loss U: 806.2871, Loss P: 67.8668, Entropy: 379.2797, Entropy Lam: 0.0001538993399388038\n",
      "2024-12-22 07:38:53,693 - INFO - Epoch 4200, Total Loss: 7593.0532, Loss U: 806.2734, Loss P: 67.8672, Entropy: 379.0713, Entropy Lam: 0.0001538993399388038\n",
      "2024-12-22 07:38:53,771 - INFO - Epoch 4210, Total Loss: 7593.0771, Loss U: 806.3320, Loss P: 67.8668, Entropy: 378.8907, Entropy Lam: 0.0001692892739326842\n",
      "2024-12-22 07:38:53,838 - INFO - Epoch 4220, Total Loss: 7593.0405, Loss U: 806.2988, Loss P: 67.8668, Entropy: 378.6953, Entropy Lam: 0.0001692892739326842\n",
      "2024-12-22 07:38:53,910 - INFO - Epoch 4230, Total Loss: 7593.0269, Loss U: 806.2168, Loss P: 67.8675, Entropy: 378.5203, Entropy Lam: 0.0001692892739326842\n",
      "2024-12-22 07:38:53,975 - INFO - Epoch 4240, Total Loss: 7593.0542, Loss U: 806.1816, Loss P: 67.8681, Entropy: 378.3174, Entropy Lam: 0.0001692892739326842\n",
      "2024-12-22 07:38:54,053 - INFO - Epoch 4250, Total Loss: 7593.0498, Loss U: 806.2520, Loss P: 67.8673, Entropy: 378.0525, Entropy Lam: 0.0001692892739326842\n",
      "2024-12-22 07:38:54,224 - INFO - Epoch 4260, Total Loss: 7593.0430, Loss U: 806.3438, Loss P: 67.8663, Entropy: 377.8397, Entropy Lam: 0.00018621820132595264\n",
      "2024-12-22 07:38:54,291 - INFO - Epoch 4270, Total Loss: 7593.0337, Loss U: 806.2734, Loss P: 67.8669, Entropy: 377.6243, Entropy Lam: 0.00018621820132595264\n",
      "2024-12-22 07:38:54,365 - INFO - Epoch 4280, Total Loss: 7593.0425, Loss U: 806.3066, Loss P: 67.8667, Entropy: 377.4081, Entropy Lam: 0.00018621820132595264\n",
      "2024-12-22 07:38:54,418 - INFO - Epoch 4290, Total Loss: 7593.0732, Loss U: 806.3984, Loss P: 67.8660, Entropy: 377.1697, Entropy Lam: 0.00018621820132595264\n",
      "2024-12-22 07:38:54,471 - INFO - Epoch 4300, Total Loss: 7593.0752, Loss U: 806.2520, Loss P: 67.8675, Entropy: 376.9582, Entropy Lam: 0.0002048400214585479\n",
      "2024-12-22 07:38:54,522 - INFO - Epoch 4310, Total Loss: 7593.0527, Loss U: 806.2754, Loss P: 67.8670, Entropy: 376.7560, Entropy Lam: 0.0002048400214585479\n",
      "2024-12-22 07:38:54,574 - INFO - Epoch 4320, Total Loss: 7593.0381, Loss U: 806.2812, Loss P: 67.8668, Entropy: 376.5246, Entropy Lam: 0.0002048400214585479\n",
      "2024-12-22 07:38:54,622 - INFO - Epoch 4330, Total Loss: 7593.0571, Loss U: 806.3535, Loss P: 67.8663, Entropy: 376.2875, Entropy Lam: 0.0002048400214585479\n",
      "2024-12-22 07:38:54,678 - INFO - Epoch 4340, Total Loss: 7593.0518, Loss U: 806.3477, Loss P: 67.8663, Entropy: 376.0303, Entropy Lam: 0.00022532402360440272\n",
      "2024-12-22 07:38:54,743 - INFO - Epoch 4350, Total Loss: 7593.0747, Loss U: 806.2324, Loss P: 67.8676, Entropy: 375.7850, Entropy Lam: 0.00022532402360440272\n",
      "2024-12-22 07:38:54,808 - INFO - Epoch 4360, Total Loss: 7593.0654, Loss U: 806.3262, Loss P: 67.8665, Entropy: 375.5784, Entropy Lam: 0.00022532402360440272\n",
      "2024-12-22 07:38:54,875 - INFO - Epoch 4370, Total Loss: 7593.0640, Loss U: 806.2617, Loss P: 67.8671, Entropy: 375.3780, Entropy Lam: 0.000247856425964843\n",
      "2024-12-22 07:38:54,939 - INFO - Epoch 4380, Total Loss: 7593.0620, Loss U: 806.2949, Loss P: 67.8667, Entropy: 375.1509, Entropy Lam: 0.000247856425964843\n",
      "2024-12-22 07:38:55,002 - INFO - Epoch 4390, Total Loss: 7593.0640, Loss U: 806.3301, Loss P: 67.8664, Entropy: 374.9055, Entropy Lam: 0.000247856425964843\n",
      "2024-12-22 07:38:55,065 - INFO - Epoch 4400, Total Loss: 7593.0796, Loss U: 806.3047, Loss P: 67.8667, Entropy: 374.6558, Entropy Lam: 0.0002726420685613273\n",
      "2024-12-22 07:38:55,121 - INFO - Epoch 4410, Total Loss: 7593.0737, Loss U: 806.3027, Loss P: 67.8667, Entropy: 374.3715, Entropy Lam: 0.0002726420685613273\n",
      "2024-12-22 07:38:55,184 - INFO - Epoch 4420, Total Loss: 7593.0864, Loss U: 806.3691, Loss P: 67.8661, Entropy: 374.0951, Entropy Lam: 0.0002726420685613273\n",
      "2024-12-22 07:38:55,250 - INFO - Epoch 4430, Total Loss: 7593.0845, Loss U: 806.3555, Loss P: 67.8663, Entropy: 373.8689, Entropy Lam: 0.0002726420685613273\n",
      "2024-12-22 07:38:55,330 - INFO - Epoch 4440, Total Loss: 7593.0815, Loss U: 806.3301, Loss P: 67.8664, Entropy: 373.6516, Entropy Lam: 0.00029990627541746006\n",
      "2024-12-22 07:38:55,391 - INFO - Epoch 4450, Total Loss: 7593.0864, Loss U: 806.3125, Loss P: 67.8666, Entropy: 373.4052, Entropy Lam: 0.00029990627541746006\n",
      "2024-12-22 07:38:55,463 - INFO - Epoch 4460, Total Loss: 7593.0752, Loss U: 806.3223, Loss P: 67.8664, Entropy: 373.1651, Entropy Lam: 0.00029990627541746006\n",
      "2024-12-22 07:38:55,535 - INFO - Epoch 4470, Total Loss: 7593.0825, Loss U: 806.2910, Loss P: 67.8668, Entropy: 372.8989, Entropy Lam: 0.00029990627541746006\n",
      "2024-12-22 07:38:55,608 - INFO - Epoch 4480, Total Loss: 7593.0938, Loss U: 806.3770, Loss P: 67.8661, Entropy: 372.6204, Entropy Lam: 0.00029990627541746006\n",
      "2024-12-22 07:38:55,671 - INFO - Epoch 4490, Total Loss: 7593.1172, Loss U: 806.4121, Loss P: 67.8658, Entropy: 372.3278, Entropy Lam: 0.0003298969029592061\n",
      "2024-12-22 07:38:55,729 - INFO - Epoch 4500, Total Loss: 7593.0972, Loss U: 806.3145, Loss P: 67.8666, Entropy: 372.0760, Entropy Lam: 0.0003298969029592061\n",
      "2024-12-22 07:38:55,780 - INFO - Epoch 4510, Total Loss: 7593.0815, Loss U: 806.3125, Loss P: 67.8665, Entropy: 371.8197, Entropy Lam: 0.0003298969029592061\n",
      "2024-12-22 07:38:55,840 - INFO - Epoch 4520, Total Loss: 7593.0762, Loss U: 806.2637, Loss P: 67.8669, Entropy: 371.5384, Entropy Lam: 0.0003298969029592061\n",
      "2024-12-22 07:38:55,912 - INFO - Epoch 4530, Total Loss: 7593.0825, Loss U: 806.2129, Loss P: 67.8675, Entropy: 371.2494, Entropy Lam: 0.0003298969029592061\n",
      "2024-12-22 07:38:55,981 - INFO - Epoch 4540, Total Loss: 7593.1636, Loss U: 806.2148, Loss P: 67.8681, Entropy: 370.9425, Entropy Lam: 0.00036288659325512675\n",
      "2024-12-22 07:38:56,060 - INFO - Epoch 4550, Total Loss: 7593.1094, Loss U: 806.4570, Loss P: 67.8652, Entropy: 370.6290, Entropy Lam: 0.00036288659325512675\n",
      "2024-12-22 07:38:56,143 - INFO - Epoch 4560, Total Loss: 7593.0981, Loss U: 806.2617, Loss P: 67.8670, Entropy: 370.3451, Entropy Lam: 0.00036288659325512675\n",
      "2024-12-22 07:38:56,228 - INFO - Epoch 4570, Total Loss: 7593.1045, Loss U: 806.3262, Loss P: 67.8664, Entropy: 370.0435, Entropy Lam: 0.00036288659325512675\n",
      "2024-12-22 07:38:56,308 - INFO - Epoch 4580, Total Loss: 7593.1421, Loss U: 806.3516, Loss P: 67.8666, Entropy: 369.7580, Entropy Lam: 0.00036288659325512675\n",
      "2024-12-22 07:38:56,391 - INFO - Epoch 4590, Total Loss: 7593.1367, Loss U: 806.3027, Loss P: 67.8669, Entropy: 369.4942, Entropy Lam: 0.00039917525258063944\n",
      "2024-12-22 07:38:56,461 - INFO - Epoch 4600, Total Loss: 7593.1255, Loss U: 806.2891, Loss P: 67.8669, Entropy: 369.1679, Entropy Lam: 0.00039917525258063944\n",
      "2024-12-22 07:38:56,514 - INFO - Epoch 4610, Total Loss: 7593.1353, Loss U: 806.3652, Loss P: 67.8662, Entropy: 368.8695, Entropy Lam: 0.00039917525258063944\n",
      "2024-12-22 07:38:56,582 - INFO - Epoch 4620, Total Loss: 7593.1108, Loss U: 806.3613, Loss P: 67.8660, Entropy: 368.5352, Entropy Lam: 0.00039917525258063944\n",
      "2024-12-22 07:38:56,630 - INFO - Epoch 4630, Total Loss: 7593.0942, Loss U: 806.2812, Loss P: 67.8667, Entropy: 368.2083, Entropy Lam: 0.00039917525258063944\n",
      "2024-12-22 07:38:56,681 - INFO - Epoch 4640, Total Loss: 7593.0967, Loss U: 806.2910, Loss P: 67.8666, Entropy: 367.9055, Entropy Lam: 0.00039917525258063944\n",
      "2024-12-22 07:38:56,728 - INFO - Epoch 4650, Total Loss: 7593.1040, Loss U: 806.3125, Loss P: 67.8664, Entropy: 367.5891, Entropy Lam: 0.00039917525258063944\n",
      "2024-12-22 07:38:56,780 - INFO - Epoch 4660, Total Loss: 7593.1001, Loss U: 806.2754, Loss P: 67.8668, Entropy: 367.2564, Entropy Lam: 0.00039917525258063944\n",
      "2024-12-22 07:38:56,832 - INFO - Epoch 4670, Total Loss: 7593.1572, Loss U: 806.2461, Loss P: 67.8675, Entropy: 366.8984, Entropy Lam: 0.0004390927778387034\n",
      "2024-12-22 07:38:56,891 - INFO - Epoch 4680, Total Loss: 7593.1787, Loss U: 806.4102, Loss P: 67.8661, Entropy: 366.5578, Entropy Lam: 0.0004390927778387034\n",
      "2024-12-22 07:38:56,943 - INFO - Epoch 4690, Total Loss: 7593.1470, Loss U: 806.4141, Loss P: 67.8657, Entropy: 366.2842, Entropy Lam: 0.0004390927778387034\n",
      "2024-12-22 07:38:57,013 - INFO - Epoch 4700, Total Loss: 7593.1333, Loss U: 806.3203, Loss P: 67.8665, Entropy: 365.9965, Entropy Lam: 0.0004390927778387034\n",
      "2024-12-22 07:38:57,083 - INFO - Epoch 4710, Total Loss: 7593.1387, Loss U: 806.3281, Loss P: 67.8665, Entropy: 365.6620, Entropy Lam: 0.0004390927778387034\n",
      "2024-12-22 07:38:57,141 - INFO - Epoch 4720, Total Loss: 7593.1191, Loss U: 806.3809, Loss P: 67.8658, Entropy: 365.2840, Entropy Lam: 0.0004390927778387034\n",
      "2024-12-22 07:38:57,189 - INFO - Epoch 4730, Total Loss: 7593.1396, Loss U: 806.3711, Loss P: 67.8659, Entropy: 364.9265, Entropy Lam: 0.0004830020556225738\n",
      "2024-12-22 07:38:57,233 - INFO - Epoch 4740, Total Loss: 7593.1401, Loss U: 806.3184, Loss P: 67.8665, Entropy: 364.5689, Entropy Lam: 0.0004830020556225738\n",
      "2024-12-22 07:38:57,283 - INFO - Epoch 4750, Total Loss: 7593.1602, Loss U: 806.3047, Loss P: 67.8666, Entropy: 364.2048, Entropy Lam: 0.0005313022611848312\n",
      "2024-12-22 07:38:57,337 - INFO - Epoch 4760, Total Loss: 7593.1729, Loss U: 806.2773, Loss P: 67.8670, Entropy: 363.8381, Entropy Lam: 0.0005313022611848312\n",
      "2024-12-22 07:38:57,466 - INFO - Epoch 4770, Total Loss: 7593.1665, Loss U: 806.3359, Loss P: 67.8664, Entropy: 363.4794, Entropy Lam: 0.0005313022611848312\n",
      "2024-12-22 07:38:57,519 - INFO - Epoch 4780, Total Loss: 7593.1904, Loss U: 806.3770, Loss P: 67.8660, Entropy: 363.0558, Entropy Lam: 0.0005844324873033144\n",
      "2024-12-22 07:38:57,578 - INFO - Epoch 4790, Total Loss: 7593.1821, Loss U: 806.3867, Loss P: 67.8658, Entropy: 362.6058, Entropy Lam: 0.0005844324873033144\n",
      "2024-12-22 07:38:57,638 - INFO - Epoch 4800, Total Loss: 7593.1777, Loss U: 806.3555, Loss P: 67.8661, Entropy: 362.1891, Entropy Lam: 0.0005844324873033144\n",
      "2024-12-22 07:38:57,705 - INFO - Epoch 4810, Total Loss: 7593.2065, Loss U: 806.4609, Loss P: 67.8653, Entropy: 361.7536, Entropy Lam: 0.0005844324873033144\n",
      "2024-12-22 07:38:57,765 - INFO - Epoch 4820, Total Loss: 7593.1885, Loss U: 806.3242, Loss P: 67.8663, Entropy: 361.3258, Entropy Lam: 0.0006428757360336459\n",
      "2024-12-22 07:38:57,823 - INFO - Epoch 4830, Total Loss: 7593.1919, Loss U: 806.2930, Loss P: 67.8667, Entropy: 360.8807, Entropy Lam: 0.0006428757360336459\n",
      "2024-12-22 07:38:57,872 - INFO - Epoch 4840, Total Loss: 7593.2026, Loss U: 806.3457, Loss P: 67.8662, Entropy: 360.4250, Entropy Lam: 0.0006428757360336459\n",
      "2024-12-22 07:38:57,935 - INFO - Epoch 4850, Total Loss: 7593.2246, Loss U: 806.3203, Loss P: 67.8665, Entropy: 359.9330, Entropy Lam: 0.0007071633096370106\n",
      "2024-12-22 07:38:57,992 - INFO - Epoch 4860, Total Loss: 7593.2183, Loss U: 806.3320, Loss P: 67.8663, Entropy: 359.4094, Entropy Lam: 0.0007071633096370106\n",
      "2024-12-22 07:38:58,050 - INFO - Epoch 4870, Total Loss: 7593.2583, Loss U: 806.3262, Loss P: 67.8668, Entropy: 358.9484, Entropy Lam: 0.0007071633096370106\n",
      "2024-12-22 07:38:58,116 - INFO - Epoch 4880, Total Loss: 7593.2173, Loss U: 806.3730, Loss P: 67.8659, Entropy: 358.5248, Entropy Lam: 0.0007071633096370106\n",
      "2024-12-22 07:38:58,163 - INFO - Epoch 4890, Total Loss: 7593.2432, Loss U: 806.3691, Loss P: 67.8660, Entropy: 358.0025, Entropy Lam: 0.0007778796406007117\n",
      "2024-12-22 07:38:58,214 - INFO - Epoch 4900, Total Loss: 7593.2710, Loss U: 806.4043, Loss P: 67.8659, Entropy: 357.4214, Entropy Lam: 0.0007778796406007117\n",
      "2024-12-22 07:38:58,271 - INFO - Epoch 4910, Total Loss: 7593.3081, Loss U: 806.3438, Loss P: 67.8666, Entropy: 356.7681, Entropy Lam: 0.0008556676046607829\n",
      "2024-12-22 07:38:58,401 - INFO - Epoch 4920, Total Loss: 7593.2637, Loss U: 806.3164, Loss P: 67.8664, Entropy: 356.1891, Entropy Lam: 0.0008556676046607829\n",
      "2024-12-22 07:38:58,472 - INFO - Epoch 4930, Total Loss: 7593.2529, Loss U: 806.2949, Loss P: 67.8665, Entropy: 355.6056, Entropy Lam: 0.0008556676046607829\n",
      "2024-12-22 07:38:58,524 - INFO - Epoch 4940, Total Loss: 7593.2769, Loss U: 806.3262, Loss P: 67.8665, Entropy: 355.0207, Entropy Lam: 0.0008556676046607829\n",
      "2024-12-22 07:38:58,588 - INFO - Epoch 4950, Total Loss: 7593.2754, Loss U: 806.3066, Loss P: 67.8667, Entropy: 354.4579, Entropy Lam: 0.0008556676046607829\n",
      "2024-12-22 07:38:58,658 - INFO - Epoch 4960, Total Loss: 7593.3047, Loss U: 806.2461, Loss P: 67.8673, Entropy: 353.8487, Entropy Lam: 0.0009412343651268612\n",
      "2024-12-22 07:38:58,732 - INFO - Epoch 4970, Total Loss: 7593.3008, Loss U: 806.2695, Loss P: 67.8670, Entropy: 353.2260, Entropy Lam: 0.0009412343651268612\n",
      "2024-12-22 07:38:58,799 - INFO - Epoch 4980, Total Loss: 7593.3032, Loss U: 806.2129, Loss P: 67.8676, Entropy: 352.5966, Entropy Lam: 0.0009412343651268612\n",
      "2024-12-22 07:38:58,858 - INFO - Epoch 4990, Total Loss: 7593.2886, Loss U: 806.2441, Loss P: 67.8671, Entropy: 351.9311, Entropy Lam: 0.0009412343651268612\n",
      "2024-12-22 07:38:58,913 - INFO - Epoch 5000, Total Loss: 7593.3457, Loss U: 806.3340, Loss P: 67.8665, Entropy: 351.2617, Entropy Lam: 0.0010353578016395475\n",
      "2024-12-22 07:38:58,968 - INFO - Epoch 5010, Total Loss: 7593.3311, Loss U: 806.3379, Loss P: 67.8663, Entropy: 350.5782, Entropy Lam: 0.0010353578016395475\n",
      "2024-12-22 07:38:59,028 - INFO - Epoch 5020, Total Loss: 7593.3237, Loss U: 806.3145, Loss P: 67.8665, Entropy: 349.9142, Entropy Lam: 0.0010353578016395475\n",
      "2024-12-22 07:38:59,078 - INFO - Epoch 5030, Total Loss: 7593.3306, Loss U: 806.3906, Loss P: 67.8658, Entropy: 349.2449, Entropy Lam: 0.0010353578016395475\n",
      "2024-12-22 07:38:59,131 - INFO - Epoch 5040, Total Loss: 7593.3848, Loss U: 806.4277, Loss P: 67.8656, Entropy: 348.5518, Entropy Lam: 0.0011388935818035023\n",
      "2024-12-22 07:38:59,270 - INFO - Epoch 5050, Total Loss: 7593.3613, Loss U: 806.3281, Loss P: 67.8664, Entropy: 347.7882, Entropy Lam: 0.0011388935818035023\n",
      "2024-12-22 07:38:59,335 - INFO - Epoch 5060, Total Loss: 7593.3726, Loss U: 806.3887, Loss P: 67.8659, Entropy: 347.0323, Entropy Lam: 0.0011388935818035023\n",
      "2024-12-22 07:38:59,394 - INFO - Epoch 5070, Total Loss: 7593.3462, Loss U: 806.3184, Loss P: 67.8663, Entropy: 346.3192, Entropy Lam: 0.0011388935818035023\n",
      "2024-12-22 07:38:59,487 - INFO - Epoch 5080, Total Loss: 7593.3481, Loss U: 806.3105, Loss P: 67.8664, Entropy: 345.5866, Entropy Lam: 0.0011388935818035023\n",
      "2024-12-22 07:38:59,624 - INFO - Epoch 5090, Total Loss: 7593.3770, Loss U: 806.2969, Loss P: 67.8669, Entropy: 344.8418, Entropy Lam: 0.0011388935818035023\n",
      "2024-12-22 07:39:00,754 - INFO - Epoch 5100, Total Loss: 7593.3691, Loss U: 806.2812, Loss P: 67.8670, Entropy: 344.1387, Entropy Lam: 0.0012527829399838525\n",
      "2024-12-22 07:39:00,951 - INFO - Epoch 5110, Total Loss: 7593.3994, Loss U: 806.2871, Loss P: 67.8668, Entropy: 343.3526, Entropy Lam: 0.0012527829399838525\n",
      "2024-12-22 07:39:01,363 - INFO - Epoch 5120, Total Loss: 7593.3936, Loss U: 806.3770, Loss P: 67.8659, Entropy: 342.5474, Entropy Lam: 0.0012527829399838525\n",
      "2024-12-22 07:39:01,495 - INFO - Epoch 5130, Total Loss: 7593.4043, Loss U: 806.4121, Loss P: 67.8656, Entropy: 341.7094, Entropy Lam: 0.0012527829399838525\n",
      "2024-12-22 07:39:01,727 - INFO - Epoch 5140, Total Loss: 7593.4834, Loss U: 806.4551, Loss P: 67.8656, Entropy: 340.9350, Entropy Lam: 0.001378061233982238\n",
      "2024-12-22 07:39:01,912 - INFO - Epoch 5150, Total Loss: 7593.4678, Loss U: 806.3594, Loss P: 67.8664, Entropy: 340.1231, Entropy Lam: 0.001378061233982238\n",
      "2024-12-22 07:39:02,064 - INFO - Epoch 5160, Total Loss: 7593.4268, Loss U: 806.3789, Loss P: 67.8658, Entropy: 339.3045, Entropy Lam: 0.001378061233982238\n",
      "2024-12-22 07:39:02,299 - INFO - Epoch 5170, Total Loss: 7593.4219, Loss U: 806.3320, Loss P: 67.8662, Entropy: 338.4878, Entropy Lam: 0.001378061233982238\n",
      "2024-12-22 07:39:02,432 - INFO - Epoch 5180, Total Loss: 7593.4111, Loss U: 806.3340, Loss P: 67.8661, Entropy: 337.6129, Entropy Lam: 0.001378061233982238\n",
      "2024-12-22 07:39:02,518 - INFO - Epoch 5190, Total Loss: 7593.4121, Loss U: 806.3301, Loss P: 67.8662, Entropy: 336.7302, Entropy Lam: 0.001378061233982238\n",
      "2024-12-22 07:39:02,613 - INFO - Epoch 5200, Total Loss: 7593.4609, Loss U: 806.2559, Loss P: 67.8674, Entropy: 335.8559, Entropy Lam: 0.001378061233982238\n",
      "2024-12-22 07:39:02,722 - INFO - Epoch 5210, Total Loss: 7593.4668, Loss U: 806.3828, Loss P: 67.8658, Entropy: 335.0096, Entropy Lam: 0.001515867357380462\n",
      "2024-12-22 07:39:02,834 - INFO - Epoch 5220, Total Loss: 7593.4653, Loss U: 806.3164, Loss P: 67.8664, Entropy: 334.1265, Entropy Lam: 0.001515867357380462\n",
      "2024-12-22 07:39:02,930 - INFO - Epoch 5230, Total Loss: 7593.4541, Loss U: 806.3828, Loss P: 67.8657, Entropy: 333.2256, Entropy Lam: 0.001515867357380462\n",
      "2024-12-22 07:39:03,199 - INFO - Epoch 5240, Total Loss: 7593.4819, Loss U: 806.4883, Loss P: 67.8649, Entropy: 332.3033, Entropy Lam: 0.001515867357380462\n",
      "2024-12-22 07:39:03,399 - INFO - Epoch 5250, Total Loss: 7593.4790, Loss U: 806.4707, Loss P: 67.8651, Entropy: 331.3326, Entropy Lam: 0.001515867357380462\n",
      "2024-12-22 07:39:03,583 - INFO - Epoch 5260, Total Loss: 7593.5107, Loss U: 806.4258, Loss P: 67.8653, Entropy: 330.3662, Entropy Lam: 0.0016674540931185084\n",
      "2024-12-22 07:39:03,736 - INFO - Epoch 5270, Total Loss: 7593.5088, Loss U: 806.3184, Loss P: 67.8664, Entropy: 329.4001, Entropy Lam: 0.0016674540931185084\n",
      "2024-12-22 07:39:03,828 - INFO - Epoch 5280, Total Loss: 7593.5034, Loss U: 806.3848, Loss P: 67.8657, Entropy: 328.4029, Entropy Lam: 0.0016674540931185084\n",
      "2024-12-22 07:39:03,937 - INFO - Epoch 5290, Total Loss: 7593.5894, Loss U: 806.2852, Loss P: 67.8670, Entropy: 327.3957, Entropy Lam: 0.0018341995024303594\n",
      "2024-12-22 07:39:04,090 - INFO - Epoch 5300, Total Loss: 7593.5659, Loss U: 806.3828, Loss P: 67.8658, Entropy: 326.3125, Entropy Lam: 0.0018341995024303594\n",
      "2024-12-22 07:39:04,183 - INFO - Epoch 5310, Total Loss: 7593.5498, Loss U: 806.3594, Loss P: 67.8659, Entropy: 325.2228, Entropy Lam: 0.0018341995024303594\n",
      "2024-12-22 07:39:04,275 - INFO - Epoch 5320, Total Loss: 7593.5752, Loss U: 806.3516, Loss P: 67.8663, Entropy: 324.1405, Entropy Lam: 0.0018341995024303594\n",
      "2024-12-22 07:39:04,381 - INFO - Epoch 5330, Total Loss: 7593.6108, Loss U: 806.3652, Loss P: 67.8665, Entropy: 323.0189, Entropy Lam: 0.0018341995024303594\n",
      "2024-12-22 07:39:04,453 - INFO - Epoch 5340, Total Loss: 7593.6133, Loss U: 806.4551, Loss P: 67.8651, Entropy: 322.0361, Entropy Lam: 0.0020176194526733953\n",
      "2024-12-22 07:39:04,510 - INFO - Epoch 5350, Total Loss: 7593.6079, Loss U: 806.3242, Loss P: 67.8664, Entropy: 320.9138, Entropy Lam: 0.0020176194526733953\n",
      "2024-12-22 07:39:04,589 - INFO - Epoch 5360, Total Loss: 7593.5869, Loss U: 806.3828, Loss P: 67.8656, Entropy: 319.7563, Entropy Lam: 0.0020176194526733953\n",
      "2024-12-22 07:39:04,770 - INFO - Epoch 5370, Total Loss: 7593.5864, Loss U: 806.4043, Loss P: 67.8654, Entropy: 318.5615, Entropy Lam: 0.0020176194526733953\n",
      "2024-12-22 07:39:05,768 - INFO - Epoch 5380, Total Loss: 7593.5879, Loss U: 806.3633, Loss P: 67.8658, Entropy: 317.3583, Entropy Lam: 0.0020176194526733953\n",
      "2024-12-22 07:39:05,898 - INFO - Epoch 5390, Total Loss: 7593.7803, Loss U: 806.4707, Loss P: 67.8667, Entropy: 316.1954, Entropy Lam: 0.0020176194526733953\n",
      "2024-12-22 07:39:05,976 - INFO - Epoch 5400, Total Loss: 7593.6929, Loss U: 806.4258, Loss P: 67.8657, Entropy: 315.0412, Entropy Lam: 0.002219381397940735\n",
      "2024-12-22 07:39:06,065 - INFO - Epoch 5410, Total Loss: 7593.6582, Loss U: 806.3906, Loss P: 67.8657, Entropy: 313.8951, Entropy Lam: 0.002219381397940735\n",
      "2024-12-22 07:39:06,162 - INFO - Epoch 5420, Total Loss: 7593.6489, Loss U: 806.3691, Loss P: 67.8659, Entropy: 312.7148, Entropy Lam: 0.002219381397940735\n",
      "2024-12-22 07:39:06,244 - INFO - Epoch 5430, Total Loss: 7593.6323, Loss U: 806.3906, Loss P: 67.8655, Entropy: 311.4622, Entropy Lam: 0.002219381397940735\n",
      "2024-12-22 07:39:06,319 - INFO - Epoch 5440, Total Loss: 7593.6382, Loss U: 806.3750, Loss P: 67.8657, Entropy: 310.1834, Entropy Lam: 0.002219381397940735\n",
      "2024-12-22 07:39:06,392 - INFO - Epoch 5450, Total Loss: 7593.6724, Loss U: 806.3438, Loss P: 67.8664, Entropy: 308.9014, Entropy Lam: 0.002219381397940735\n",
      "2024-12-22 07:39:06,458 - INFO - Epoch 5460, Total Loss: 7593.7305, Loss U: 806.2949, Loss P: 67.8668, Entropy: 307.5796, Entropy Lam: 0.0024413195377348085\n",
      "2024-12-22 07:39:06,523 - INFO - Epoch 5470, Total Loss: 7593.7129, Loss U: 806.3770, Loss P: 67.8659, Entropy: 306.1894, Entropy Lam: 0.0024413195377348085\n",
      "2024-12-22 07:39:06,586 - INFO - Epoch 5480, Total Loss: 7593.7031, Loss U: 806.3438, Loss P: 67.8661, Entropy: 304.8455, Entropy Lam: 0.0024413195377348085\n",
      "2024-12-22 07:39:06,648 - INFO - Epoch 5490, Total Loss: 7593.7104, Loss U: 806.3457, Loss P: 67.8662, Entropy: 303.4875, Entropy Lam: 0.0024413195377348085\n",
      "2024-12-22 07:39:06,749 - INFO - Epoch 5500, Total Loss: 7593.7832, Loss U: 806.3906, Loss P: 67.8658, Entropy: 302.0785, Entropy Lam: 0.0026854514915082895\n",
      "2024-12-22 07:39:06,868 - INFO - Epoch 5510, Total Loss: 7593.7646, Loss U: 806.4004, Loss P: 67.8656, Entropy: 300.5747, Entropy Lam: 0.0026854514915082895\n",
      "2024-12-22 07:39:07,014 - INFO - Epoch 5520, Total Loss: 7593.7793, Loss U: 806.3418, Loss P: 67.8663, Entropy: 299.1072, Entropy Lam: 0.0026854514915082895\n",
      "2024-12-22 07:39:07,113 - INFO - Epoch 5530, Total Loss: 7593.7808, Loss U: 806.2617, Loss P: 67.8672, Entropy: 297.6293, Entropy Lam: 0.0026854514915082895\n",
      "2024-12-22 07:39:07,193 - INFO - Epoch 5540, Total Loss: 7593.8457, Loss U: 806.2949, Loss P: 67.8668, Entropy: 296.1519, Entropy Lam: 0.0029539966406591186\n",
      "2024-12-22 07:39:07,261 - INFO - Epoch 5550, Total Loss: 7593.8438, Loss U: 806.4336, Loss P: 67.8654, Entropy: 294.5289, Entropy Lam: 0.0029539966406591186\n",
      "2024-12-22 07:39:07,333 - INFO - Epoch 5560, Total Loss: 7593.8394, Loss U: 806.4062, Loss P: 67.8657, Entropy: 292.8711, Entropy Lam: 0.0029539966406591186\n",
      "2024-12-22 07:39:07,400 - INFO - Epoch 5570, Total Loss: 7593.9160, Loss U: 806.5430, Loss P: 67.8643, Entropy: 291.2190, Entropy Lam: 0.0032493963047250307\n",
      "2024-12-22 07:39:07,466 - INFO - Epoch 5580, Total Loss: 7593.9009, Loss U: 806.3652, Loss P: 67.8660, Entropy: 289.4601, Entropy Lam: 0.0032493963047250307\n",
      "2024-12-22 07:39:07,534 - INFO - Epoch 5590, Total Loss: 7593.9004, Loss U: 806.4766, Loss P: 67.8649, Entropy: 287.6708, Entropy Lam: 0.0032493963047250307\n",
      "2024-12-22 07:39:07,603 - INFO - Epoch 5600, Total Loss: 7593.9658, Loss U: 806.6328, Loss P: 67.8640, Entropy: 285.8542, Entropy Lam: 0.0032493963047250307\n",
      "2024-12-22 07:39:07,663 - INFO - Epoch 5610, Total Loss: 7594.0249, Loss U: 806.5059, Loss P: 67.8650, Entropy: 284.1179, Entropy Lam: 0.003574335935197534\n",
      "2024-12-22 07:39:07,725 - INFO - Epoch 5620, Total Loss: 7593.9717, Loss U: 806.4805, Loss P: 67.8648, Entropy: 282.2784, Entropy Lam: 0.003574335935197534\n",
      "2024-12-22 07:39:07,800 - INFO - Epoch 5630, Total Loss: 7593.9565, Loss U: 806.4141, Loss P: 67.8654, Entropy: 280.4019, Entropy Lam: 0.003574335935197534\n",
      "2024-12-22 07:39:07,878 - INFO - Epoch 5640, Total Loss: 7593.9854, Loss U: 806.5312, Loss P: 67.8646, Entropy: 278.5154, Entropy Lam: 0.003574335935197534\n",
      "2024-12-22 07:39:08,018 - INFO - Epoch 5650, Total Loss: 7593.9702, Loss U: 806.2480, Loss P: 67.8673, Entropy: 276.5413, Entropy Lam: 0.003574335935197534\n",
      "2024-12-22 07:39:08,193 - INFO - Epoch 5660, Total Loss: 7594.0454, Loss U: 806.5293, Loss P: 67.8644, Entropy: 274.5925, Entropy Lam: 0.003931769528717288\n",
      "2024-12-22 07:39:08,294 - INFO - Epoch 5670, Total Loss: 7594.0386, Loss U: 806.3867, Loss P: 67.8658, Entropy: 272.5812, Entropy Lam: 0.003931769528717288\n",
      "2024-12-22 07:39:08,368 - INFO - Epoch 5680, Total Loss: 7594.0332, Loss U: 806.4727, Loss P: 67.8650, Entropy: 270.5486, Entropy Lam: 0.003931769528717288\n",
      "2024-12-22 07:39:08,449 - INFO - Epoch 5690, Total Loss: 7594.0098, Loss U: 806.4238, Loss P: 67.8653, Entropy: 268.5459, Entropy Lam: 0.003931769528717288\n",
      "2024-12-22 07:39:08,518 - INFO - Epoch 5700, Total Loss: 7594.0518, Loss U: 806.2500, Loss P: 67.8675, Entropy: 266.5346, Entropy Lam: 0.003931769528717288\n",
      "2024-12-22 07:39:08,603 - INFO - Epoch 5710, Total Loss: 7594.1030, Loss U: 806.4316, Loss P: 67.8653, Entropy: 264.4984, Entropy Lam: 0.004324946481589017\n",
      "2024-12-22 07:39:08,675 - INFO - Epoch 5720, Total Loss: 7594.0869, Loss U: 806.4316, Loss P: 67.8652, Entropy: 262.3907, Entropy Lam: 0.004324946481589017\n",
      "2024-12-22 07:39:08,772 - INFO - Epoch 5730, Total Loss: 7594.0820, Loss U: 806.4609, Loss P: 67.8650, Entropy: 260.3124, Entropy Lam: 0.004324946481589017\n",
      "2024-12-22 07:39:08,836 - INFO - Epoch 5740, Total Loss: 7594.2041, Loss U: 806.4922, Loss P: 67.8660, Entropy: 258.2257, Entropy Lam: 0.004324946481589017\n",
      "2024-12-22 07:39:08,902 - INFO - Epoch 5750, Total Loss: 7594.1953, Loss U: 806.4316, Loss P: 67.8654, Entropy: 256.1324, Entropy Lam: 0.004757441129747918\n",
      "2024-12-22 07:39:08,988 - INFO - Epoch 5760, Total Loss: 7594.1660, Loss U: 806.4570, Loss P: 67.8650, Entropy: 254.0308, Entropy Lam: 0.004757441129747918\n",
      "2024-12-22 07:39:09,112 - INFO - Epoch 5770, Total Loss: 7594.1699, Loss U: 806.4766, Loss P: 67.8650, Entropy: 251.8883, Entropy Lam: 0.004757441129747918\n",
      "2024-12-22 07:39:09,263 - INFO - Epoch 5780, Total Loss: 7594.1470, Loss U: 806.4707, Loss P: 67.8649, Entropy: 249.7462, Entropy Lam: 0.004757441129747918\n",
      "2024-12-22 07:39:09,379 - INFO - Epoch 5790, Total Loss: 7594.1514, Loss U: 806.5762, Loss P: 67.8640, Entropy: 247.6512, Entropy Lam: 0.004757441129747918\n",
      "2024-12-22 07:39:09,467 - INFO - Epoch 5800, Total Loss: 7594.1445, Loss U: 806.3535, Loss P: 67.8662, Entropy: 245.5950, Entropy Lam: 0.004757441129747918\n",
      "2024-12-22 07:39:09,541 - INFO - Epoch 5810, Total Loss: 7594.2598, Loss U: 806.4141, Loss P: 67.8657, Entropy: 243.6164, Entropy Lam: 0.00523318524272271\n",
      "2024-12-22 07:39:09,613 - INFO - Epoch 5820, Total Loss: 7594.2319, Loss U: 806.5195, Loss P: 67.8645, Entropy: 241.5058, Entropy Lam: 0.00523318524272271\n",
      "2024-12-22 07:39:09,686 - INFO - Epoch 5830, Total Loss: 7594.2568, Loss U: 806.6094, Loss P: 67.8639, Entropy: 239.4054, Entropy Lam: 0.00523318524272271\n",
      "2024-12-22 07:39:09,768 - INFO - Epoch 5840, Total Loss: 7594.2065, Loss U: 806.2695, Loss P: 67.8669, Entropy: 237.4466, Entropy Lam: 0.00523318524272271\n",
      "2024-12-22 07:39:09,843 - INFO - Epoch 5850, Total Loss: 7594.1831, Loss U: 806.4648, Loss P: 67.8649, Entropy: 235.5647, Entropy Lam: 0.00523318524272271\n",
      "2024-12-22 07:39:09,908 - INFO - Epoch 5860, Total Loss: 7594.1846, Loss U: 806.3457, Loss P: 67.8662, Entropy: 233.7386, Entropy Lam: 0.00523318524272271\n",
      "2024-12-22 07:39:09,976 - INFO - Epoch 5870, Total Loss: 7594.3052, Loss U: 806.3145, Loss P: 67.8678, Entropy: 231.9284, Entropy Lam: 0.00523318524272271\n",
      "2024-12-22 07:39:10,038 - INFO - Epoch 5880, Total Loss: 7594.3076, Loss U: 806.3555, Loss P: 67.8663, Entropy: 230.1930, Entropy Lam: 0.005756503766994982\n",
      "2024-12-22 07:39:10,108 - INFO - Epoch 5890, Total Loss: 7594.2749, Loss U: 806.4336, Loss P: 67.8653, Entropy: 228.4280, Entropy Lam: 0.005756503766994982\n",
      "2024-12-22 07:39:10,174 - INFO - Epoch 5900, Total Loss: 7594.2563, Loss U: 806.4160, Loss P: 67.8654, Entropy: 226.6674, Entropy Lam: 0.005756503766994982\n",
      "2024-12-22 07:39:10,238 - INFO - Epoch 5910, Total Loss: 7594.2529, Loss U: 806.4062, Loss P: 67.8655, Entropy: 224.9381, Entropy Lam: 0.005756503766994982\n",
      "2024-12-22 07:39:10,308 - INFO - Epoch 5920, Total Loss: 7594.3403, Loss U: 806.3730, Loss P: 67.8668, Entropy: 223.2707, Entropy Lam: 0.005756503766994982\n",
      "2024-12-22 07:39:10,395 - INFO - Epoch 5930, Total Loss: 7594.2495, Loss U: 806.3398, Loss P: 67.8663, Entropy: 221.7337, Entropy Lam: 0.00633215414369448\n",
      "2024-12-22 07:39:10,507 - INFO - Epoch 5940, Total Loss: 7594.3525, Loss U: 806.4336, Loss P: 67.8652, Entropy: 220.1566, Entropy Lam: 0.00633215414369448\n",
      "2024-12-22 07:39:10,621 - INFO - Epoch 5950, Total Loss: 7594.3350, Loss U: 806.4141, Loss P: 67.8654, Entropy: 218.5589, Entropy Lam: 0.00633215414369448\n",
      "2024-12-22 07:39:10,718 - INFO - Epoch 5960, Total Loss: 7594.3413, Loss U: 806.4180, Loss P: 67.8655, Entropy: 217.0244, Entropy Lam: 0.00633215414369448\n",
      "2024-12-22 07:39:10,810 - INFO - Epoch 5970, Total Loss: 7594.3447, Loss U: 806.3125, Loss P: 67.8667, Entropy: 215.5419, Entropy Lam: 0.00633215414369448\n",
      "2024-12-22 07:39:10,898 - INFO - Epoch 5980, Total Loss: 7594.4771, Loss U: 806.2539, Loss P: 67.8673, Entropy: 214.0076, Entropy Lam: 0.006965369558063929\n",
      "2024-12-22 07:39:10,978 - INFO - Epoch 5990, Total Loss: 7594.4604, Loss U: 806.4160, Loss P: 67.8656, Entropy: 212.5147, Entropy Lam: 0.006965369558063929\n",
      "2024-12-22 07:39:11,054 - INFO - Epoch 6000, Total Loss: 7594.4541, Loss U: 806.2070, Loss P: 67.8678, Entropy: 211.0874, Entropy Lam: 0.006965369558063929\n",
      "2024-12-22 07:39:11,132 - INFO - Epoch 6010, Total Loss: 7594.4214, Loss U: 806.4707, Loss P: 67.8649, Entropy: 209.8111, Entropy Lam: 0.006965369558063929\n",
      "2024-12-22 07:39:11,193 - INFO - Epoch 6020, Total Loss: 7594.4067, Loss U: 806.3223, Loss P: 67.8663, Entropy: 208.5756, Entropy Lam: 0.006965369558063929\n",
      "2024-12-22 07:39:11,258 - INFO - Epoch 6030, Total Loss: 7594.4014, Loss U: 806.4277, Loss P: 67.8653, Entropy: 207.3787, Entropy Lam: 0.006965369558063929\n",
      "2024-12-22 07:39:11,318 - INFO - Epoch 6040, Total Loss: 7594.3906, Loss U: 806.3398, Loss P: 67.8661, Entropy: 206.2224, Entropy Lam: 0.006965369558063929\n",
      "2024-12-22 07:39:11,378 - INFO - Epoch 6050, Total Loss: 7594.4805, Loss U: 806.3164, Loss P: 67.8674, Entropy: 205.1145, Entropy Lam: 0.006965369558063929\n",
      "2024-12-22 07:39:11,428 - INFO - Epoch 6060, Total Loss: 7594.5664, Loss U: 806.4766, Loss P: 67.8653, Entropy: 204.0494, Entropy Lam: 0.007661906513870323\n",
      "2024-12-22 07:39:11,503 - INFO - Epoch 6070, Total Loss: 7594.5283, Loss U: 806.3672, Loss P: 67.8661, Entropy: 202.9270, Entropy Lam: 0.007661906513870323\n",
      "2024-12-22 07:39:11,571 - INFO - Epoch 6080, Total Loss: 7594.5010, Loss U: 806.3535, Loss P: 67.8660, Entropy: 201.8664, Entropy Lam: 0.007661906513870323\n",
      "2024-12-22 07:39:11,658 - INFO - Epoch 6090, Total Loss: 7594.5044, Loss U: 806.3262, Loss P: 67.8664, Entropy: 200.8370, Entropy Lam: 0.007661906513870323\n",
      "2024-12-22 07:39:11,749 - INFO - Epoch 6100, Total Loss: 7594.5620, Loss U: 806.2715, Loss P: 67.8676, Entropy: 199.8622, Entropy Lam: 0.007661906513870323\n",
      "2024-12-22 07:39:11,839 - INFO - Epoch 6110, Total Loss: 7594.6812, Loss U: 806.4434, Loss P: 67.8656, Entropy: 198.8989, Entropy Lam: 0.008428097165257355\n",
      "2024-12-22 07:39:11,928 - INFO - Epoch 6120, Total Loss: 7594.6396, Loss U: 806.3848, Loss P: 67.8659, Entropy: 197.8918, Entropy Lam: 0.008428097165257355\n",
      "2024-12-22 07:39:13,108 - INFO - Epoch 6130, Total Loss: 7594.6235, Loss U: 806.4199, Loss P: 67.8654, Entropy: 196.9160, Entropy Lam: 0.008428097165257355\n",
      "2024-12-22 07:39:13,288 - INFO - Epoch 6140, Total Loss: 7594.6143, Loss U: 806.3301, Loss P: 67.8663, Entropy: 195.9890, Entropy Lam: 0.008428097165257355\n",
      "2024-12-22 07:39:13,425 - INFO - Epoch 6150, Total Loss: 7594.6084, Loss U: 806.3320, Loss P: 67.8663, Entropy: 195.1123, Entropy Lam: 0.008428097165257355\n",
      "2024-12-22 07:39:13,548 - INFO - Epoch 6160, Total Loss: 7594.6807, Loss U: 806.2227, Loss P: 67.8682, Entropy: 194.2533, Entropy Lam: 0.008428097165257355\n",
      "2024-12-22 07:39:13,624 - INFO - Epoch 6170, Total Loss: 7594.7798, Loss U: 806.5156, Loss P: 67.8647, Entropy: 193.5147, Entropy Lam: 0.009270906881783092\n",
      "2024-12-22 07:39:13,711 - INFO - Epoch 6180, Total Loss: 7594.7612, Loss U: 806.3301, Loss P: 67.8665, Entropy: 192.6254, Entropy Lam: 0.009270906881783092\n",
      "2024-12-22 07:39:13,789 - INFO - Epoch 6190, Total Loss: 7594.7480, Loss U: 806.4023, Loss P: 67.8657, Entropy: 191.7816, Entropy Lam: 0.009270906881783092\n",
      "2024-12-22 07:39:13,859 - INFO - Epoch 6200, Total Loss: 7594.7437, Loss U: 806.3535, Loss P: 67.8662, Entropy: 190.9794, Entropy Lam: 0.009270906881783092\n",
      "2024-12-22 07:39:13,931 - INFO - Epoch 6210, Total Loss: 7594.7896, Loss U: 806.2871, Loss P: 67.8674, Entropy: 190.2198, Entropy Lam: 0.009270906881783092\n",
      "2024-12-22 07:39:13,998 - INFO - Epoch 6220, Total Loss: 7594.7754, Loss U: 806.5117, Loss P: 67.8651, Entropy: 189.5068, Entropy Lam: 0.009270906881783092\n",
      "2024-12-22 07:39:14,072 - INFO - Epoch 6230, Total Loss: 7594.9116, Loss U: 806.3418, Loss P: 67.8664, Entropy: 188.7990, Entropy Lam: 0.010197997569961401\n",
      "2024-12-22 07:39:14,143 - INFO - Epoch 6240, Total Loss: 7594.8916, Loss U: 806.4004, Loss P: 67.8657, Entropy: 188.0129, Entropy Lam: 0.010197997569961401\n",
      "2024-12-22 07:39:14,258 - INFO - Epoch 6250, Total Loss: 7594.8838, Loss U: 806.3789, Loss P: 67.8660, Entropy: 187.2626, Entropy Lam: 0.010197997569961401\n",
      "2024-12-22 07:39:14,343 - INFO - Epoch 6260, Total Loss: 7594.8848, Loss U: 806.3691, Loss P: 67.8661, Entropy: 186.5581, Entropy Lam: 0.010197997569961401\n",
      "2024-12-22 07:39:14,425 - INFO - Epoch 6270, Total Loss: 7594.8945, Loss U: 806.2637, Loss P: 67.8674, Entropy: 185.8833, Entropy Lam: 0.010197997569961401\n",
      "2024-12-22 07:39:14,513 - INFO - Epoch 6280, Total Loss: 7594.8975, Loss U: 806.3203, Loss P: 67.8669, Entropy: 185.2238, Entropy Lam: 0.010197997569961401\n",
      "2024-12-22 07:39:14,589 - INFO - Epoch 6290, Total Loss: 7595.0630, Loss U: 806.4180, Loss P: 67.8657, Entropy: 184.5562, Entropy Lam: 0.011217797326957542\n",
      "2024-12-22 07:39:14,681 - INFO - Epoch 6300, Total Loss: 7595.0479, Loss U: 806.4355, Loss P: 67.8655, Entropy: 183.8269, Entropy Lam: 0.011217797326957542\n",
      "2024-12-22 07:39:14,808 - INFO - Epoch 6310, Total Loss: 7595.0396, Loss U: 806.3906, Loss P: 67.8659, Entropy: 183.1481, Entropy Lam: 0.011217797326957542\n",
      "2024-12-22 07:39:14,893 - INFO - Epoch 6320, Total Loss: 7595.0425, Loss U: 806.4824, Loss P: 67.8651, Entropy: 182.4986, Entropy Lam: 0.011217797326957542\n",
      "2024-12-22 07:39:14,971 - INFO - Epoch 6330, Total Loss: 7595.1523, Loss U: 806.6406, Loss P: 67.8647, Entropy: 181.8585, Entropy Lam: 0.012339577059653297\n",
      "2024-12-22 07:39:15,034 - INFO - Epoch 6340, Total Loss: 7595.2480, Loss U: 806.3887, Loss P: 67.8662, Entropy: 181.0739, Entropy Lam: 0.012339577059653297\n",
      "2024-12-22 07:39:15,104 - INFO - Epoch 6350, Total Loss: 7595.2329, Loss U: 806.3066, Loss P: 67.8670, Entropy: 180.4133, Entropy Lam: 0.012339577059653297\n",
      "2024-12-22 07:39:15,175 - INFO - Epoch 6360, Total Loss: 7595.2178, Loss U: 806.4805, Loss P: 67.8652, Entropy: 179.7745, Entropy Lam: 0.012339577059653297\n",
      "2024-12-22 07:39:15,242 - INFO - Epoch 6370, Total Loss: 7595.2095, Loss U: 806.3789, Loss P: 67.8662, Entropy: 179.1197, Entropy Lam: 0.012339577059653297\n",
      "2024-12-22 07:39:15,338 - INFO - Epoch 6380, Total Loss: 7595.2070, Loss U: 806.4297, Loss P: 67.8657, Entropy: 178.4941, Entropy Lam: 0.012339577059653297\n",
      "2024-12-22 07:39:15,436 - INFO - Epoch 6390, Total Loss: 7595.1978, Loss U: 806.4219, Loss P: 67.8658, Entropy: 177.8980, Entropy Lam: 0.012339577059653297\n",
      "2024-12-22 07:39:15,577 - INFO - Epoch 6400, Total Loss: 7595.2085, Loss U: 806.3809, Loss P: 67.8664, Entropy: 177.3130, Entropy Lam: 0.012339577059653297\n",
      "2024-12-22 07:39:15,689 - INFO - Epoch 6410, Total Loss: 7595.4766, Loss U: 806.5000, Loss P: 67.8658, Entropy: 176.6970, Entropy Lam: 0.013573534765618627\n",
      "2024-12-22 07:39:15,815 - INFO - Epoch 6420, Total Loss: 7595.4370, Loss U: 806.4844, Loss P: 67.8656, Entropy: 176.0375, Entropy Lam: 0.013573534765618627\n",
      "2024-12-22 07:39:15,896 - INFO - Epoch 6430, Total Loss: 7595.4092, Loss U: 806.4219, Loss P: 67.8661, Entropy: 175.4369, Entropy Lam: 0.013573534765618627\n",
      "2024-12-22 07:39:15,978 - INFO - Epoch 6440, Total Loss: 7595.3945, Loss U: 806.4258, Loss P: 67.8660, Entropy: 174.8445, Entropy Lam: 0.013573534765618627\n",
      "2024-12-22 07:39:16,058 - INFO - Epoch 6450, Total Loss: 7595.3994, Loss U: 806.4453, Loss P: 67.8659, Entropy: 174.2622, Entropy Lam: 0.013573534765618627\n",
      "2024-12-22 07:39:16,138 - INFO - Epoch 6460, Total Loss: 7595.4419, Loss U: 806.4414, Loss P: 67.8664, Entropy: 173.6850, Entropy Lam: 0.013573534765618627\n",
      "2024-12-22 07:39:16,220 - INFO - Epoch 6470, Total Loss: 7595.6235, Loss U: 806.4258, Loss P: 67.8661, Entropy: 173.1029, Entropy Lam: 0.01493088824218049\n",
      "2024-12-22 07:39:16,301 - INFO - Epoch 6480, Total Loss: 7595.6167, Loss U: 806.4688, Loss P: 67.8657, Entropy: 172.3989, Entropy Lam: 0.01493088824218049\n",
      "2024-12-22 07:39:16,373 - INFO - Epoch 6490, Total Loss: 7595.6025, Loss U: 806.4766, Loss P: 67.8656, Entropy: 171.7519, Entropy Lam: 0.01493088824218049\n",
      "2024-12-22 07:39:16,533 - INFO - Epoch 6500, Total Loss: 7595.6040, Loss U: 806.5117, Loss P: 67.8654, Entropy: 171.1500, Entropy Lam: 0.01493088824218049\n",
      "2024-12-22 07:39:16,604 - INFO - Epoch 6510, Total Loss: 7595.6221, Loss U: 806.6836, Loss P: 67.8639, Entropy: 170.5762, Entropy Lam: 0.01493088824218049\n",
      "2024-12-22 07:39:16,670 - INFO - Epoch 6520, Total Loss: 7595.5967, Loss U: 806.5117, Loss P: 67.8655, Entropy: 170.0117, Entropy Lam: 0.01493088824218049\n",
      "2024-12-22 07:39:16,736 - INFO - Epoch 6530, Total Loss: 7595.8472, Loss U: 806.5312, Loss P: 67.8653, Entropy: 169.3353, Entropy Lam: 0.016423977066398542\n",
      "2024-12-22 07:39:16,804 - INFO - Epoch 6540, Total Loss: 7595.8477, Loss U: 806.5039, Loss P: 67.8657, Entropy: 168.6112, Entropy Lam: 0.016423977066398542\n",
      "2024-12-22 07:39:16,875 - INFO - Epoch 6550, Total Loss: 7595.8350, Loss U: 806.4102, Loss P: 67.8667, Entropy: 168.0019, Entropy Lam: 0.016423977066398542\n",
      "2024-12-22 07:39:16,949 - INFO - Epoch 6560, Total Loss: 7595.8140, Loss U: 806.5117, Loss P: 67.8655, Entropy: 167.4502, Entropy Lam: 0.016423977066398542\n",
      "2024-12-22 07:39:17,013 - INFO - Epoch 6570, Total Loss: 7595.8149, Loss U: 806.4375, Loss P: 67.8664, Entropy: 166.9046, Entropy Lam: 0.016423977066398542\n",
      "2024-12-22 07:39:17,079 - INFO - Epoch 6580, Total Loss: 7595.8774, Loss U: 806.3574, Loss P: 67.8679, Entropy: 166.3617, Entropy Lam: 0.016423977066398542\n",
      "2024-12-22 07:39:17,148 - INFO - Epoch 6590, Total Loss: 7596.1230, Loss U: 806.5605, Loss P: 67.8657, Entropy: 165.8122, Entropy Lam: 0.018066374773038397\n",
      "2024-12-22 07:39:17,238 - INFO - Epoch 6600, Total Loss: 7596.0884, Loss U: 806.4648, Loss P: 67.8664, Entropy: 165.1526, Entropy Lam: 0.018066374773038397\n",
      "2024-12-22 07:39:17,321 - INFO - Epoch 6610, Total Loss: 7596.0605, Loss U: 806.4727, Loss P: 67.8661, Entropy: 164.5400, Entropy Lam: 0.018066374773038397\n",
      "2024-12-22 07:39:17,408 - INFO - Epoch 6620, Total Loss: 7596.0537, Loss U: 806.4316, Loss P: 67.8666, Entropy: 164.0107, Entropy Lam: 0.018066374773038397\n",
      "2024-12-22 07:39:17,473 - INFO - Epoch 6630, Total Loss: 7596.0601, Loss U: 806.4980, Loss P: 67.8661, Entropy: 163.5215, Entropy Lam: 0.018066374773038397\n",
      "2024-12-22 07:39:17,532 - INFO - Epoch 6640, Total Loss: 7596.1094, Loss U: 806.5645, Loss P: 67.8660, Entropy: 163.0351, Entropy Lam: 0.018066374773038397\n",
      "2024-12-22 07:39:17,595 - INFO - Epoch 6650, Total Loss: 7596.3535, Loss U: 806.4219, Loss P: 67.8670, Entropy: 162.4293, Entropy Lam: 0.01987301225034224\n",
      "2024-12-22 07:39:17,655 - INFO - Epoch 6660, Total Loss: 7596.3252, Loss U: 806.4551, Loss P: 67.8665, Entropy: 161.8318, Entropy Lam: 0.01987301225034224\n",
      "2024-12-22 07:39:17,715 - INFO - Epoch 6670, Total Loss: 7596.3169, Loss U: 806.4668, Loss P: 67.8664, Entropy: 161.3210, Entropy Lam: 0.01987301225034224\n",
      "2024-12-22 07:39:17,770 - INFO - Epoch 6680, Total Loss: 7596.3071, Loss U: 806.4395, Loss P: 67.8667, Entropy: 160.8638, Entropy Lam: 0.01987301225034224\n",
      "2024-12-22 07:39:17,837 - INFO - Epoch 6690, Total Loss: 7596.3462, Loss U: 806.5781, Loss P: 67.8658, Entropy: 160.4249, Entropy Lam: 0.01987301225034224\n",
      "2024-12-22 07:39:17,935 - INFO - Epoch 6700, Total Loss: 7596.3125, Loss U: 806.5156, Loss P: 67.8662, Entropy: 160.0201, Entropy Lam: 0.01987301225034224\n",
      "2024-12-22 07:39:18,090 - INFO - Epoch 6710, Total Loss: 7596.6196, Loss U: 806.3418, Loss P: 67.8679, Entropy: 159.5328, Entropy Lam: 0.021860313475376463\n",
      "2024-12-22 07:39:18,173 - INFO - Epoch 6720, Total Loss: 7596.6123, Loss U: 806.3516, Loss P: 67.8679, Entropy: 158.9526, Entropy Lam: 0.021860313475376463\n",
      "2024-12-22 07:39:18,248 - INFO - Epoch 6730, Total Loss: 7596.6338, Loss U: 806.2500, Loss P: 67.8692, Entropy: 158.4778, Entropy Lam: 0.021860313475376463\n",
      "2024-12-22 07:39:18,318 - INFO - Epoch 6740, Total Loss: 7596.6108, Loss U: 806.4160, Loss P: 67.8674, Entropy: 158.0827, Entropy Lam: 0.021860313475376463\n",
      "2024-12-22 07:39:18,389 - INFO - Epoch 6750, Total Loss: 7596.9390, Loss U: 806.4785, Loss P: 67.8667, Entropy: 157.5705, Entropy Lam: 0.024046344822914113\n",
      "2024-12-22 07:39:18,453 - INFO - Epoch 6760, Total Loss: 7596.9512, Loss U: 806.3789, Loss P: 67.8680, Entropy: 157.0162, Entropy Lam: 0.024046344822914113\n",
      "2024-12-22 07:39:18,518 - INFO - Epoch 6770, Total Loss: 7596.9321, Loss U: 806.4180, Loss P: 67.8675, Entropy: 156.5683, Entropy Lam: 0.024046344822914113\n",
      "2024-12-22 07:39:18,574 - INFO - Epoch 6780, Total Loss: 7596.9121, Loss U: 806.4180, Loss P: 67.8674, Entropy: 156.1519, Entropy Lam: 0.024046344822914113\n",
      "2024-12-22 07:39:18,628 - INFO - Epoch 6790, Total Loss: 7596.9868, Loss U: 806.3809, Loss P: 67.8686, Entropy: 155.7170, Entropy Lam: 0.024046344822914113\n",
      "2024-12-22 07:39:18,699 - INFO - Epoch 6800, Total Loss: 7596.9390, Loss U: 806.2441, Loss P: 67.8696, Entropy: 155.3664, Entropy Lam: 0.024046344822914113\n",
      "2024-12-22 07:39:18,774 - INFO - Epoch 6810, Total Loss: 7597.2739, Loss U: 806.4648, Loss P: 67.8671, Entropy: 154.8120, Entropy Lam: 0.026450979305205527\n",
      "2024-12-22 07:39:18,843 - INFO - Epoch 6820, Total Loss: 7597.2515, Loss U: 806.3027, Loss P: 67.8687, Entropy: 154.2650, Entropy Lam: 0.026450979305205527\n",
      "2024-12-22 07:39:18,918 - INFO - Epoch 6830, Total Loss: 7597.2788, Loss U: 806.1934, Loss P: 67.8702, Entropy: 153.7931, Entropy Lam: 0.026450979305205527\n",
      "2024-12-22 07:39:18,984 - INFO - Epoch 6840, Total Loss: 7597.2871, Loss U: 806.2832, Loss P: 67.8695, Entropy: 153.2851, Entropy Lam: 0.026450979305205527\n",
      "2024-12-22 07:39:19,052 - INFO - Epoch 6850, Total Loss: 7597.6528, Loss U: 806.3730, Loss P: 67.8684, Entropy: 152.6124, Entropy Lam: 0.029096077235726082\n",
      "2024-12-22 07:39:19,125 - INFO - Epoch 6860, Total Loss: 7597.6294, Loss U: 806.1641, Loss P: 67.8704, Entropy: 151.9596, Entropy Lam: 0.029096077235726082\n",
      "2024-12-22 07:39:19,268 - INFO - Epoch 6870, Total Loss: 7597.7007, Loss U: 806.0508, Loss P: 67.8725, Entropy: 151.3301, Entropy Lam: 0.029096077235726082\n",
      "2024-12-22 07:39:19,318 - INFO - Epoch 6880, Total Loss: 7597.6265, Loss U: 806.3711, Loss P: 67.8687, Entropy: 150.6800, Entropy Lam: 0.029096077235726082\n",
      "2024-12-22 07:39:19,368 - INFO - Epoch 6890, Total Loss: 7597.6006, Loss U: 806.2266, Loss P: 67.8701, Entropy: 150.0239, Entropy Lam: 0.029096077235726082\n",
      "2024-12-22 07:39:19,418 - INFO - Epoch 6900, Total Loss: 7597.6123, Loss U: 806.2246, Loss P: 67.8704, Entropy: 149.2943, Entropy Lam: 0.029096077235726082\n",
      "2024-12-22 07:39:19,469 - INFO - Epoch 6910, Total Loss: 7597.6509, Loss U: 806.2090, Loss P: 67.8712, Entropy: 148.5261, Entropy Lam: 0.029096077235726082\n",
      "2024-12-22 07:39:19,518 - INFO - Epoch 6920, Total Loss: 7598.0117, Loss U: 806.0918, Loss P: 67.8720, Entropy: 147.5085, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:19,568 - INFO - Epoch 6930, Total Loss: 7597.9854, Loss U: 806.1719, Loss P: 67.8712, Entropy: 146.5684, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:19,624 - INFO - Epoch 6940, Total Loss: 7597.9624, Loss U: 806.1914, Loss P: 67.8711, Entropy: 145.5810, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:19,681 - INFO - Epoch 6950, Total Loss: 7598.0459, Loss U: 806.3418, Loss P: 67.8708, Entropy: 144.4995, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:19,741 - INFO - Epoch 6960, Total Loss: 7597.9443, Loss U: 806.1836, Loss P: 67.8717, Entropy: 143.4564, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:19,814 - INFO - Epoch 6970, Total Loss: 7597.9067, Loss U: 806.0488, Loss P: 67.8730, Entropy: 142.4480, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:19,878 - INFO - Epoch 6980, Total Loss: 7597.8906, Loss U: 805.9629, Loss P: 67.8740, Entropy: 141.4217, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:19,941 - INFO - Epoch 6990, Total Loss: 7597.8975, Loss U: 805.9668, Loss P: 67.8744, Entropy: 140.4274, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,013 - INFO - Epoch 7000, Total Loss: 7597.8511, Loss U: 806.1133, Loss P: 67.8727, Entropy: 139.4606, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,083 - INFO - Epoch 7010, Total Loss: 7597.8579, Loss U: 805.9297, Loss P: 67.8749, Entropy: 138.5431, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,186 - INFO - Epoch 7020, Total Loss: 7597.8242, Loss U: 805.9297, Loss P: 67.8749, Entropy: 137.6840, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,270 - INFO - Epoch 7030, Total Loss: 7597.8003, Loss U: 805.9570, Loss P: 67.8746, Entropy: 136.8937, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,370 - INFO - Epoch 7040, Total Loss: 7597.7915, Loss U: 805.9492, Loss P: 67.8748, Entropy: 136.1433, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,448 - INFO - Epoch 7050, Total Loss: 7597.7925, Loss U: 805.9199, Loss P: 67.8754, Entropy: 135.4279, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,517 - INFO - Epoch 7060, Total Loss: 7597.7676, Loss U: 805.7539, Loss P: 67.8770, Entropy: 134.7909, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,579 - INFO - Epoch 7070, Total Loss: 7597.7568, Loss U: 805.8047, Loss P: 67.8766, Entropy: 134.1896, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,642 - INFO - Epoch 7080, Total Loss: 7597.7378, Loss U: 805.9590, Loss P: 67.8750, Entropy: 133.6484, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,717 - INFO - Epoch 7090, Total Loss: 7597.7148, Loss U: 805.8965, Loss P: 67.8756, Entropy: 133.1590, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,783 - INFO - Epoch 7100, Total Loss: 7597.7114, Loss U: 805.8613, Loss P: 67.8760, Entropy: 132.6819, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,849 - INFO - Epoch 7110, Total Loss: 7597.7090, Loss U: 805.8496, Loss P: 67.8763, Entropy: 132.2574, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,928 - INFO - Epoch 7120, Total Loss: 7597.7310, Loss U: 805.8887, Loss P: 67.8762, Entropy: 131.8513, Entropy Lam: 0.032005684959298696\n",
      "2024-12-22 07:39:20,989 - INFO - Epoch 7130, Total Loss: 7598.1260, Loss U: 805.9121, Loss P: 67.8759, Entropy: 131.4642, Entropy Lam: 0.03520625345522857\n",
      "2024-12-22 07:39:21,051 - INFO - Epoch 7140, Total Loss: 7598.1094, Loss U: 805.6914, Loss P: 67.8781, Entropy: 130.9876, Entropy Lam: 0.03520625345522857\n",
      "2024-12-22 07:39:21,108 - INFO - Epoch 7150, Total Loss: 7598.0703, Loss U: 805.9004, Loss P: 67.8757, Entropy: 130.6543, Entropy Lam: 0.03520625345522857\n",
      "2024-12-22 07:39:21,158 - INFO - Epoch 7160, Total Loss: 7598.0762, Loss U: 805.8789, Loss P: 67.8761, Entropy: 130.3396, Entropy Lam: 0.03520625345522857\n",
      "2024-12-22 07:39:21,218 - INFO - Epoch 7170, Total Loss: 7598.0688, Loss U: 805.7988, Loss P: 67.8769, Entropy: 130.0182, Entropy Lam: 0.03520625345522857\n",
      "2024-12-22 07:39:21,268 - INFO - Epoch 7180, Total Loss: 7598.0762, Loss U: 805.7949, Loss P: 67.8771, Entropy: 129.7259, Entropy Lam: 0.03520625345522857\n",
      "2024-12-22 07:39:21,325 - INFO - Epoch 7190, Total Loss: 7598.0674, Loss U: 805.7363, Loss P: 67.8777, Entropy: 129.4753, Entropy Lam: 0.03872687880075143\n",
      "2024-12-22 07:39:21,465 - INFO - Epoch 7200, Total Loss: 7598.5200, Loss U: 805.8398, Loss P: 67.8768, Entropy: 129.1008, Entropy Lam: 0.03872687880075143\n",
      "2024-12-22 07:39:21,538 - INFO - Epoch 7210, Total Loss: 7598.4995, Loss U: 805.8672, Loss P: 67.8764, Entropy: 128.8613, Entropy Lam: 0.03872687880075143\n",
      "2024-12-22 07:39:21,596 - INFO - Epoch 7220, Total Loss: 7598.4883, Loss U: 805.7891, Loss P: 67.8772, Entropy: 128.6535, Entropy Lam: 0.03872687880075143\n",
      "2024-12-22 07:39:21,648 - INFO - Epoch 7230, Total Loss: 7598.4736, Loss U: 805.8047, Loss P: 67.8770, Entropy: 128.4251, Entropy Lam: 0.03872687880075143\n",
      "2024-12-22 07:39:21,700 - INFO - Epoch 7240, Total Loss: 7598.4678, Loss U: 805.8203, Loss P: 67.8768, Entropy: 128.2210, Entropy Lam: 0.03872687880075143\n",
      "2024-12-22 07:39:21,758 - INFO - Epoch 7250, Total Loss: 7598.5039, Loss U: 805.8574, Loss P: 67.8769, Entropy: 128.0197, Entropy Lam: 0.03872687880075143\n",
      "2024-12-22 07:39:21,818 - INFO - Epoch 7260, Total Loss: 7598.4614, Loss U: 805.8574, Loss P: 67.8765, Entropy: 127.8662, Entropy Lam: 0.03872687880075143\n",
      "2024-12-22 07:39:21,878 - INFO - Epoch 7270, Total Loss: 7598.4634, Loss U: 805.7793, Loss P: 67.8774, Entropy: 127.7072, Entropy Lam: 0.03872687880075143\n",
      "2024-12-22 07:39:21,948 - INFO - Epoch 7280, Total Loss: 7598.4883, Loss U: 805.7520, Loss P: 67.8780, Entropy: 127.5343, Entropy Lam: 0.03872687880075143\n",
      "2024-12-22 07:39:22,043 - INFO - Epoch 7290, Total Loss: 7598.9463, Loss U: 805.6992, Loss P: 67.8783, Entropy: 127.2592, Entropy Lam: 0.042599566680826574\n",
      "2024-12-22 07:39:22,204 - INFO - Epoch 7300, Total Loss: 7598.9429, Loss U: 805.7070, Loss P: 67.8782, Entropy: 127.1233, Entropy Lam: 0.042599566680826574\n",
      "2024-12-22 07:39:22,694 - INFO - Epoch 7310, Total Loss: 7598.9360, Loss U: 805.7773, Loss P: 67.8775, Entropy: 127.0148, Entropy Lam: 0.042599566680826574\n",
      "2024-12-22 07:39:22,920 - INFO - Epoch 7320, Total Loss: 7598.9365, Loss U: 805.7207, Loss P: 67.8781, Entropy: 126.8646, Entropy Lam: 0.042599566680826574\n",
      "2024-12-22 07:39:23,052 - INFO - Epoch 7330, Total Loss: 7598.9121, Loss U: 805.7305, Loss P: 67.8778, Entropy: 126.7494, Entropy Lam: 0.042599566680826574\n",
      "2024-12-22 07:39:23,168 - INFO - Epoch 7340, Total Loss: 7598.9189, Loss U: 805.7617, Loss P: 67.8776, Entropy: 126.6508, Entropy Lam: 0.042599566680826574\n",
      "2024-12-22 07:39:23,239 - INFO - Epoch 7350, Total Loss: 7598.9067, Loss U: 805.7637, Loss P: 67.8775, Entropy: 126.5408, Entropy Lam: 0.042599566680826574\n",
      "2024-12-22 07:39:23,318 - INFO - Epoch 7360, Total Loss: 7598.9277, Loss U: 805.8438, Loss P: 67.8770, Entropy: 126.4362, Entropy Lam: 0.042599566680826574\n",
      "2024-12-22 07:39:23,380 - INFO - Epoch 7370, Total Loss: 7599.4673, Loss U: 805.7715, Loss P: 67.8778, Entropy: 126.2775, Entropy Lam: 0.046859523348909235\n",
      "2024-12-22 07:39:23,443 - INFO - Epoch 7380, Total Loss: 7599.4531, Loss U: 805.8203, Loss P: 67.8772, Entropy: 126.0952, Entropy Lam: 0.046859523348909235\n",
      "2024-12-22 07:39:23,514 - INFO - Epoch 7390, Total Loss: 7599.4385, Loss U: 805.6836, Loss P: 67.8785, Entropy: 126.0131, Entropy Lam: 0.046859523348909235\n",
      "2024-12-22 07:39:23,581 - INFO - Epoch 7400, Total Loss: 7600.0273, Loss U: 805.5449, Loss P: 67.8800, Entropy: 125.7763, Entropy Lam: 0.05154547568380016\n",
      "2024-12-22 07:39:23,653 - INFO - Epoch 7410, Total Loss: 7600.0171, Loss U: 805.6406, Loss P: 67.8790, Entropy: 125.6516, Entropy Lam: 0.05154547568380016\n",
      "2024-12-22 07:39:23,727 - INFO - Epoch 7420, Total Loss: 7600.0186, Loss U: 805.7617, Loss P: 67.8778, Entropy: 125.6075, Entropy Lam: 0.05154547568380016\n",
      "2024-12-22 07:39:23,796 - INFO - Epoch 7430, Total Loss: 7600.0127, Loss U: 805.7109, Loss P: 67.8783, Entropy: 125.5195, Entropy Lam: 0.05154547568380016\n",
      "2024-12-22 07:39:23,863 - INFO - Epoch 7440, Total Loss: 7600.6738, Loss U: 805.5957, Loss P: 67.8797, Entropy: 125.3798, Entropy Lam: 0.05670002325218018\n",
      "2024-12-22 07:39:23,929 - INFO - Epoch 7450, Total Loss: 7600.6587, Loss U: 805.6992, Loss P: 67.8786, Entropy: 125.1664, Entropy Lam: 0.05670002325218018\n",
      "2024-12-22 07:39:23,993 - INFO - Epoch 7460, Total Loss: 7600.6499, Loss U: 805.7070, Loss P: 67.8785, Entropy: 125.0946, Entropy Lam: 0.05670002325218018\n",
      "2024-12-22 07:39:24,048 - INFO - Epoch 7470, Total Loss: 7600.6650, Loss U: 805.5840, Loss P: 67.8799, Entropy: 125.0227, Entropy Lam: 0.05670002325218018\n",
      "2024-12-22 07:39:24,112 - INFO - Epoch 7480, Total Loss: 7601.3555, Loss U: 805.5371, Loss P: 67.8803, Entropy: 124.9126, Entropy Lam: 0.06237002557739821\n",
      "2024-12-22 07:39:24,173 - INFO - Epoch 7490, Total Loss: 7601.3462, Loss U: 805.7871, Loss P: 67.8778, Entropy: 124.6925, Entropy Lam: 0.06237002557739821\n",
      "2024-12-22 07:39:24,248 - INFO - Epoch 7500, Total Loss: 7601.3335, Loss U: 805.6602, Loss P: 67.8790, Entropy: 124.6305, Entropy Lam: 0.06237002557739821\n",
      "2024-12-22 07:39:24,312 - INFO - Epoch 7510, Total Loss: 7601.3281, Loss U: 805.6719, Loss P: 67.8789, Entropy: 124.5800, Entropy Lam: 0.06237002557739821\n",
      "2024-12-22 07:39:24,388 - INFO - Epoch 7520, Total Loss: 7601.3394, Loss U: 805.7383, Loss P: 67.8783, Entropy: 124.5169, Entropy Lam: 0.06237002557739821\n",
      "2024-12-22 07:39:24,462 - INFO - Epoch 7530, Total Loss: 7601.4170, Loss U: 805.8984, Loss P: 67.8776, Entropy: 124.4663, Entropy Lam: 0.06237002557739821\n",
      "2024-12-22 07:39:24,543 - INFO - Epoch 7540, Total Loss: 7601.3452, Loss U: 805.5078, Loss P: 67.8808, Entropy: 124.3747, Entropy Lam: 0.06237002557739821\n",
      "2024-12-22 07:39:24,623 - INFO - Epoch 7550, Total Loss: 7601.3418, Loss U: 805.7852, Loss P: 67.8780, Entropy: 124.3662, Entropy Lam: 0.06237002557739821\n",
      "2024-12-22 07:39:24,730 - INFO - Epoch 7560, Total Loss: 7602.0972, Loss U: 805.4590, Loss P: 67.8812, Entropy: 124.2239, Entropy Lam: 0.06860702813513804\n",
      "2024-12-22 07:39:25,167 - INFO - Epoch 7570, Total Loss: 7602.0830, Loss U: 805.7012, Loss P: 67.8787, Entropy: 124.0101, Entropy Lam: 0.06860702813513804\n",
      "2024-12-22 07:39:25,488 - INFO - Epoch 7580, Total Loss: 7602.0840, Loss U: 805.6445, Loss P: 67.8793, Entropy: 123.9784, Entropy Lam: 0.06860702813513804\n",
      "2024-12-22 07:39:25,578 - INFO - Epoch 7590, Total Loss: 7602.0835, Loss U: 805.6543, Loss P: 67.8793, Entropy: 123.9449, Entropy Lam: 0.06860702813513804\n",
      "2024-12-22 07:39:25,663 - INFO - Epoch 7600, Total Loss: 7602.0825, Loss U: 805.6914, Loss P: 67.8789, Entropy: 123.8822, Entropy Lam: 0.06860702813513804\n",
      "2024-12-22 07:39:25,761 - INFO - Epoch 7610, Total Loss: 7602.9883, Loss U: 805.6445, Loss P: 67.8801, Entropy: 123.7469, Entropy Lam: 0.07546773094865185\n",
      "2024-12-22 07:39:25,838 - INFO - Epoch 7620, Total Loss: 7602.9580, Loss U: 805.5469, Loss P: 67.8809, Entropy: 123.5244, Entropy Lam: 0.07546773094865185\n",
      "2024-12-22 07:39:25,909 - INFO - Epoch 7630, Total Loss: 7602.9277, Loss U: 805.6602, Loss P: 67.8795, Entropy: 123.5157, Entropy Lam: 0.07546773094865185\n",
      "2024-12-22 07:39:25,977 - INFO - Epoch 7640, Total Loss: 7602.9146, Loss U: 805.6035, Loss P: 67.8799, Entropy: 123.4940, Entropy Lam: 0.07546773094865185\n",
      "2024-12-22 07:39:26,047 - INFO - Epoch 7650, Total Loss: 7602.9175, Loss U: 805.6133, Loss P: 67.8799, Entropy: 123.4400, Entropy Lam: 0.07546773094865185\n",
      "2024-12-22 07:39:26,104 - INFO - Epoch 7660, Total Loss: 7602.9023, Loss U: 805.6152, Loss P: 67.8797, Entropy: 123.4150, Entropy Lam: 0.07546773094865185\n",
      "2024-12-22 07:39:26,160 - INFO - Epoch 7670, Total Loss: 7602.9106, Loss U: 805.6328, Loss P: 67.8797, Entropy: 123.3751, Entropy Lam: 0.07546773094865185\n",
      "2024-12-22 07:39:26,218 - INFO - Epoch 7680, Total Loss: 7602.9346, Loss U: 805.6875, Loss P: 67.8794, Entropy: 123.3152, Entropy Lam: 0.07546773094865185\n",
      "2024-12-22 07:39:26,278 - INFO - Epoch 7690, Total Loss: 7603.8447, Loss U: 805.4336, Loss P: 67.8818, Entropy: 123.2565, Entropy Lam: 0.08301450404351704\n",
      "2024-12-22 07:39:26,335 - INFO - Epoch 7700, Total Loss: 7603.8403, Loss U: 805.6855, Loss P: 67.8795, Entropy: 122.9734, Entropy Lam: 0.08301450404351704\n",
      "2024-12-22 07:39:26,400 - INFO - Epoch 7710, Total Loss: 7603.8398, Loss U: 805.4980, Loss P: 67.8814, Entropy: 122.9428, Entropy Lam: 0.08301450404351704\n",
      "2024-12-22 07:39:26,495 - INFO - Epoch 7720, Total Loss: 7603.8301, Loss U: 805.5508, Loss P: 67.8807, Entropy: 122.9376, Entropy Lam: 0.08301450404351704\n",
      "2024-12-22 07:39:26,556 - INFO - Epoch 7730, Total Loss: 7603.8169, Loss U: 805.5742, Loss P: 67.8804, Entropy: 122.8924, Entropy Lam: 0.08301450404351704\n",
      "2024-12-22 07:39:26,668 - INFO - Epoch 7740, Total Loss: 7603.8311, Loss U: 805.5664, Loss P: 67.8806, Entropy: 122.8718, Entropy Lam: 0.08301450404351704\n",
      "2024-12-22 07:39:26,768 - INFO - Epoch 7750, Total Loss: 7604.8921, Loss U: 805.3613, Loss P: 67.8832, Entropy: 122.7805, Entropy Lam: 0.09131595444786875\n",
      "2024-12-22 07:39:26,919 - INFO - Epoch 7760, Total Loss: 7604.8486, Loss U: 805.6660, Loss P: 67.8800, Entropy: 122.4870, Entropy Lam: 0.09131595444786875\n",
      "2024-12-22 07:39:26,968 - INFO - Epoch 7770, Total Loss: 7604.8374, Loss U: 805.5859, Loss P: 67.8807, Entropy: 122.4540, Entropy Lam: 0.09131595444786875\n",
      "2024-12-22 07:39:27,026 - INFO - Epoch 7780, Total Loss: 7604.8267, Loss U: 805.5762, Loss P: 67.8807, Entropy: 122.4574, Entropy Lam: 0.09131595444786875\n",
      "2024-12-22 07:39:27,078 - INFO - Epoch 7790, Total Loss: 7604.8296, Loss U: 805.5742, Loss P: 67.8808, Entropy: 122.4012, Entropy Lam: 0.09131595444786875\n",
      "2024-12-22 07:39:27,126 - INFO - Epoch 7800, Total Loss: 7605.9463, Loss U: 805.3633, Loss P: 67.8830, Entropy: 122.2554, Entropy Lam: 0.10044754989265563\n",
      "2024-12-22 07:39:27,190 - INFO - Epoch 7810, Total Loss: 7605.9873, Loss U: 805.4961, Loss P: 67.8824, Entropy: 121.9369, Entropy Lam: 0.10044754989265563\n",
      "2024-12-22 07:39:27,260 - INFO - Epoch 7820, Total Loss: 7605.9604, Loss U: 805.6250, Loss P: 67.8809, Entropy: 121.9368, Entropy Lam: 0.1104923048819212\n",
      "2024-12-22 07:39:27,399 - INFO - Epoch 7830, Total Loss: 7607.1538, Loss U: 805.5391, Loss P: 67.8819, Entropy: 121.5344, Entropy Lam: 0.1104923048819212\n",
      "2024-12-22 07:39:27,484 - INFO - Epoch 7840, Total Loss: 7607.1597, Loss U: 805.5684, Loss P: 67.8817, Entropy: 121.4422, Entropy Lam: 0.1104923048819212\n",
      "2024-12-22 07:39:27,617 - INFO - Epoch 7850, Total Loss: 7607.1470, Loss U: 805.5273, Loss P: 67.8820, Entropy: 121.4660, Entropy Lam: 0.1104923048819212\n",
      "2024-12-22 07:39:27,726 - INFO - Epoch 7860, Total Loss: 7607.1421, Loss U: 805.5508, Loss P: 67.8817, Entropy: 121.4297, Entropy Lam: 0.1104923048819212\n",
      "2024-12-22 07:39:27,801 - INFO - Epoch 7870, Total Loss: 7607.1445, Loss U: 805.5840, Loss P: 67.8815, Entropy: 121.3888, Entropy Lam: 0.1104923048819212\n",
      "2024-12-22 07:39:27,861 - INFO - Epoch 7880, Total Loss: 7607.2080, Loss U: 805.6250, Loss P: 67.8818, Entropy: 121.3437, Entropy Lam: 0.1104923048819212\n",
      "2024-12-22 07:39:27,923 - INFO - Epoch 7890, Total Loss: 7608.5029, Loss U: 805.5039, Loss P: 67.8828, Entropy: 121.1053, Entropy Lam: 0.12154153537011333\n",
      "2024-12-22 07:39:27,981 - INFO - Epoch 7900, Total Loss: 7608.4941, Loss U: 805.6523, Loss P: 67.8816, Entropy: 120.8102, Entropy Lam: 0.12154153537011333\n",
      "2024-12-22 07:39:28,051 - INFO - Epoch 7910, Total Loss: 7608.4795, Loss U: 805.4902, Loss P: 67.8830, Entropy: 120.8294, Entropy Lam: 0.12154153537011333\n",
      "2024-12-22 07:39:28,109 - INFO - Epoch 7920, Total Loss: 7608.4819, Loss U: 805.5293, Loss P: 67.8827, Entropy: 120.8355, Entropy Lam: 0.12154153537011333\n",
      "2024-12-22 07:39:28,182 - INFO - Epoch 7930, Total Loss: 7608.5327, Loss U: 805.6621, Loss P: 67.8819, Entropy: 120.7820, Entropy Lam: 0.12154153537011333\n",
      "2024-12-22 07:39:28,232 - INFO - Epoch 7940, Total Loss: 7609.9517, Loss U: 805.3613, Loss P: 67.8850, Entropy: 120.3621, Entropy Lam: 0.1336956889071247\n",
      "2024-12-22 07:39:28,295 - INFO - Epoch 7950, Total Loss: 7609.9336, Loss U: 805.5391, Loss P: 67.8833, Entropy: 120.1574, Entropy Lam: 0.1336956889071247\n",
      "2024-12-22 07:39:28,348 - INFO - Epoch 7960, Total Loss: 7609.9214, Loss U: 805.4375, Loss P: 67.8842, Entropy: 120.1873, Entropy Lam: 0.1336956889071247\n",
      "2024-12-22 07:39:28,406 - INFO - Epoch 7970, Total Loss: 7609.9307, Loss U: 805.4922, Loss P: 67.8837, Entropy: 120.1996, Entropy Lam: 0.1336956889071247\n",
      "2024-12-22 07:39:28,469 - INFO - Epoch 7980, Total Loss: 7609.9219, Loss U: 805.5254, Loss P: 67.8833, Entropy: 120.1778, Entropy Lam: 0.1336956889071247\n",
      "2024-12-22 07:39:28,538 - INFO - Epoch 7990, Total Loss: 7611.5317, Loss U: 805.3750, Loss P: 67.8850, Entropy: 120.0794, Entropy Lam: 0.14706525779783716\n",
      "2024-12-22 07:39:28,610 - INFO - Epoch 8000, Total Loss: 7611.5835, Loss U: 805.5703, Loss P: 67.8844, Entropy: 119.5205, Entropy Lam: 0.14706525779783716\n",
      "2024-12-22 07:39:28,684 - INFO - Epoch 8010, Total Loss: 7611.5581, Loss U: 805.4512, Loss P: 67.8855, Entropy: 119.4136, Entropy Lam: 0.1617717835776209\n",
      "2024-12-22 07:39:28,760 - INFO - Epoch 8020, Total Loss: 7613.2871, Loss U: 805.6094, Loss P: 67.8845, Entropy: 118.8381, Entropy Lam: 0.1617717835776209\n",
      "2024-12-22 07:39:28,828 - INFO - Epoch 8030, Total Loss: 7613.2710, Loss U: 805.3789, Loss P: 67.8870, Entropy: 118.6227, Entropy Lam: 0.1617717835776209\n",
      "2024-12-22 07:39:28,895 - INFO - Epoch 8040, Total Loss: 7613.2646, Loss U: 805.3750, Loss P: 67.8870, Entropy: 118.6123, Entropy Lam: 0.1617717835776209\n",
      "2024-12-22 07:39:28,956 - INFO - Epoch 8050, Total Loss: 7613.2578, Loss U: 805.4043, Loss P: 67.8866, Entropy: 118.6204, Entropy Lam: 0.1617717835776209\n",
      "2024-12-22 07:39:29,098 - INFO - Epoch 8060, Total Loss: 7615.1812, Loss U: 805.1719, Loss P: 67.8896, Entropy: 118.3133, Entropy Lam: 0.177948961935383\n",
      "2024-12-22 07:39:29,160 - INFO - Epoch 8070, Total Loss: 7615.1641, Loss U: 805.5195, Loss P: 67.8870, Entropy: 117.7247, Entropy Lam: 0.177948961935383\n",
      "2024-12-22 07:39:29,221 - INFO - Epoch 8080, Total Loss: 7615.1831, Loss U: 805.5137, Loss P: 67.8875, Entropy: 117.5848, Entropy Lam: 0.177948961935383\n",
      "2024-12-22 07:39:29,284 - INFO - Epoch 8090, Total Loss: 7617.2759, Loss U: 805.3242, Loss P: 67.8900, Entropy: 117.2319, Entropy Lam: 0.19574385812892134\n",
      "2024-12-22 07:39:29,346 - INFO - Epoch 8100, Total Loss: 7617.2466, Loss U: 805.6387, Loss P: 67.8879, Entropy: 116.5806, Entropy Lam: 0.19574385812892134\n",
      "2024-12-22 07:39:29,403 - INFO - Epoch 8110, Total Loss: 7617.2358, Loss U: 805.4980, Loss P: 67.8895, Entropy: 116.4294, Entropy Lam: 0.19574385812892134\n",
      "2024-12-22 07:39:29,488 - INFO - Epoch 8120, Total Loss: 7617.2417, Loss U: 805.5098, Loss P: 67.8896, Entropy: 116.3244, Entropy Lam: 0.19574385812892134\n",
      "2024-12-22 07:39:29,562 - INFO - Epoch 8130, Total Loss: 7617.2251, Loss U: 805.5332, Loss P: 67.8894, Entropy: 116.2169, Entropy Lam: 0.19574385812892134\n",
      "2024-12-22 07:39:29,633 - INFO - Epoch 8140, Total Loss: 7617.2290, Loss U: 805.5801, Loss P: 67.8891, Entropy: 116.1578, Entropy Lam: 0.19574385812892134\n",
      "2024-12-22 07:39:29,702 - INFO - Epoch 8150, Total Loss: 7617.2334, Loss U: 805.6016, Loss P: 67.8890, Entropy: 116.1262, Entropy Lam: 0.19574385812892134\n",
      "2024-12-22 07:39:29,767 - INFO - Epoch 8160, Total Loss: 7617.2485, Loss U: 805.6270, Loss P: 67.8890, Entropy: 116.1003, Entropy Lam: 0.19574385812892134\n",
      "2024-12-22 07:39:29,861 - INFO - Epoch 8170, Total Loss: 7617.2197, Loss U: 805.5156, Loss P: 67.8898, Entropy: 116.0841, Entropy Lam: 0.19574385812892134\n",
      "2024-12-22 07:39:29,949 - INFO - Epoch 8180, Total Loss: 7617.2153, Loss U: 805.5488, Loss P: 67.8895, Entropy: 116.0720, Entropy Lam: 0.19574385812892134\n",
      "2024-12-22 07:39:30,074 - INFO - Epoch 8190, Total Loss: 7617.2212, Loss U: 805.5762, Loss P: 67.8893, Entropy: 116.0597, Entropy Lam: 0.19574385812892134\n",
      "2024-12-22 07:39:30,171 - INFO - Epoch 8200, Total Loss: 7619.4961, Loss U: 805.4961, Loss P: 67.8916, Entropy: 115.3834, Entropy Lam: 0.21531824394181348\n",
      "2024-12-22 07:39:30,251 - INFO - Epoch 8210, Total Loss: 7619.4707, Loss U: 805.9062, Loss P: 67.8882, Entropy: 114.9281, Entropy Lam: 0.21531824394181348\n",
      "2024-12-22 07:39:30,332 - INFO - Epoch 8220, Total Loss: 7619.4746, Loss U: 805.8848, Loss P: 67.8887, Entropy: 114.7919, Entropy Lam: 0.21531824394181348\n",
      "2024-12-22 07:39:30,408 - INFO - Epoch 8230, Total Loss: 7619.4663, Loss U: 805.9141, Loss P: 67.8888, Entropy: 114.5740, Entropy Lam: 0.21531824394181348\n",
      "2024-12-22 07:39:30,491 - INFO - Epoch 8240, Total Loss: 7619.5488, Loss U: 806.1777, Loss P: 67.8874, Entropy: 114.3808, Entropy Lam: 0.21531824394181348\n",
      "2024-12-22 07:39:30,561 - INFO - Epoch 8250, Total Loss: 7619.5127, Loss U: 805.8223, Loss P: 67.8908, Entropy: 114.3040, Entropy Lam: 0.23685006833599484\n",
      "2024-12-22 07:39:30,621 - INFO - Epoch 8260, Total Loss: 7621.9136, Loss U: 806.3242, Loss P: 67.8877, Entropy: 113.2302, Entropy Lam: 0.23685006833599484\n",
      "2024-12-22 07:39:30,697 - INFO - Epoch 8270, Total Loss: 7621.8945, Loss U: 806.5820, Loss P: 67.8865, Entropy: 112.5635, Entropy Lam: 0.23685006833599484\n",
      "2024-12-22 07:39:30,779 - INFO - Epoch 8280, Total Loss: 7621.8760, Loss U: 806.4766, Loss P: 67.8887, Entropy: 111.9973, Entropy Lam: 0.23685006833599484\n",
      "2024-12-22 07:39:30,867 - INFO - Epoch 8290, Total Loss: 7621.8691, Loss U: 806.5605, Loss P: 67.8898, Entropy: 111.1638, Entropy Lam: 0.23685006833599484\n",
      "2024-12-22 07:39:30,953 - INFO - Epoch 8300, Total Loss: 7621.9194, Loss U: 806.5312, Loss P: 67.8926, Entropy: 110.3067, Entropy Lam: 0.23685006833599484\n",
      "2024-12-22 07:39:31,030 - INFO - Epoch 8310, Total Loss: 7624.4336, Loss U: 806.5703, Loss P: 67.8945, Entropy: 109.0748, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,102 - INFO - Epoch 8320, Total Loss: 7624.4053, Loss U: 807.1172, Loss P: 67.8944, Entropy: 106.9008, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,178 - INFO - Epoch 8330, Total Loss: 7624.2495, Loss U: 807.3359, Loss P: 67.8973, Entropy: 104.3277, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,267 - INFO - Epoch 8340, Total Loss: 7624.1621, Loss U: 807.2656, Loss P: 67.9050, Entropy: 101.3356, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,431 - INFO - Epoch 8350, Total Loss: 7623.9697, Loss U: 807.6406, Loss P: 67.9081, Entropy: 97.9531, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,500 - INFO - Epoch 8360, Total Loss: 7623.7769, Loss U: 807.7617, Loss P: 67.9136, Entropy: 94.6340, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,548 - INFO - Epoch 8370, Total Loss: 7623.5366, Loss U: 807.5879, Loss P: 67.9215, Entropy: 91.3542, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,591 - INFO - Epoch 8380, Total Loss: 7623.3140, Loss U: 807.6406, Loss P: 67.9265, Entropy: 88.3846, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,639 - INFO - Epoch 8390, Total Loss: 7623.1084, Loss U: 807.7617, Loss P: 67.9298, Entropy: 85.8339, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,692 - INFO - Epoch 8400, Total Loss: 7622.9248, Loss U: 807.8477, Loss P: 67.9326, Entropy: 83.7559, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,756 - INFO - Epoch 8410, Total Loss: 7622.7671, Loss U: 807.8867, Loss P: 67.9348, Entropy: 82.1242, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,810 - INFO - Epoch 8420, Total Loss: 7622.6650, Loss U: 807.9414, Loss P: 67.9366, Entropy: 80.8615, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,865 - INFO - Epoch 8430, Total Loss: 7622.5483, Loss U: 807.8418, Loss P: 67.9388, Entropy: 79.9314, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,920 - INFO - Epoch 8440, Total Loss: 7622.4331, Loss U: 807.9219, Loss P: 67.9388, Entropy: 79.1950, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:31,990 - INFO - Epoch 8450, Total Loss: 7622.3613, Loss U: 807.9121, Loss P: 67.9396, Entropy: 78.6326, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,050 - INFO - Epoch 8460, Total Loss: 7622.3086, Loss U: 807.8965, Loss P: 67.9404, Entropy: 78.1879, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,150 - INFO - Epoch 8470, Total Loss: 7622.2593, Loss U: 807.9004, Loss P: 67.9408, Entropy: 77.8307, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,241 - INFO - Epoch 8480, Total Loss: 7622.2202, Loss U: 807.9180, Loss P: 67.9410, Entropy: 77.5435, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,325 - INFO - Epoch 8490, Total Loss: 7622.1890, Loss U: 807.9336, Loss P: 67.9411, Entropy: 77.3055, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,404 - INFO - Epoch 8500, Total Loss: 7622.1826, Loss U: 808.0020, Loss P: 67.9409, Entropy: 77.1010, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,479 - INFO - Epoch 8510, Total Loss: 7622.1504, Loss U: 808.0176, Loss P: 67.9409, Entropy: 76.9315, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,528 - INFO - Epoch 8520, Total Loss: 7622.1123, Loss U: 807.8594, Loss P: 67.9424, Entropy: 76.7993, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,590 - INFO - Epoch 8530, Total Loss: 7622.0942, Loss U: 807.9355, Loss P: 67.9418, Entropy: 76.6821, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,655 - INFO - Epoch 8540, Total Loss: 7622.0811, Loss U: 807.9219, Loss P: 67.9421, Entropy: 76.5694, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,706 - INFO - Epoch 8550, Total Loss: 7622.0645, Loss U: 807.9258, Loss P: 67.9421, Entropy: 76.4808, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,758 - INFO - Epoch 8560, Total Loss: 7622.0513, Loss U: 807.9258, Loss P: 67.9422, Entropy: 76.3954, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,809 - INFO - Epoch 8570, Total Loss: 7622.0410, Loss U: 807.9219, Loss P: 67.9423, Entropy: 76.3233, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,861 - INFO - Epoch 8580, Total Loss: 7622.0288, Loss U: 807.9219, Loss P: 67.9424, Entropy: 76.2580, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,922 - INFO - Epoch 8590, Total Loss: 7622.0171, Loss U: 807.9258, Loss P: 67.9424, Entropy: 76.1981, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:32,978 - INFO - Epoch 8600, Total Loss: 7622.0068, Loss U: 807.9258, Loss P: 67.9424, Entropy: 76.1448, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,046 - INFO - Epoch 8610, Total Loss: 7621.9966, Loss U: 807.9355, Loss P: 67.9424, Entropy: 76.0968, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,108 - INFO - Epoch 8620, Total Loss: 7621.9990, Loss U: 808.0312, Loss P: 67.9415, Entropy: 76.0548, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,175 - INFO - Epoch 8630, Total Loss: 7622.0088, Loss U: 807.9883, Loss P: 67.9422, Entropy: 75.9972, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,232 - INFO - Epoch 8640, Total Loss: 7621.9922, Loss U: 808.0156, Loss P: 67.9418, Entropy: 75.9733, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,298 - INFO - Epoch 8650, Total Loss: 7621.9785, Loss U: 807.9102, Loss P: 67.9428, Entropy: 75.9472, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,354 - INFO - Epoch 8660, Total Loss: 7621.9712, Loss U: 807.8926, Loss P: 67.9430, Entropy: 75.9129, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,398 - INFO - Epoch 8670, Total Loss: 7621.9653, Loss U: 807.9102, Loss P: 67.9429, Entropy: 75.8808, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,443 - INFO - Epoch 8680, Total Loss: 7621.9658, Loss U: 807.8984, Loss P: 67.9430, Entropy: 75.8587, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,496 - INFO - Epoch 8690, Total Loss: 7621.9824, Loss U: 807.8301, Loss P: 67.9439, Entropy: 75.8371, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,542 - INFO - Epoch 8700, Total Loss: 7621.9580, Loss U: 807.8945, Loss P: 67.9431, Entropy: 75.8072, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,594 - INFO - Epoch 8710, Total Loss: 7621.9521, Loss U: 807.9863, Loss P: 67.9422, Entropy: 75.7778, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,724 - INFO - Epoch 8720, Total Loss: 7621.9448, Loss U: 807.9102, Loss P: 67.9429, Entropy: 75.7657, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,771 - INFO - Epoch 8730, Total Loss: 7621.9424, Loss U: 807.9004, Loss P: 67.9431, Entropy: 75.7450, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,822 - INFO - Epoch 8740, Total Loss: 7621.9399, Loss U: 807.9277, Loss P: 67.9428, Entropy: 75.7262, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,872 - INFO - Epoch 8750, Total Loss: 7621.9316, Loss U: 807.9238, Loss P: 67.9428, Entropy: 75.7090, Entropy Lam: 0.26053507516959434\n",
      "2024-12-22 07:39:33,923 - INFO - Epoch 8760, Total Loss: 7623.9229, Loss U: 807.6211, Loss P: 67.9468, Entropy: 75.4319, Entropy Lam: 0.2865885826865538\n",
      "2024-12-22 07:39:33,973 - INFO - Epoch 8770, Total Loss: 7623.8848, Loss U: 807.5859, Loss P: 67.9468, Entropy: 75.4376, Entropy Lam: 0.2865885826865538\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m g_rel_alpha_w_grid[graph_name][rel_alpha]:\n\u001b[0;32m    115\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating with parameters: w=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, rel_alpha=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrel_alpha\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, G_name=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgraph_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, G=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(G)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, seed=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 117\u001b[0m     algo_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_alpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_params\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgraph_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m algo_name \u001b[38;5;129;01min\u001b[39;00m algo_results:\n\u001b[0;32m    119\u001b[0m         row \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    120\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph\u001b[39m\u001b[38;5;124m\"\u001b[39m: graph_name,\n\u001b[0;32m    121\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgo\u001b[39m\u001b[38;5;124m\"\u001b[39m: algo_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39malgo_results[algo_name]\n\u001b[0;32m    125\u001b[0m         }\n",
      "Cell \u001b[1;32mIn[3], line 48\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(G, alpha, w, seed, beta)\u001b[0m\n\u001b[0;32m     31\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGreedy Local Search: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_clusters=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_greedy_local_search[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_clusters\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub_opt_gap=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_greedy_local_search[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msub_opt_gap\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_greedy_local_search[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_U=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_greedy_local_search[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_U\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_P=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_greedy_local_search[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_P\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m soft_optim_katz \u001b[38;5;241m=\u001b[39m SoftColorOptimizer(\n\u001b[0;32m     39\u001b[0m     G, k_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, w\u001b[38;5;241m=\u001b[39mw,\n\u001b[0;32m     40\u001b[0m     use_katz_utility \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, seed\u001b[38;5;241m=\u001b[39mseed\n\u001b[0;32m     46\u001b[0m )\n\u001b[1;32m---> 48\u001b[0m \u001b[43msoft_optim_katz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_report_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m sf_eval_soft_assign_katz \u001b[38;5;241m=\u001b[39m SamplingFreeEvaluator(G, soft_optim_katz\u001b[38;5;241m.\u001b[39mcolors, w\u001b[38;5;241m=\u001b[39mw, alpha\u001b[38;5;241m=\u001b[39malpha, beta\u001b[38;5;241m=\u001b[39mbeta, use_katz_utility\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     50\u001b[0m eval_soft_assign_katz \u001b[38;5;241m=\u001b[39m sf_eval_soft_assign_katz\u001b[38;5;241m.\u001b[39mget_results()\n",
      "File \u001b[1;32m~\\Desktop\\Data\\Professional\\master\\AnonymiGraph\\src\\anonymigraph\\anonymization\\method_private_colors_soft_assignment.py:142\u001b[0m, in \u001b[0;36mSoftColorOptimizer.fit\u001b[1;34m(self, max_epochs, epoch_report_frequency)\u001b[0m\n\u001b[0;32m    139\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m loss_U \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m*\u001b[39m loss_P \u001b[38;5;241m+\u001b[39m entropy_scheduler\u001b[38;5;241m.\u001b[39mlam \u001b[38;5;241m*\u001b[39m entropy\n\u001b[0;32m    141\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 142\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_log\u001b[38;5;241m.\u001b[39mappend(total_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\jmeis\\Desktop\\Data\\Professional\\master\\AnonymiGraph\\venv\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jmeis\\Desktop\\Data\\Professional\\master\\AnonymiGraph\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jmeis\\Desktop\\Data\\Professional\\master\\AnonymiGraph\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate(G, alpha, w, seed, beta=1):\n",
    "    eager_local_search = LocalSearchColorOptimizer(G, w=w, alpha=alpha, beta=beta, is_eager=True)\n",
    "    eager_local_search.fit(seed = seed)\n",
    "\n",
    "    greedy_local_search = LocalSearchColorOptimizer(G, w=w, alpha=alpha, beta=beta, is_eager=False)\n",
    "    greedy_local_search.fit()\n",
    "\n",
    "    opti1d_coloropti = Optimal1dColorOptimizer(G, w=w, alpha=alpha, beta=beta)\n",
    "    opti1d_coloropti.fit()\n",
    "\n",
    "    sf_eval_opti1d = SamplingFreeEvaluator(G, opti1d_coloropti.colors, w=w, alpha=alpha, beta=beta, use_katz_utility=True)\n",
    "    eval_opti1d = sf_eval_opti1d.get_results()\n",
    "    logging.info(f\"Optimal 1D         : \"\n",
    "                 f\"num_clusters={eval_opti1d['num_clusters']}, \"\n",
    "                 f\"sub_opt_gap={eval_opti1d['sub_opt_gap']:.5g}, \"\n",
    "                 f\"total_loss={eval_opti1d['total_loss']:.5g}, \"\n",
    "                 f\"loss_U={eval_opti1d['loss_U']:.5g}, \"\n",
    "                 f\"loss_P={eval_opti1d['loss_P']:.5g} \")\n",
    "\n",
    "    sf_eval_eager_local_search = SamplingFreeEvaluator(G, eager_local_search.colors, w=w, alpha=alpha, beta=beta, use_katz_utility=True)\n",
    "    eval_eager_local_search = sf_eval_eager_local_search.get_results()\n",
    "    logging.info(f\"Eager Local Search : \"\n",
    "                f\"num_clusters={eval_eager_local_search['num_clusters']}, \"\n",
    "                f\"sub_opt_gap={eval_eager_local_search['sub_opt_gap']:.5g}, \"\n",
    "                f\"total_loss={eval_eager_local_search['total_loss']:.5g}, \"\n",
    "                f\"loss_U={eval_eager_local_search['loss_U']:.5g}, \"\n",
    "                f\"loss_P={eval_eager_local_search['loss_P']:.5g} \")\n",
    "\n",
    "    sf_eval_greedy_local_search = SamplingFreeEvaluator(G, greedy_local_search.colors, w=w, alpha=alpha, beta=beta, use_katz_utility=True)\n",
    "    eval_greedy_local_search = sf_eval_greedy_local_search.get_results()\n",
    "    logging.info(f\"Greedy Local Search: \"\n",
    "                f\"num_clusters={eval_greedy_local_search['num_clusters']}, \"\n",
    "                f\"sub_opt_gap={eval_greedy_local_search['sub_opt_gap']:.5g}, \"\n",
    "                f\"total_loss={eval_greedy_local_search['total_loss']:.5g}, \"\n",
    "                f\"loss_U={eval_greedy_local_search['loss_U']:.5g}, \"\n",
    "                f\"loss_P={eval_greedy_local_search['loss_P']:.5g} \")\n",
    "\n",
    "    soft_optim_katz = SoftColorOptimizer(\n",
    "        G, k_max=50, w=w,\n",
    "        use_katz_utility = True,\n",
    "        use_entropy_reg = True,\n",
    "        alpha=alpha, beta=beta,\n",
    "        eps_utility = 1e-9, eps_privacy = 1e-9,\n",
    "        lr=0.1, patience=20, threshold=1e-3, initial_lam=1e-7, factor=1.1,\n",
    "        device='cpu', seed=seed\n",
    "    )\n",
    "\n",
    "    soft_optim_katz.fit(max_epochs=int(1e8), epoch_report_frequency=10)\n",
    "    sf_eval_soft_assign_katz = SamplingFreeEvaluator(G, soft_optim_katz.colors, w=w, alpha=alpha, beta=beta, use_katz_utility=True)\n",
    "    eval_soft_assign_katz = sf_eval_soft_assign_katz.get_results()\n",
    "    logging.info(f\"Soft Assign w/ Entropy Reg.: \"\n",
    "                f\"num_clusters={eval_soft_assign_katz['num_clusters']}, \"\n",
    "                f\"sub_opt_gap={eval_soft_assign_katz['sub_opt_gap']:.5g}, \"\n",
    "                f\"total_loss={eval_soft_assign_katz['total_loss']:.5g}, \"\n",
    "                f\"loss_U={eval_soft_assign_katz['loss_U']:.5g}, \"\n",
    "                f\"loss_P={eval_soft_assign_katz['loss_P']:.5g} \")\n",
    "\n",
    "    return {\"Optimal 1D\": eval_opti1d, \"First-Improv.\": eval_eager_local_search, \"Best-Improv.\": eval_greedy_local_search, \"Soft Assign\": eval_soft_assign_katz}\n",
    "\n",
    "\n",
    "def no_self_loops_LFR(*args, **kwargs):\n",
    "    G = nx.generators.community.LFR_benchmark_graph(*args, **kwargs)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    return G\n",
    "\n",
    "\n",
    "graph_types = {\n",
    "    \"Erdos-Renyi\":  nx.erdos_renyi_graph,\n",
    "    \"Barabasi-Albert\": nx.barabasi_albert_graph,\n",
    "    \"LFR\": no_self_loops_LFR\n",
    "}\n",
    "\n",
    "graph_params = {\n",
    "    \"Erdos-Renyi\": {\"n\": 200, \"p\": 0.05, \"seed\": 1, \"g_type\": \"Erdos-Renyi\"}, # Erdos-Renyi - No High Degree Nodes\n",
    "    \"Barabasi-Albert\": {\"n\": 200, \"m\": 5, \"seed\": 1, \"g_type\": \"Barabasi-Albert\"},\n",
    "    \"LFR\": {\"n\": 200, \"tau1\": 3, \"tau2\": 1.5, \"mu\": 0.1, \"average_degree\": 7, \"seed\": 42, \"min_community\": 60, \"g_type\": \"LFR\"} # Default LFR - PowerLaw + Communities\n",
    "    #\"Barabasi-Albert (Dense)\": {\"n\": 200, \"m\": 10, \"seed\": 42, \"g_type\": \"Barabasi-Albert\"},\n",
    "}\n",
    "\n",
    "\n",
    "# Encodes grid of Graph x Relative Alpha x w (dicts are nested in this order)\n",
    "g_rel_alpha_w_grid = {\n",
    "    \"Erdos-Renyi\": {\n",
    "        0.9:  10**(-np.linspace(-2, 2, 4 + 1)),\n",
    "        0.01: 10**(-np.linspace(4, 8, 4 + 1)),\n",
    "    },\n",
    "    \"Barabasi-Albert\": {\n",
    "        0.9: 10**(-np.linspace(-2, 2, 4 + 1)),\n",
    "        0.01: 10**(-np.linspace(4, 8, 4 + 1)),\n",
    "    },\n",
    "    \"LFR\": {\n",
    "        0.9:  10**(-np.linspace(-1, 2, 3 + 1)),\n",
    "        0.01: 10**(-np.linspace(4, 9, 4 + 1)),\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "graphs = {}\n",
    "\n",
    "for graph_name in g_rel_alpha_w_grid.keys():\n",
    "    params = graph_params[graph_name].copy()\n",
    "    g_type = params.pop(\"g_type\")\n",
    "    graphs[graph_name] = graph_types[g_type](**params)\n",
    "\n",
    "results = []\n",
    "\n",
    "for graph_name in g_rel_alpha_w_grid.keys():\n",
    "    G = graphs[graph_name]\n",
    "\n",
    "    eigenvalues, _ = eigs(nx.adjacency_matrix(G).astype(np.float64), k=1, which='LM')\n",
    "    max_alpha = 1 / np.abs(eigenvalues).max()\n",
    "\n",
    "    for rel_alpha in g_rel_alpha_w_grid[graph_name]:\n",
    "        for w in g_rel_alpha_w_grid[graph_name][rel_alpha]:\n",
    "            logging.info(f\"Evaluating with parameters: w={w:.3g}, rel_alpha={rel_alpha:.3g}, G_name={graph_name}, G={str(G)}, seed={params[\"seed\"]}\")\n",
    "\n",
    "            algo_results = evaluate(G, rel_alpha * max_alpha, w, graph_params[graph_name][\"seed\"])\n",
    "            for algo_name in algo_results:\n",
    "                row = {\n",
    "                    \"graph\": graph_name,\n",
    "                    \"algo\": algo_name,\n",
    "                    \"rel_alpha\": rel_alpha,\n",
    "                    \"w\": w,\n",
    "                    **algo_results[algo_name]\n",
    "                }\n",
    "                results.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "rank_counts = Counter()\n",
    "rows = []\n",
    "\n",
    "for graph_name in g_rel_alpha_w_grid.keys():\n",
    "    for rel_alpha in g_rel_alpha_w_grid[graph_name]:\n",
    "        for w in g_rel_alpha_w_grid[graph_name][rel_alpha]:\n",
    "            df_graph = df_results[(df_results['graph'] == graph_name) & (df_results['rel_alpha'] == rel_alpha) & (df_results['w'] == w)]\n",
    "            num_clusters_opt1d = df_graph[df_graph[\"algo\"] == \"Optimal 1D\"][\"num_clusters\"].item()\n",
    "            gap_opt1d = df_graph[df_graph[\"algo\"] == \"Optimal 1D\"][\"sub_opt_gap\"].item()\n",
    "            gap_eager = df_graph[df_graph[\"algo\"] == \"First-Improv.\"][\"sub_opt_gap\"].item()\n",
    "            gap_greedy = df_graph[df_graph[\"algo\"] == \"Best-Improv.\"][\"sub_opt_gap\"].item()\n",
    "            gap_soft = df_graph[df_graph[\"algo\"] == \"Soft Assign\"][\"sub_opt_gap\"].item()\n",
    "\n",
    "            gaps = {\"Soft Assign\": gap_soft, \"Optimal 1D\": gap_opt1d,\"Eager\": gap_eager,\"Greedy\": gap_greedy, \"num_clusters_opt1d\":num_clusters_opt1d}\n",
    "            rows.append(gaps)\n",
    "\n",
    "            # for ranking\n",
    "            df = pd.DataFrame(list(gaps.items()), columns=['algo', 'gap'])\n",
    "            df['rank'] = df['gap'].rank(method='average', ascending=True)\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                rank_counts[(row['algo'], row['rank'])] += 1\n",
    "\n",
    "average_ranks = (\n",
    "    pd.DataFrame([(algo, rank * freq, freq) for (algo, rank), freq in rank_counts.items()],\n",
    "                 columns=[\"algo\", \"rank_sum\", \"count\"])\n",
    "    .groupby(\"algo\")\n",
    "    .apply(lambda x: x[\"rank_sum\"].sum() / x[\"count\"].sum(), include_groups=False)\n",
    "    .sort_values()\n",
    ")\n",
    "display(average_ranks)\n",
    "\n",
    "\n",
    "# Normalize\n",
    "df = pd.DataFrame(rows)\n",
    "df[\"Soft Assign\"] = df[\"Soft Assign\"] / df[\"Greedy\"]\n",
    "df[\"Optimal 1D\"] = df[\"Optimal 1D\"] / df[\"Greedy\"]\n",
    "df[\"Eager\"] = df[\"Eager\"] / df[\"Greedy\"]\n",
    "df[\"Greedy\"] = df[\"Greedy\"] / df[\"Greedy\"]\n",
    "\n",
    "# Hue Choice\n",
    "df.rename(columns={\"num_clusters_opt1d\": \"hue\"}, inplace=True)\n",
    "#df['hue'] = df[\"Optimal 1D\"]\n",
    "df['hue'] = df['hue'] + np.random.normal(0, 1e-7, size=len(df)) # need unique values here, otherwise we plot areas in parallel coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = df.where(df <= 2, np.nan)\n",
    "df_stats = df_stats.drop(\"hue\", axis=1)\n",
    "stats = df_stats.describe()\n",
    "stats = stats.drop([\"count\", \"25%\", \"75%\", \"std\"])\n",
    "latex_code = stats.to_latex(index=True,\n",
    "                         caption=\"Descriptive statistics for the normalized suboptimality gaps for the proposed algorithms across various graphs and hyperparameter configurations.\",\n",
    "                         label=\"tab:exp1_descriptive\",\n",
    "                         bold_rows=True,\n",
    "                         column_format=\"lcccc\",\n",
    "                         float_format=\"%.2f\"\n",
    ")\n",
    "\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plot\n",
    "df_long_no_outlier = df.melt(id_vars='hue', var_name='algorithm', value_name='gap')\n",
    "df_long_no_outlier[\"gap\"] = df_long_no_outlier[\"gap\"].where(df_long_no_outlier[\"gap\"] <= 2, np.nan)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\", palette=\"pastel\", font_scale=1.5)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='algorithm', y='gap', data=df_long_no_outlier, inner=None, linewidth=1, density_norm=\"width\", bw_method=0.4)\n",
    "sns.stripplot(x='algorithm', y='gap', data=df_long_no_outlier, jitter=True, edgecolor=\"black\", linewidth=0.5, alpha=0.8)\n",
    "\n",
    "plt.ylabel('Normalized Suboptimality Gap')\n",
    "plt.xlabel('Algorithm')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"suboptimality_gap_improvements.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_dfs = {}\n",
    "for graph_name in g_rel_alpha_w_grid.keys():\n",
    "    for rel_alpha in g_rel_alpha_w_grid[graph_name]:\n",
    "        df_graph = df_results[(df_results['graph'] == graph_name) & (df_results['rel_alpha'] == rel_alpha)]\n",
    "        pivot_df = df_graph.pivot(index='w', columns=['algo'], values=['sub_opt_gap', 'num_clusters', 'total_loss', 'loss_U', 'loss_P'])\n",
    "        pivot_df.sort_index(axis=0, inplace=True, ascending=False)\n",
    "        pivot_df = pivot_df.reindex(columns=['First-Improv.', 'Best-Improv.', 'Optimal 1D', 'Soft Assign'], level=1)\n",
    "\n",
    "        pivot_dfs[graph_name] = pivot_dfs.get(graph_name, {})\n",
    "        pivot_dfs[graph_name][rel_alpha] = pivot_df\n",
    "\n",
    "        print(f\"Graph Name: {graph_name}, rel_alpha: {rel_alpha}, {str(graphs[graph_name])}\")\n",
    "        display(pivot_df.style.format(formatter=\"{:.4g}\").format_index(\"{:.3g}\", axis=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
