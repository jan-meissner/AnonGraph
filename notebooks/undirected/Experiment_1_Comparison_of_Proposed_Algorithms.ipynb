{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import eigs\n",
    "import logging\n",
    "\n",
    "from anonymigraph.anonymization.method_private_colors_climbing import (\n",
    "    LocalSearchColorOptimizer,\n",
    "    Optimal1dColorOptimizer,\n",
    ")\n",
    "    \n",
    "from anonymigraph.anonymization._method_private_colors.helpers import SamplingFreeEvaluator\n",
    "from anonymigraph.anonymization.method_private_colors_soft_assignment import SoftColorOptimizer\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating with parameters: w=100, rel_alpha=0.9, G_name=Erdos-Renyi, G=Graph with 200 nodes and 1025 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=8128.1646, loss_U=1666.4085, loss_P=64.6176\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=8226.6103, loss_U=621.0867, loss_P=76.0552\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=1,  total_loss=8128.1646, loss_U=1666.4085, loss_P=64.6176\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=1, sub_opt_gap=1666.4, total_loss=8128.2, loss_U=1666.4, loss_P=64.618 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=2, sub_opt_gap=1251.7, total_loss=7713.4, loss_U=824.89, loss_P=68.885 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=2, sub_opt_gap=1255.7, total_loss=7717.5, loss_U=782.75, loss_P=69.347 \n",
      "C:\\Users\\jmeis\\Desktop\\Data\\Professional\\master\\AnonGraph\\src\\anonymigraph\\anonymization\\method_private_colors_soft_assignment.py:108: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\SparseCsrTensorImpl.cpp:55.)\n",
      "  self.A_pytorch = torch.sparse_csr_tensor(crow_indices, col_indices, values, size=A_scipy.shape)\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 9021.9512, Loss U: 1660.0713, Loss P: 73.6188, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 7593.3853, Loss U: 806.8359, Loss P: 67.8655, Entropy: 456.0085, Entropy Lam: 1.1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 7593.0854, Loss U: 806.1113, Loss P: 67.8697, Entropy: 418.6511, Entropy Lam: 3.797498335832414e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 7593.0127, Loss U: 806.1504, Loss P: 67.8686, Entropy: 398.3146, Entropy Lam: 2.8102436848064324e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 7593.0322, Loss U: 806.0898, Loss P: 67.8693, Entropy: 384.3452, Entropy Lam: 3.0448163954141954e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 7593.0991, Loss U: 806.2852, Loss P: 67.8668, Entropy: 366.1617, Entropy Lam: 0.00036288659325512675\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 7593.8535, Loss U: 806.3867, Loss P: 67.8660, Entropy: 292.2643, Entropy Lam: 0.0029539966406591186\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 7000, Total Loss: 7595.5869, Loss U: 806.4043, Loss P: 67.8663, Entropy: 170.6411, Entropy Lam: 0.01493088824218049\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 8000, Total Loss: 7600.6333, Loss U: 805.6133, Loss P: 67.8794, Entropy: 124.9246, Entropy Lam: 0.05670002325218018\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 9000, Total Loss: 7622.1812, Loss U: 807.9355, Loss P: 67.9413, Entropy: 77.2244, Entropy Lam: 0.26053507516959434\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 10000, Total Loss: 7651.7725, Loss U: 801.5879, Loss P: 68.0339, Entropy: 69.2466, Entropy Lam: 0.6757608868127227\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 11000, Total Loss: 7705.3599, Loss U: 803.5176, Loss P: 68.1802, Entropy: 52.6078, Entropy Lam: 1.593408822728173\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 12000, Total Loss: 7790.9170, Loss U: 794.5957, Loss P: 68.7277, Entropy: 32.8843, Entropy Lam: 3.7571746543709272\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 13000, Total Loss: 7894.9087, Loss U: 814.4062, Loss P: 69.1977, Entropy: 19.9576, Entropy Lam: 8.053837546325145\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 14000, Total Loss: 8083.9790, Loss U: 824.0957, Loss P: 70.7177, Entropy: 7.4423, Entropy Lam: 25.27639229688789\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 15000, Total Loss: 8258.1553, Loss U: 821.5547, Loss P: 72.7834, Entropy: 2.1944, Entropy Lam: 72.11649713244103\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 15432, Total Loss: 8316.3223, Loss U: 818.7559, Loss P: 74.8742, Entropy: 0.0794, Entropy Lam: 127.75877377644444\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 3/50\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=3, sub_opt_gap=1850.5, total_loss=8312.3, loss_U=818.58, loss_P=74.937 \n",
      "INFO:root:Evaluating with parameters: w=10, rel_alpha=0.9, G_name=Erdos-Renyi, G=Graph with 200 nodes and 1025 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=2312.5841, loss_U=1666.4085, loss_P=64.6176\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=1381.6391, loss_U=621.0867, loss_P=76.0552\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=1187.5235, loss_U=314.0244, loss_P=87.3499\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=1168.6557, loss_U=168.3568, loss_P=100.0299\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=1277.9825, loss_U=105.3483, loss_P=117.2634\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=1311.6016, loss_U=70.3869, loss_P=124.1215\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=1403.5377, loss_U=55.4069, loss_P=134.8131\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=4,  total_loss=1168.6557, loss_U=168.3568, loss_P=100.0299\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=4, sub_opt_gap=522.48, total_loss=1168.7, loss_U=168.36, loss_P=100.03 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=4, sub_opt_gap=473.48, total_loss=1119.7, loss_U=219.79, loss_P=89.986 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=4, sub_opt_gap=472.21, total_loss=1118.4, loss_U=214.23, loss_P=90.415 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 2396.2593, Loss U: 1660.0713, Loss P: 73.6188, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 1074.3813, Loss U: 186.1543, Loss P: 88.8227, Entropy: 207.9253, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 1074.2843, Loss U: 186.7090, Loss P: 88.7575, Entropy: 184.4171, Entropy Lam: 7.400249944258173e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 1074.1592, Loss U: 187.0059, Loss P: 88.7152, Entropy: 194.3898, Entropy Lam: 6.024006916124241e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 1074.1327, Loss U: 187.1211, Loss P: 88.6998, Entropy: 184.8826, Entropy Lam: 7.17951778908585e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 1074.2677, Loss U: 187.1719, Loss P: 88.6943, Entropy: 178.8762, Entropy Lam: 0.0008556676046607829\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 1075.3961, Loss U: 186.5449, Loss P: 88.7719, Entropy: 122.1063, Entropy Lam: 0.009270906881783092\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 7000, Total Loss: 1076.8967, Loss U: 187.5000, Loss P: 88.7858, Entropy: 52.8750, Entropy Lam: 0.029096077235726082\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 8000, Total Loss: 1084.5421, Loss U: 182.3770, Loss P: 89.3718, Entropy: 43.1529, Entropy Lam: 0.19574385812892134\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 9000, Total Loss: 1101.8790, Loss U: 170.5957, Loss P: 91.3601, Entropy: 23.7877, Entropy Lam: 0.743336975493995\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 10000, Total Loss: 1121.3221, Loss U: 167.1250, Loss P: 94.7778, Entropy: 1.8794, Entropy Lam: 3.415613322155388\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 10221, Total Loss: 1121.4719, Loss U: 161.5391, Loss P: 95.9527, Entropy: 0.0812, Entropy Lam: 5.000799464967706\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 5/50\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=5, sub_opt_gap=475.08, total_loss=1121.3, loss_U=161.51, loss_P=95.974 \n",
      "INFO:root:Evaluating with parameters: w=1, rel_alpha=0.9, G_name=Erdos-Renyi, G=Graph with 200 nodes and 1025 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=1731.0260, loss_U=1666.4085, loss_P=64.6176\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=697.1419, loss_U=621.0867, loss_P=76.0552\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=401.3743, loss_U=314.0244, loss_P=87.3499\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=268.3867, loss_U=168.3568, loss_P=100.0299\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=222.6117, loss_U=105.3483, loss_P=117.2634\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=194.5084, loss_U=70.3869, loss_P=124.1215\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=190.2200, loss_U=55.4069, loss_P=134.8131\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=194.9837, loss_U=41.3792, loss_P=153.6044\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=207.0354, loss_U=33.9675, loss_P=173.0680\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=208.4056, loss_U=26.9011, loss_P=181.5045\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=7,  total_loss=190.2200, loss_U=55.4069, loss_P=134.8131\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=7, sub_opt_gap=125.6, total_loss=190.22, loss_U=55.407, loss_P=134.81 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=8, sub_opt_gap=125.36, total_loss=189.98, loss_U=47.953, loss_P=142.03 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=8, sub_opt_gap=124.44, total_loss=189.06, loss_U=47.746, loss_P=141.31 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 1733.6902, Loss U: 1660.0713, Loss P: 73.6188, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 180.7656, Loss U: 47.4844, Loss P: 133.2812, Entropy: 238.0553, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 180.6289, Loss U: 47.6074, Loss P: 133.0214, Entropy: 193.3691, Entropy Lam: 6.115909044841464e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 180.5867, Loss U: 47.7676, Loss P: 132.8170, Entropy: 186.8502, Entropy Lam: 1.1739085287969579e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 180.6129, Loss U: 47.7480, Loss P: 132.8217, Entropy: 174.0609, Entropy Lam: 0.000247856425964843\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 180.9263, Loss U: 47.3457, Loss P: 133.2561, Entropy: 109.8387, Entropy Lam: 0.0029539966406591186\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 182.1976, Loss U: 47.4375, Loss P: 133.4778, Entropy: 64.5285, Entropy Lam: 0.01987301225034224\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 7000, Total Loss: 185.4227, Loss U: 47.7988, Loss P: 134.5198, Entropy: 30.9024, Entropy Lam: 0.10044754989265563\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 8000, Total Loss: 190.7401, Loss U: 44.2676, Loss P: 145.3240, Entropy: 2.0565, Entropy Lam: 0.5584800717460517\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 8078, Total Loss: 190.6626, Loss U: 44.5352, Loss P: 146.0831, Entropy: 0.0657, Entropy Lam: 0.6757608868127227\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 9/50\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=9, sub_opt_gap=126.02, total_loss=190.63, loss_U=44.538, loss_P=146.1 \n",
      "INFO:root:Evaluating with parameters: w=0.1, rel_alpha=0.9, G_name=Erdos-Renyi, G=Graph with 200 nodes and 1025 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=1672.8702, loss_U=1666.4085, loss_P=64.6176\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=628.6922, loss_U=621.0867, loss_P=76.0552\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=322.7594, loss_U=314.0244, loss_P=87.3499\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=178.3598, loss_U=168.3568, loss_P=100.0299\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=117.0746, loss_U=105.3483, loss_P=117.2634\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=82.7991, loss_U=70.3869, loss_P=124.1215\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=68.8882, loss_U=55.4069, loss_P=134.8131\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=56.7397, loss_U=41.3792, loss_P=153.6044\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=51.2743, loss_U=33.9675, loss_P=173.0680\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=45.0516, loss_U=26.9011, loss_P=181.5045\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=40.0397, loss_U=20.4970, loss_P=195.4270\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=38.2448, loss_U=16.7260, loss_P=215.1872\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=36.8181, loss_U=14.0217, loss_P=227.9644\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=37.4125, loss_U=12.1741, loss_P=252.3832\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=38.0453, loss_U=10.6669, loss_P=273.7835\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=38.0038, loss_U=9.1599, loss_P=288.4387\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=38.0385, loss_U=7.8123, loss_P=302.2620\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=38.2331, loss_U=6.5135, loss_P=317.1955\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=39.4119, loss_U=5.8242, loss_P=335.8774\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=13,  total_loss=36.8181, loss_U=14.0217, loss_P=227.9644\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=13, sub_opt_gap=30.356, total_loss=36.818, loss_U=14.022, loss_P=227.96 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=15, sub_opt_gap=29.876, total_loss=36.338, loss_U=11.695, loss_P=246.42 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=15, sub_opt_gap=29.876, total_loss=36.338, loss_U=11.695, loss_P=246.42 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 1667.4333, Loss U: 1660.0713, Loss P: 73.6188, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 35.0286, Loss U: 10.7754, Loss P: 242.5318, Entropy: 99.1714, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 34.9447, Loss U: 10.6484, Loss P: 242.9621, Entropy: 70.1522, Entropy Lam: 6.115909044841464e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 34.9320, Loss U: 10.6523, Loss P: 242.7866, Entropy: 62.6099, Entropy Lam: 1.5624722518287514e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 34.8763, Loss U: 10.6562, Loss P: 242.0137, Entropy: 56.6745, Entropy Lam: 0.0003298969029592061\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 35.1378, Loss U: 10.4434, Loss P: 244.2258, Entropy: 42.9381, Entropy Lam: 0.00633215414369448\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 36.2011, Loss U: 9.4512, Loss P: 259.0576, Entropy: 18.0150, Entropy Lam: 0.046859523348909235\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6849, Total Loss: 36.7308, Loss U: 8.6914, Loss P: 280.2188, Entropy: 0.0812, Entropy Lam: 0.21531824394181348\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 18/50\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=18, sub_opt_gap=30.258, total_loss=36.72, loss_U=8.6881, loss_P=280.32 \n",
      "INFO:root:Evaluating with parameters: w=0.01, rel_alpha=0.9, G_name=Erdos-Renyi, G=Graph with 200 nodes and 1025 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=1667.0547, loss_U=1666.4085, loss_P=64.6176\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=621.8473, loss_U=621.0867, loss_P=76.0552\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=314.8979, loss_U=314.0244, loss_P=87.3499\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=169.3571, loss_U=168.3568, loss_P=100.0299\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=106.5209, loss_U=105.3483, loss_P=117.2634\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=71.6282, loss_U=70.3869, loss_P=124.1215\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=56.7551, loss_U=55.4069, loss_P=134.8131\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=42.9153, loss_U=41.3792, loss_P=153.6044\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=35.6982, loss_U=33.9675, loss_P=173.0680\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=28.7162, loss_U=26.9011, loss_P=181.5045\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=22.4513, loss_U=20.4970, loss_P=195.4270\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=18.8779, loss_U=16.7260, loss_P=215.1872\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=16.3013, loss_U=14.0217, loss_P=227.9644\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=14.6980, loss_U=12.1741, loss_P=252.3832\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=13.4047, loss_U=10.6669, loss_P=273.7835\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=12.0443, loss_U=9.1599, loss_P=288.4387\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=10.8349, loss_U=7.8123, loss_P=302.2620\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=9.6855, loss_U=6.5135, loss_P=317.1955\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=9.1830, loss_U=5.8242, loss_P=335.8774\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=20,  total_loss=8.7302, loss_U=5.1775, loss_P=355.2754\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=21,  total_loss=8.2945, loss_U=4.5905, loss_P=370.3963\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=22,  total_loss=7.9351, loss_U=4.0406, loss_P=389.4451\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=23,  total_loss=7.7456, loss_U=3.6710, loss_P=407.4568\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=24,  total_loss=7.4679, loss_U=3.3046, loss_P=416.3235\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=25,  total_loss=7.3584, loss_U=2.9653, loss_P=439.3072\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=26,  total_loss=7.1783, loss_U=2.6732, loss_P=450.5092\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=27,  total_loss=7.1445, loss_U=2.4461, loss_P=469.8410\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=28,  total_loss=7.1641, loss_U=2.2311, loss_P=493.3003\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=29,  total_loss=7.0835, loss_U=2.0219, loss_P=506.1527\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=30,  total_loss=7.0637, loss_U=1.8409, loss_P=522.2808\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=31,  total_loss=7.1572, loss_U=1.7019, loss_P=545.5337\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=32,  total_loss=7.1899, loss_U=1.5670, loss_P=562.2813\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=33,  total_loss=7.1180, loss_U=1.4335, loss_P=568.4480\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=34,  total_loss=7.2420, loss_U=1.3349, loss_P=590.7020\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=35,  total_loss=7.2746, loss_U=1.2392, loss_P=603.5353\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=36,  total_loss=7.3663, loss_U=1.1489, loss_P=621.7421\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=37,  total_loss=7.3996, loss_U=1.0633, loss_P=633.6254\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=38,  total_loss=7.5980, loss_U=0.9805, loss_P=661.7524\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=39,  total_loss=7.5447, loss_U=0.9072, loss_P=663.7524\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=40,  total_loss=7.6030, loss_U=0.8437, loss_P=675.9286\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=41,  total_loss=7.7270, loss_U=0.7967, loss_P=693.0286\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=42,  total_loss=7.8285, loss_U=0.7504, loss_P=707.8119\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=43,  total_loss=7.8220, loss_U=0.7064, loss_P=711.5619\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=44,  total_loss=7.8280, loss_U=0.6643, loss_P=716.3667\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=45,  total_loss=7.8908, loss_U=0.6263, loss_P=726.4500\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=46,  total_loss=7.9701, loss_U=0.5893, loss_P=738.0833\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=47,  total_loss=8.0017, loss_U=0.5525, loss_P=744.9167\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=30,  total_loss=7.0637, loss_U=1.8409, loss_P=522.2808\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=30, sub_opt_gap=6.4175, total_loss=7.0637, loss_U=1.8409, loss_P=522.28 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=34, sub_opt_gap=6.1699, total_loss=6.816, loss_U=1.8412, loss_P=497.48 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=34, sub_opt_gap=6.1699, total_loss=6.816, loss_U=1.8412, loss_P=497.48 \n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 1660.8076, Loss U: 1660.0713, Loss P: 73.6188, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 7.3131, Loss U: 2.6172, Loss P: 469.5954, Entropy: 56.1163, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 7.2381, Loss U: 2.5547, Loss P: 468.3359, Entropy: 39.8007, Entropy Lam: 1.1918176537727233e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 7.2244, Loss U: 2.5430, Loss P: 468.0180, Entropy: 33.6599, Entropy Lam: 3.684227838451177e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 7.2318, Loss U: 2.5273, Loss P: 468.2186, Entropy: 28.6025, Entropy Lam: 0.0007778796406007117\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 7.3888, Loss U: 2.3789, Loss P: 491.5302, Entropy: 7.6636, Entropy Lam: 0.012339577059653297\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5492, Total Loss: 7.4119, Loss U: 2.3516, Loss P: 505.6134, Entropy: 0.0998, Entropy Lam: 0.042599566680826574\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 31/50\n",
      "INFO:anonymigraph.utils:Katz converged after 287 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=31, sub_opt_gap=6.7532, total_loss=7.3994, loss_U=2.3429, loss_P=505.65 \n",
      "INFO:root:Evaluating with parameters: w=0.0001, rel_alpha=0.01, G_name=Erdos-Renyi, G=Graph with 200 nodes and 1025 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0082, loss_U=0.0018, loss_P=64.6176\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0080, loss_U=0.0006, loss_P=73.7210\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0090, loss_U=0.0003, loss_P=86.9039\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=2,  total_loss=0.0080, loss_U=0.0006, loss_P=73.7210\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=2, sub_opt_gap=0.0015328, total_loss=0.0079945, loss_U=0.00062244, loss_P=73.721 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=2, sub_opt_gap=0.0012489, total_loss=0.0077106, loss_U=0.00080739, loss_P=69.032 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=2, sub_opt_gap=0.0012653, total_loss=0.0077271, loss_U=0.00083668, loss_P=68.904 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0092, Loss U: 0.0018, Loss P: 73.6188, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0079, Loss U: 0.0008, Loss P: 70.1279, Entropy: 17.9223, Entropy Lam: 7.289048368510332e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1307, Total Loss: 0.0079, Loss U: 0.0007, Loss P: 72.2539, Entropy: 0.0922, Entropy Lam: 2.7680149049219957e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 3/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=3, sub_opt_gap=0.00145, total_loss=0.0079118, loss_U=0.00068594, loss_P=72.258 \n",
      "INFO:root:Evaluating with parameters: w=1e-05, rel_alpha=0.01, G_name=Erdos-Renyi, G=Graph with 200 nodes and 1025 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0024, loss_U=0.0018, loss_P=64.6176\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0014, loss_U=0.0006, loss_P=73.7210\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0012, loss_U=0.0003, loss_P=86.9039\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=0.0012, loss_U=0.0002, loss_P=97.2686\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=0.0013, loss_U=0.0001, loss_P=114.9369\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=0.0013, loss_U=0.0001, loss_P=125.8204\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=0.0014, loss_U=0.0001, loss_P=136.3086\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=4,  total_loss=0.0012, loss_U=0.0002, loss_P=97.2686\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=4, sub_opt_gap=0.00051127, total_loss=0.0011574, loss_U=0.00018476, loss_P=97.269 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=4, sub_opt_gap=0.00047522, total_loss=0.0011214, loss_U=0.00022439, loss_P=89.7 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=4, sub_opt_gap=0.00047788, total_loss=0.0011241, loss_U=0.00021313, loss_P=91.093 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0026, Loss U: 0.0018, Loss P: 73.6188, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0012, Loss U: 0.0003, Loss P: 93.9787, Entropy: 0.1296, Entropy Lam: 7.289048368510332e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1053, Total Loss: 0.0012, Loss U: 0.0003, Loss P: 93.9786, Entropy: 0.0998, Entropy Lam: 8.819748525897503e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 5/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=5, sub_opt_gap=0.00049769, total_loss=0.0011439, loss_U=0.00020408, loss_P=93.979 \n",
      "INFO:root:Evaluating with parameters: w=1e-06, rel_alpha=0.01, G_name=Erdos-Renyi, G=Graph with 200 nodes and 1025 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0018, loss_U=0.0018, loss_P=64.6176\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0007, loss_U=0.0006, loss_P=73.7210\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0004, loss_U=0.0003, loss_P=86.9039\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=0.0003, loss_U=0.0002, loss_P=97.2686\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=0.0002, loss_U=0.0001, loss_P=114.9369\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=0.0002, loss_U=0.0001, loss_P=125.8204\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=0.0002, loss_U=0.0001, loss_P=136.3086\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=0.0002, loss_U=0.0000, loss_P=147.6638\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=0.0002, loss_U=0.0000, loss_P=161.2659\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=0.0002, loss_U=0.0000, loss_P=173.9192\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=7,  total_loss=0.0002, loss_U=0.0001, loss_P=136.3086\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=7, sub_opt_gap=0.00012235, total_loss=0.00018696, loss_U=5.0655e-05, loss_P=136.31 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=8, sub_opt_gap=0.00012035, total_loss=0.00018497, loss_U=4.3372e-05, loss_P=141.59 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=8, sub_opt_gap=0.00012035, total_loss=0.00018497, loss_U=4.3372e-05, loss_P=141.59 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0019, Loss U: 0.0018, Loss P: 73.6188, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0004, Loss U: 0.0002, Loss P: 172.8527, Entropy: 0.1495, Entropy Lam: 6.626407607736665e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1091, Total Loss: 0.0003, Loss U: 0.0002, Loss P: 172.8540, Entropy: 0.1000, Entropy Lam: 9.701723378487254e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 11/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=11, sub_opt_gap=0.00014094, total_loss=0.00020556, loss_U=3.2698e-05, loss_P=172.86 \n",
      "INFO:root:Evaluating with parameters: w=1e-07, rel_alpha=0.01, G_name=Erdos-Renyi, G=Graph with 200 nodes and 1025 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0018, loss_U=0.0018, loss_P=64.6176\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0006, loss_U=0.0006, loss_P=73.7210\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0003, loss_U=0.0003, loss_P=86.9039\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=0.0002, loss_U=0.0002, loss_P=97.2686\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=0.0001, loss_U=0.0001, loss_P=114.9369\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=0.0001, loss_U=0.0001, loss_P=125.8204\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=0.0001, loss_U=0.0001, loss_P=136.3086\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=0.0001, loss_U=0.0000, loss_P=147.6638\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=0.0000, loss_U=0.0000, loss_P=161.2659\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=0.0000, loss_U=0.0000, loss_P=173.9192\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=0.0000, loss_U=0.0000, loss_P=195.9416\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=0.0000, loss_U=0.0000, loss_P=209.8183\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=0.0000, loss_U=0.0000, loss_P=230.4730\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=0.0000, loss_U=0.0000, loss_P=246.2771\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=0.0000, loss_U=0.0000, loss_P=255.7104\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=0.0000, loss_U=0.0000, loss_P=277.8778\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=0.0000, loss_U=0.0000, loss_P=292.3469\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=0.0000, loss_U=0.0000, loss_P=303.0628\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=0.0000, loss_U=0.0000, loss_P=323.4322\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=20,  total_loss=0.0000, loss_U=0.0000, loss_P=340.0193\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=21,  total_loss=0.0000, loss_U=0.0000, loss_P=359.6204\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=22,  total_loss=0.0000, loss_U=0.0000, loss_P=380.2906\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=23,  total_loss=0.0000, loss_U=0.0000, loss_P=398.8852\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=24,  total_loss=0.0000, loss_U=0.0000, loss_P=407.9044\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=25,  total_loss=0.0000, loss_U=0.0000, loss_P=419.7258\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=26,  total_loss=0.0000, loss_U=0.0000, loss_P=433.6131\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=15,  total_loss=0.0000, loss_U=0.0000, loss_P=255.7104\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=15, sub_opt_gap=2.0651e-05, total_loss=2.7113e-05, loss_U=1.5419e-06, loss_P=255.71 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=15, sub_opt_gap=2.0651e-05, total_loss=2.7113e-05, loss_U=1.5419e-06, loss_P=255.71 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=15, sub_opt_gap=2.0651e-05, total_loss=2.7113e-05, loss_U=1.5419e-06, loss_P=255.71 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0019, Loss U: 0.0018, Loss P: 73.6188, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0003, Loss U: 0.0002, Loss P: 498.8687, Entropy: 0.1739, Entropy Lam: 6.626407607736665e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1108, Total Loss: 0.0003, Loss U: 0.0002, Loss P: 498.8981, Entropy: 0.0996, Entropy Lam: 1.067189571633598e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 34/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=34, sub_opt_gap=5.1532e-05, total_loss=5.7994e-05, loss_U=8.1004e-06, loss_P=498.93 \n",
      "INFO:root:Evaluating with parameters: w=1e-08, rel_alpha=0.01, G_name=Erdos-Renyi, G=Graph with 200 nodes and 1025 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0018, loss_U=0.0018, loss_P=64.6176\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0006, loss_U=0.0006, loss_P=73.7210\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0003, loss_U=0.0003, loss_P=86.9039\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=0.0002, loss_U=0.0002, loss_P=97.2686\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=0.0001, loss_U=0.0001, loss_P=114.9369\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=0.0001, loss_U=0.0001, loss_P=125.8204\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=0.0001, loss_U=0.0001, loss_P=136.3086\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=0.0000, loss_U=0.0000, loss_P=147.6638\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=0.0000, loss_U=0.0000, loss_P=161.2659\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=0.0000, loss_U=0.0000, loss_P=173.9192\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=0.0000, loss_U=0.0000, loss_P=195.9416\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=0.0000, loss_U=0.0000, loss_P=209.8183\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=0.0000, loss_U=0.0000, loss_P=230.4730\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=0.0000, loss_U=0.0000, loss_P=246.2771\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=0.0000, loss_U=0.0000, loss_P=255.7104\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=0.0000, loss_U=0.0000, loss_P=277.8778\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=0.0000, loss_U=0.0000, loss_P=292.3469\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=0.0000, loss_U=0.0000, loss_P=303.0628\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=0.0000, loss_U=0.0000, loss_P=323.4322\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=20,  total_loss=0.0000, loss_U=0.0000, loss_P=340.0193\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=21,  total_loss=0.0000, loss_U=0.0000, loss_P=359.6204\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=22,  total_loss=0.0000, loss_U=0.0000, loss_P=380.2906\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=23,  total_loss=0.0000, loss_U=0.0000, loss_P=398.8852\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=24,  total_loss=0.0000, loss_U=0.0000, loss_P=407.9044\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=25,  total_loss=0.0000, loss_U=0.0000, loss_P=419.7258\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=26,  total_loss=0.0000, loss_U=0.0000, loss_P=433.6131\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=17,  total_loss=0.0000, loss_U=0.0000, loss_P=292.3469\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=17, sub_opt_gap=2.2891e-06, total_loss=2.9352e-06, loss_U=1.1767e-08, loss_P=292.35 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=17, sub_opt_gap=2.2891e-06, total_loss=2.9352e-06, loss_U=1.1767e-08, loss_P=292.35 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=17, sub_opt_gap=2.2891e-06, total_loss=2.9352e-06, loss_U=1.1767e-08, loss_P=292.35 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0019, Loss U: 0.0018, Loss P: 73.6188, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0003, Loss U: 0.0003, Loss P: 739.1742, Entropy: 0.1707, Entropy Lam: 6.626407607736665e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1121, Total Loss: 0.0003, Loss U: 0.0003, Loss P: 739.2223, Entropy: 0.1000, Entropy Lam: 1.1739085287969579e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 49/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=49, sub_opt_gap=2.4284e-05, total_loss=2.493e-05, loss_U=1.7537e-05, loss_P=739.28 \n",
      "INFO:root:Evaluating with parameters: w=100, rel_alpha=0.9, G_name=Barabasi-Albert, G=Graph with 200 nodes and 975 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=15306.4688, loss_U=3728.0073, loss_P=115.7846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=14365.4314, loss_U=1076.5651, loss_P=132.8887\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=16080.2587, loss_U=581.0774, loss_P=154.9918\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=2,  total_loss=14365.4314, loss_U=1076.5651, loss_P=132.8887\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=2, sub_opt_gap=2787, total_loss=14365, loss_U=1076.6, loss_P=132.89 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=2, sub_opt_gap=2444.6, total_loss=14023, loss_U=1487.7, loss_P=125.35 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=2, sub_opt_gap=2444.6, total_loss=14023, loss_U=1487.7, loss_P=125.35 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 16274.8643, Loss U: 3714.4707, Loss P: 125.6039, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 13851.5557, Loss U: 1579.2764, Loss P: 122.7228, Entropy: 383.4413, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 13850.4141, Loss U: 1574.1846, Loss P: 122.7623, Entropy: 352.1760, Entropy Lam: 3.452271214393103e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 13850.2393, Loss U: 1571.0127, Loss P: 122.7923, Entropy: 343.8714, Entropy Lam: 2.8102436848064324e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 13850.1084, Loss U: 1571.2031, Loss P: 122.7889, Entropy: 340.3875, Entropy Lam: 3.0448163954141954e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 13850.1318, Loss U: 1571.8145, Loss P: 122.7826, Entropy: 341.2233, Entropy Lam: 0.0001692892739326842\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 13850.7568, Loss U: 1571.2100, Loss P: 122.7878, Entropy: 344.3579, Entropy Lam: 0.002219381397940735\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 7000, Total Loss: 13854.9375, Loss U: 1571.9590, Loss P: 122.7818, Entropy: 292.2665, Entropy Lam: 0.016423977066398542\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 8000, Total Loss: 13874.1553, Loss U: 1559.2422, Loss P: 122.9253, Entropy: 245.1061, Entropy Lam: 0.09131595444786875\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 9000, Total Loss: 13897.5986, Loss U: 1556.5166, Loss P: 122.9929, Entropy: 194.1076, Entropy Lam: 0.23685006833599484\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 10000, Total Loss: 13935.2100, Loss U: 1509.3525, Loss P: 123.7181, Entropy: 117.0948, Entropy Lam: 0.461553778302522\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 11000, Total Loss: 14002.5078, Loss U: 1499.0488, Loss P: 123.9304, Entropy: 101.4609, Entropy Lam: 1.0883196658207586\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 12000, Total Loss: 14105.8916, Loss U: 1495.9580, Loss P: 124.3782, Entropy: 73.7751, Entropy Lam: 2.332909857356319\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 13000, Total Loss: 14310.5967, Loss U: 1463.3213, Loss P: 125.6280, Entropy: 47.0134, Entropy Lam: 6.050967352610925\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 14000, Total Loss: 14548.1504, Loss U: 1382.9619, Loss P: 128.5864, Entropy: 21.4855, Entropy Lam: 14.267864497405327\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 15000, Total Loss: 14750.7861, Loss U: 1331.5254, Loss P: 132.1949, Entropy: 5.3981, Entropy Lam: 37.00716596187357\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 16000, Total Loss: 14878.8320, Loss U: 1367.1777, Loss P: 134.5237, Entropy: 0.4640, Entropy Lam: 127.75877377644444\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 16537, Total Loss: 14908.8506, Loss U: 1354.3750, Loss P: 135.2871, Entropy: 0.0941, Entropy Lam: 273.8622778465079\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 5/50\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=5, sub_opt_gap=3328.7, total_loss=14907, loss_U=1350.1, loss_P=135.57 \n",
      "INFO:root:Evaluating with parameters: w=10, rel_alpha=0.9, G_name=Barabasi-Albert, G=Graph with 200 nodes and 975 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=4885.8535, loss_U=3728.0073, loss_P=115.7846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=2405.4517, loss_U=1076.5651, loss_P=132.8887\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=2130.9956, loss_U=581.0774, loss_P=154.9918\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=2019.0355, loss_U=356.4933, loss_P=166.2542\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=2235.0018, loss_U=233.6359, loss_P=200.1366\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=2262.8527, loss_U=133.8183, loss_P=212.9034\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=2342.2523, loss_U=90.7819, loss_P=225.1470\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=4,  total_loss=2019.0355, loss_U=356.4933, loss_P=166.2542\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=4, sub_opt_gap=861.19, total_loss=2019, loss_U=356.49, loss_P=166.25 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=4, sub_opt_gap=833.85, total_loss=1991.7, loss_U=379.01, loss_P=161.27 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=4, sub_opt_gap=833.63, total_loss=1991.5, loss_U=375.02, loss_P=161.65 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 4970.5098, Loss U: 3714.4707, Loss P: 125.6039, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 1894.8466, Loss U: 321.7656, Loss P: 157.3081, Entropy: 255.2416, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 1894.5487, Loss U: 321.2314, Loss P: 157.3317, Entropy: 196.2808, Entropy Lam: 2.143588810000001e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 1894.5090, Loss U: 321.2129, Loss P: 157.3296, Entropy: 180.7958, Entropy Lam: 2.8102436848064324e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 1894.4940, Loss U: 321.1387, Loss P: 157.3349, Entropy: 174.6568, Entropy Lam: 3.684227838451177e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 1894.5415, Loss U: 321.1660, Loss P: 157.3292, Entropy: 157.1751, Entropy Lam: 0.0005313022611848312\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 1895.2174, Loss U: 321.0020, Loss P: 157.3462, Entropy: 130.8700, Entropy Lam: 0.005756503766994982\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 7000, Total Loss: 1898.2500, Loss U: 320.8750, Loss P: 157.4365, Entropy: 85.5019, Entropy Lam: 0.03520625345522857\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 8000, Total Loss: 1903.7223, Loss U: 318.9336, Loss P: 157.7691, Entropy: 58.3959, Entropy Lam: 0.12154153537011333\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 9000, Total Loss: 1914.8644, Loss U: 315.3164, Loss P: 158.6747, Entropy: 33.5596, Entropy Lam: 0.3814494035558032\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 10000, Total Loss: 1935.9130, Loss U: 311.9922, Loss P: 160.2751, Entropy: 13.2859, Entropy Lam: 1.593408822728173\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 11000, Total Loss: 1971.6179, Loss U: 300.8447, Loss P: 164.1258, Entropy: 2.0687, Entropy Lam: 14.267864497405327\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 11369, Total Loss: 1993.7861, Loss U: 308.0840, Loss P: 168.3991, Entropy: 0.0462, Entropy Lam: 37.00716596187357\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 6/50\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=6, sub_opt_gap=834.79, total_loss=1992.6, loss_U=308.39, loss_P=168.43 \n",
      "INFO:root:Evaluating with parameters: w=1, rel_alpha=0.9, G_name=Barabasi-Albert, G=Graph with 200 nodes and 975 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=3843.7919, loss_U=3728.0073, loss_P=115.7846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=1209.4537, loss_U=1076.5651, loss_P=132.8887\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=736.0693, loss_U=581.0774, loss_P=154.9918\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=522.7475, loss_U=356.4933, loss_P=166.2542\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=433.7725, loss_U=233.6359, loss_P=200.1366\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=346.7218, loss_U=133.8183, loss_P=212.9034\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=315.9289, loss_U=90.7819, loss_P=225.1470\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=301.2574, loss_U=66.4536, loss_P=234.8038\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=311.2459, loss_U=45.7775, loss_P=265.4684\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=313.1233, loss_U=35.8628, loss_P=277.2605\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=319.6570, loss_U=28.5444, loss_P=291.1126\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=339.4572, loss_U=23.1193, loss_P=316.3379\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=344.1034, loss_U=18.2602, loss_P=325.8432\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=350.5261, loss_U=15.7299, loss_P=334.7962\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=8,  total_loss=301.2574, loss_U=66.4536, loss_P=234.8038\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=8, sub_opt_gap=185.47, total_loss=301.26, loss_U=66.454, loss_P=234.8 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=8, sub_opt_gap=184.29, total_loss=300.08, loss_U=67.551, loss_P=232.53 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=8, sub_opt_gap=184.18, total_loss=299.96, loss_U=68.506, loss_P=231.46 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 3840.0747, Loss U: 3714.4707, Loss P: 125.6039, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 292.8676, Loss U: 61.7549, Loss P: 231.1127, Entropy: 218.0693, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 292.6281, Loss U: 61.4248, Loss P: 231.2033, Entropy: 184.8279, Entropy Lam: 1.3310000000000002e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 292.5684, Loss U: 61.3740, Loss P: 231.1942, Entropy: 166.3983, Entropy Lam: 1.3109994191499957e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 292.5467, Loss U: 61.4102, Loss P: 231.1322, Entropy: 154.1264, Entropy Lam: 2.7680149049219957e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 292.6157, Loss U: 61.3848, Loss P: 231.1505, Entropy: 125.0833, Entropy Lam: 0.0006428757360336459\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 293.3866, Loss U: 61.4355, Loss P: 231.2718, Entropy: 66.6057, Entropy Lam: 0.010197997569961401\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 7000, Total Loss: 294.6920, Loss U: 60.4805, Loss P: 232.9173, Entropy: 25.1094, Entropy Lam: 0.05154547568380016\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 8000, Total Loss: 299.0066, Loss U: 57.9746, Loss P: 238.3138, Entropy: 7.8385, Entropy Lam: 0.34677218505073015\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 8920, Total Loss: 302.7971, Loss U: 58.1807, Loss P: 244.4774, Entropy: 0.0793, Entropy Lam: 1.7527497050009906\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 10/50\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=10, sub_opt_gap=186.96, total_loss=302.75, loss_U=58.182, loss_P=244.56 \n",
      "INFO:root:Evaluating with parameters: w=0.1, rel_alpha=0.9, G_name=Barabasi-Albert, G=Graph with 200 nodes and 975 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=3739.5858, loss_U=3728.0073, loss_P=115.7846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=1089.8539, loss_U=1076.5651, loss_P=132.8887\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=596.5766, loss_U=581.0774, loss_P=154.9918\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=373.1187, loss_U=356.4933, loss_P=166.2542\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=253.6495, loss_U=233.6359, loss_P=200.1366\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=155.1087, loss_U=133.8183, loss_P=212.9034\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=113.2966, loss_U=90.7819, loss_P=225.1470\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=89.9340, loss_U=66.4536, loss_P=234.8038\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=72.3243, loss_U=45.7775, loss_P=265.4684\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=63.5888, loss_U=35.8628, loss_P=277.2605\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=57.6556, loss_U=28.5444, loss_P=291.1126\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=54.7531, loss_U=23.1193, loss_P=316.3379\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=50.8445, loss_U=18.2602, loss_P=325.8432\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=49.2095, loss_U=15.7299, loss_P=334.7962\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=48.6198, loss_U=13.5142, loss_P=351.0559\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=48.7171, loss_U=11.5080, loss_P=372.0911\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=49.1177, loss_U=9.7053, loss_P=394.1238\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=48.9669, loss_U=8.3275, loss_P=406.3946\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=49.9303, loss_U=7.1109, loss_P=428.1941\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=20,  total_loss=50.3405, loss_U=6.2130, loss_P=441.2745\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=21,  total_loss=50.8000, loss_U=5.5051, loss_P=452.9491\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=22,  total_loss=51.5750, loss_U=4.8200, loss_P=467.5508\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=23,  total_loss=53.0572, loss_U=4.2416, loss_P=488.1563\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=24,  total_loss=54.5114, loss_U=3.7139, loss_P=507.9744\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=25,  total_loss=55.1623, loss_U=3.4074, loss_P=517.5494\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=26,  total_loss=56.8010, loss_U=3.1314, loss_P=536.6966\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=15,  total_loss=48.6198, loss_U=13.5142, loss_P=351.0559\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=15, sub_opt_gap=37.041, total_loss=48.62, loss_U=13.514, loss_P=351.06 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=16, sub_opt_gap=36.823, total_loss=48.401, loss_U=13.343, loss_P=350.59 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=17, sub_opt_gap=36.804, total_loss=48.383, loss_U=11.336, loss_P=370.47 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 3727.0310, Loss U: 3714.4707, Loss P: 125.6039, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 47.3892, Loss U: 10.5430, Loss P: 368.4618, Entropy: 104.3998, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 47.0681, Loss U: 10.3027, Loss P: 367.6532, Entropy: 81.5913, Entropy Lam: 1.9487171000000006e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 47.0288, Loss U: 10.2754, Loss P: 367.5319, Entropy: 75.7710, Entropy Lam: 3.400394858615784e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 47.0109, Loss U: 10.2861, Loss P: 367.1989, Entropy: 67.5704, Entropy Lam: 7.897469567994435e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 47.1250, Loss U: 10.2207, Loss P: 367.7765, Entropy: 57.0557, Entropy Lam: 0.002219381397940735\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 47.8396, Loss U: 9.9521, Loss P: 371.1310, Entropy: 35.4217, Entropy Lam: 0.021860313475376463\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 7000, Total Loss: 49.3192, Loss U: 9.5781, Loss P: 389.9713, Entropy: 5.5645, Entropy Lam: 0.1336956889071247\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 7405, Total Loss: 49.5340, Loss U: 9.2871, Loss P: 402.2571, Entropy: 0.0671, Entropy Lam: 0.3152474409552092\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 20/50\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=20, sub_opt_gap=37.941, total_loss=49.52, loss_U=9.2857, loss_P=402.34 \n",
      "INFO:root:Evaluating with parameters: w=0.01, rel_alpha=0.9, G_name=Barabasi-Albert, G=Graph with 200 nodes and 975 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=3729.1651, loss_U=3728.0073, loss_P=115.7846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=1077.8940, loss_U=1076.5651, loss_P=132.8887\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=582.6274, loss_U=581.0774, loss_P=154.9918\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=358.1559, loss_U=356.4933, loss_P=166.2542\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=235.6373, loss_U=233.6359, loss_P=200.1366\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=135.9474, loss_U=133.8183, loss_P=212.9034\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=93.0333, loss_U=90.7819, loss_P=225.1470\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=68.8016, loss_U=66.4536, loss_P=234.8038\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=48.4322, loss_U=45.7775, loss_P=265.4684\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=38.6354, loss_U=35.8628, loss_P=277.2605\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=31.4555, loss_U=28.5444, loss_P=291.1126\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=26.2827, loss_U=23.1193, loss_P=316.3379\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=21.5186, loss_U=18.2602, loss_P=325.8432\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=19.0778, loss_U=15.7299, loss_P=334.7962\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=17.0247, loss_U=13.5142, loss_P=351.0559\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=15.2289, loss_U=11.5080, loss_P=372.0911\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=13.6465, loss_U=9.7053, loss_P=394.1238\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=12.3914, loss_U=8.3275, loss_P=406.3946\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=11.3928, loss_U=7.1109, loss_P=428.1941\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=20,  total_loss=10.6258, loss_U=6.2130, loss_P=441.2745\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=21,  total_loss=10.0346, loss_U=5.5051, loss_P=452.9491\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=22,  total_loss=9.4955, loss_U=4.8200, loss_P=467.5508\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=23,  total_loss=9.1231, loss_U=4.2416, loss_P=488.1563\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=24,  total_loss=8.7937, loss_U=3.7139, loss_P=507.9744\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=25,  total_loss=8.5829, loss_U=3.4074, loss_P=517.5494\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=26,  total_loss=8.4983, loss_U=3.1314, loss_P=536.6966\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=27,  total_loss=8.3916, loss_U=2.8767, loss_P=551.4938\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=28,  total_loss=8.3133, loss_U=2.6279, loss_P=568.5453\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=29,  total_loss=8.2981, loss_U=2.4043, loss_P=589.3834\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=30,  total_loss=8.2611, loss_U=2.2146, loss_P=604.6453\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=31,  total_loss=8.2407, loss_U=2.0287, loss_P=621.1953\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=32,  total_loss=8.2285, loss_U=1.8669, loss_P=636.1647\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=33,  total_loss=8.3153, loss_U=1.7248, loss_P=659.0452\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=34,  total_loss=8.3097, loss_U=1.5879, loss_P=672.1802\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=35,  total_loss=8.2716, loss_U=1.4578, loss_P=681.3802\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=36,  total_loss=8.2445, loss_U=1.3344, loss_P=691.0135\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=37,  total_loss=8.2193, loss_U=1.2281, loss_P=699.1159\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=38,  total_loss=8.2330, loss_U=1.1452, loss_P=708.7825\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=39,  total_loss=8.2596, loss_U=1.0631, loss_P=719.6492\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=40,  total_loss=8.3041, loss_U=0.9834, loss_P=732.0659\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=41,  total_loss=8.3155, loss_U=0.9042, loss_P=741.1325\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=42,  total_loss=8.2918, loss_U=0.8338, loss_P=745.7992\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=43,  total_loss=8.4676, loss_U=0.7703, loss_P=769.7298\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=44,  total_loss=8.4652, loss_U=0.7112, loss_P=775.3964\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=45,  total_loss=8.5279, loss_U=0.6557, loss_P=787.2131\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=46,  total_loss=8.5557, loss_U=0.6102, loss_P=794.5464\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=47,  total_loss=8.6525, loss_U=0.5667, loss_P=808.5798\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=48,  total_loss=8.6754, loss_U=0.5246, loss_P=815.0798\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=49,  total_loss=8.7095, loss_U=0.4904, loss_P=821.9131\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=50,  total_loss=8.7203, loss_U=0.4579, loss_P=826.2464\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=51,  total_loss=8.7854, loss_U=0.4296, loss_P=835.5798\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=52,  total_loss=8.7746, loss_U=0.4022, loss_P=837.2464\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=53,  total_loss=8.7752, loss_U=0.3760, loss_P=839.9131\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=54,  total_loss=8.8070, loss_U=0.3503, loss_P=845.6750\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=55,  total_loss=8.8615, loss_U=0.3285, loss_P=853.3000\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=56,  total_loss=8.8535, loss_U=0.3072, loss_P=854.6333\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=57,  total_loss=8.8698, loss_U=0.2868, loss_P=858.3000\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=58,  total_loss=8.8695, loss_U=0.2665, loss_P=860.3000\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=59,  total_loss=8.8622, loss_U=0.2492, loss_P=861.3000\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=60,  total_loss=8.9038, loss_U=0.2341, loss_P=866.9667\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=61,  total_loss=8.9450, loss_U=0.2195, loss_P=872.5500\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=62,  total_loss=8.9995, loss_U=0.2056, loss_P=879.3833\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=63,  total_loss=9.0204, loss_U=0.1924, loss_P=882.8000\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=37,  total_loss=8.2193, loss_U=1.2281, loss_P=699.1159\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=37, sub_opt_gap=7.0614, total_loss=8.2193, loss_U=1.2281, loss_P=699.12 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=37, sub_opt_gap=6.8527, total_loss=8.0105, loss_U=1.5639, loss_P=644.66 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=37, sub_opt_gap=6.8527, total_loss=8.0105, loss_U=1.5639, loss_P=644.66 \n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 3715.7268, Loss U: 3714.4707, Loss P: 125.6039, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 10.2396, Loss U: 4.7920, Loss P: 544.7637, Entropy: 28.7180, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 9.5911, Loss U: 4.1270, Loss P: 546.4183, Entropy: 25.8608, Entropy Lam: 1.6105100000000003e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 9.5486, Loss U: 4.0859, Loss P: 546.2581, Entropy: 25.8878, Entropy Lam: 3.400394858615784e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 9.4469, Loss U: 3.9346, Loss P: 551.0977, Entropy: 25.3311, Entropy Lam: 5.39407797827637e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 9.3773, Loss U: 3.8369, Loss P: 551.5453, Entropy: 24.0666, Entropy Lam: 0.0010353578016395475\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 9.5072, Loss U: 3.6729, Loss P: 578.6586, Entropy: 3.1986, Entropy Lam: 0.01493088824218049\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6372, Total Loss: 9.5108, Loss U: 3.6543, Loss P: 585.2628, Entropy: 0.0997, Entropy Lam: 0.03872687880075143\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 30/50\n",
      "INFO:anonymigraph.utils:Katz converged after 286 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=30, sub_opt_gap=8.3464, total_loss=9.5042, loss_U=3.651, loss_P=585.32 \n",
      "INFO:root:Evaluating with parameters: w=0.0001, rel_alpha=0.01, G_name=Barabasi-Albert, G=Graph with 200 nodes and 975 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0165, loss_U=0.0050, loss_P=115.7846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0148, loss_U=0.0014, loss_P=134.0248\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0160, loss_U=0.0006, loss_P=154.0087\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=2,  total_loss=0.0148, loss_U=0.0014, loss_P=134.0248\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=2, sub_opt_gap=0.0031935, total_loss=0.014772, loss_U=0.0013694, loss_P=134.02 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=2, sub_opt_gap=0.0028038, total_loss=0.014382, loss_U=0.0015621, loss_P=128.2 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=2, sub_opt_gap=0.0028038, total_loss=0.014382, loss_U=0.0015621, loss_P=128.2 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0176, Loss U: 0.0050, Loss P: 125.6039, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0144, Loss U: 0.0016, Loss P: 127.2277, Entropy: 13.1115, Entropy Lam: 6.024006916124241e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1636, Total Loss: 0.0145, Loss U: 0.0013, Loss P: 131.6079, Entropy: 0.0821, Entropy Lam: 9.555938177273267e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 3/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=3, sub_opt_gap=0.0028596, total_loss=0.014438, loss_U=0.0012738, loss_P=131.64 \n",
      "INFO:root:Evaluating with parameters: w=1e-05, rel_alpha=0.01, G_name=Barabasi-Albert, G=Graph with 200 nodes and 975 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0061, loss_U=0.0050, loss_P=115.7846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0027, loss_U=0.0014, loss_P=134.0248\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0022, loss_U=0.0006, loss_P=154.0087\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=0.0020, loss_U=0.0004, loss_P=166.4319\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=0.0021, loss_U=0.0002, loss_P=189.3382\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=0.0023, loss_U=0.0002, loss_P=214.5546\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=0.0024, loss_U=0.0001, loss_P=232.1311\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=4,  total_loss=0.0020, loss_U=0.0004, loss_P=166.4319\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=4, sub_opt_gap=0.0008899, total_loss=0.0020477, loss_U=0.00038342, loss_P=166.43 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=4, sub_opt_gap=0.00087157, total_loss=0.0020294, loss_U=0.00040974, loss_P=161.97 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=4, sub_opt_gap=0.00087157, total_loss=0.0020294, loss_U=0.00040974, loss_P=161.97 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0063, Loss U: 0.0050, Loss P: 125.6039, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0022, Loss U: 0.0004, Loss P: 172.6476, Entropy: 5.1850, Entropy Lam: 6.626407607736665e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1411, Total Loss: 0.0022, Loss U: 0.0004, Loss P: 178.9431, Entropy: 0.0959, Entropy Lam: 3.684227838451177e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 7/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=7, sub_opt_gap=0.00097843, total_loss=0.0021363, loss_U=0.00034586, loss_P=179.04 \n",
      "INFO:root:Evaluating with parameters: w=1e-06, rel_alpha=0.01, G_name=Barabasi-Albert, G=Graph with 200 nodes and 975 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0051, loss_U=0.0050, loss_P=115.7846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0015, loss_U=0.0014, loss_P=134.0248\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0008, loss_U=0.0006, loss_P=154.0087\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=0.0005, loss_U=0.0004, loss_P=166.4319\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=0.0004, loss_U=0.0002, loss_P=189.3382\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=0.0004, loss_U=0.0002, loss_P=214.5546\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=0.0003, loss_U=0.0001, loss_P=232.1311\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=0.0003, loss_U=0.0001, loss_P=254.3540\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=0.0003, loss_U=0.0000, loss_P=268.6153\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=0.0003, loss_U=0.0000, loss_P=280.5343\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=0.0003, loss_U=0.0000, loss_P=294.5591\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=0.0003, loss_U=0.0000, loss_P=305.4559\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=0.0003, loss_U=0.0000, loss_P=327.4991\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=0.0004, loss_U=0.0000, loss_P=346.0730\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=9,  total_loss=0.0003, loss_U=0.0000, loss_P=268.6153\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=9, sub_opt_gap=0.00019681, total_loss=0.0003126, loss_U=4.3982e-05, loss_P=268.62 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=10, sub_opt_gap=0.0001933, total_loss=0.00030909, loss_U=4.0083e-05, loss_P=269 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=10, sub_opt_gap=0.0001933, total_loss=0.00030909, loss_U=4.0083e-05, loss_P=269 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0052, Loss U: 0.0050, Loss P: 125.6039, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0005, Loss U: 0.0002, Loss P: 294.6945, Entropy: 0.2566, Entropy Lam: 6.024006916124241e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1231, Total Loss: 0.0005, Loss U: 0.0002, Loss P: 294.7117, Entropy: 0.0999, Entropy Lam: 1.7187194770116268e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 14/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=14, sub_opt_gap=0.00022009, total_loss=0.00033587, loss_U=4.1146e-05, loss_P=294.73 \n",
      "INFO:root:Evaluating with parameters: w=1e-07, rel_alpha=0.01, G_name=Barabasi-Albert, G=Graph with 200 nodes and 975 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0050, loss_U=0.0050, loss_P=115.7846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0014, loss_U=0.0014, loss_P=134.0248\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0007, loss_U=0.0006, loss_P=154.0087\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=0.0004, loss_U=0.0004, loss_P=166.4319\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=0.0003, loss_U=0.0002, loss_P=189.3382\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=0.0002, loss_U=0.0002, loss_P=214.5546\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=0.0001, loss_U=0.0001, loss_P=232.1311\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=0.0001, loss_U=0.0001, loss_P=254.3540\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=0.0001, loss_U=0.0000, loss_P=268.6153\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=0.0001, loss_U=0.0000, loss_P=280.5343\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=0.0001, loss_U=0.0000, loss_P=294.5591\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=0.0001, loss_U=0.0000, loss_P=305.4559\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=0.0000, loss_U=0.0000, loss_P=327.4991\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=0.0000, loss_U=0.0000, loss_P=346.0730\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=0.0000, loss_U=0.0000, loss_P=360.5134\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=0.0000, loss_U=0.0000, loss_P=390.6907\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=0.0000, loss_U=0.0000, loss_P=408.7485\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=0.0000, loss_U=0.0000, loss_P=424.0012\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=0.0000, loss_U=0.0000, loss_P=444.1693\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=20,  total_loss=0.0000, loss_U=0.0000, loss_P=460.1765\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=21,  total_loss=0.0000, loss_U=0.0000, loss_P=471.0479\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=22,  total_loss=0.0001, loss_U=0.0000, loss_P=484.4625\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=23,  total_loss=0.0001, loss_U=0.0000, loss_P=506.5670\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=24,  total_loss=0.0001, loss_U=0.0000, loss_P=522.5726\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=25,  total_loss=0.0001, loss_U=0.0000, loss_P=534.7107\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=26,  total_loss=0.0001, loss_U=0.0000, loss_P=555.9985\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=15,  total_loss=0.0000, loss_U=0.0000, loss_P=360.5134\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=15, sub_opt_gap=3.3855e-05, total_loss=4.5433e-05, loss_U=9.3818e-06, loss_P=360.51 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=15, sub_opt_gap=3.3855e-05, total_loss=4.5433e-05, loss_U=9.3818e-06, loss_P=360.51 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=15, sub_opt_gap=3.3855e-05, total_loss=4.5433e-05, loss_U=9.3818e-06, loss_P=360.51 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0051, Loss U: 0.0050, Loss P: 125.6039, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0002, Loss U: 0.0002, Loss P: 608.1546, Entropy: 0.3045, Entropy Lam: 6.024006916124241e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1263, Total Loss: 0.0002, Loss U: 0.0002, Loss P: 608.2837, Entropy: 0.0997, Entropy Lam: 1.8905914247127896e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 40/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=40, sub_opt_gap=5.9481e-05, total_loss=7.106e-05, loss_U=1.0226e-05, loss_P=608.34 \n",
      "INFO:root:Evaluating with parameters: w=1e-08, rel_alpha=0.01, G_name=Barabasi-Albert, G=Graph with 200 nodes and 975 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0050, loss_U=0.0050, loss_P=115.7846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0014, loss_U=0.0014, loss_P=134.0248\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0006, loss_U=0.0006, loss_P=154.0087\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=0.0004, loss_U=0.0004, loss_P=166.4319\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=0.0003, loss_U=0.0002, loss_P=189.3382\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=0.0002, loss_U=0.0002, loss_P=214.5546\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=0.0001, loss_U=0.0001, loss_P=232.1311\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=0.0001, loss_U=0.0001, loss_P=254.3540\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=0.0000, loss_U=0.0000, loss_P=268.6153\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=0.0000, loss_U=0.0000, loss_P=280.5343\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=0.0000, loss_U=0.0000, loss_P=294.5591\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=0.0000, loss_U=0.0000, loss_P=305.4559\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=0.0000, loss_U=0.0000, loss_P=327.4991\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=0.0000, loss_U=0.0000, loss_P=346.0730\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=0.0000, loss_U=0.0000, loss_P=360.5134\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=0.0000, loss_U=0.0000, loss_P=390.6907\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=0.0000, loss_U=0.0000, loss_P=408.7485\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=0.0000, loss_U=0.0000, loss_P=424.0012\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=0.0000, loss_U=0.0000, loss_P=444.1693\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=20,  total_loss=0.0000, loss_U=0.0000, loss_P=460.1765\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=21,  total_loss=0.0000, loss_U=0.0000, loss_P=471.0479\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=22,  total_loss=0.0000, loss_U=0.0000, loss_P=484.4625\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=23,  total_loss=0.0000, loss_U=0.0000, loss_P=506.5670\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=24,  total_loss=0.0000, loss_U=0.0000, loss_P=522.5726\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=25,  total_loss=0.0000, loss_U=0.0000, loss_P=534.7107\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=26,  total_loss=0.0000, loss_U=0.0000, loss_P=555.9985\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=27,  total_loss=0.0000, loss_U=0.0000, loss_P=564.7985\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=28,  total_loss=0.0000, loss_U=0.0000, loss_P=576.2271\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=29,  total_loss=0.0000, loss_U=0.0000, loss_P=592.6747\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=30,  total_loss=0.0000, loss_U=0.0000, loss_P=604.4444\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=31,  total_loss=0.0000, loss_U=0.0000, loss_P=616.0862\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=32,  total_loss=0.0000, loss_U=0.0000, loss_P=630.5299\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=33,  total_loss=0.0000, loss_U=0.0000, loss_P=646.2908\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=34,  total_loss=0.0000, loss_U=0.0000, loss_P=651.7210\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=35,  total_loss=0.0000, loss_U=0.0000, loss_P=670.4614\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=36,  total_loss=0.0000, loss_U=0.0000, loss_P=676.1281\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=37,  total_loss=0.0000, loss_U=0.0000, loss_P=686.1092\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=38,  total_loss=0.0000, loss_U=0.0000, loss_P=697.1441\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=39,  total_loss=0.0000, loss_U=0.0000, loss_P=705.0727\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=40,  total_loss=0.0000, loss_U=0.0000, loss_P=712.1054\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=41,  total_loss=0.0000, loss_U=0.0000, loss_P=724.8840\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=42,  total_loss=0.0000, loss_U=0.0000, loss_P=738.7476\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=43,  total_loss=0.0000, loss_U=0.0000, loss_P=745.4143\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=44,  total_loss=0.0000, loss_U=0.0000, loss_P=755.2532\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=45,  total_loss=0.0000, loss_U=0.0000, loss_P=766.4421\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=46,  total_loss=0.0000, loss_U=0.0000, loss_P=771.3587\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=47,  total_loss=0.0000, loss_U=0.0000, loss_P=777.8476\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=29,  total_loss=0.0000, loss_U=0.0000, loss_P=592.6747\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=29, sub_opt_gap=4.7995e-06, total_loss=5.9574e-06, loss_U=3.0626e-08, loss_P=592.67 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=29, sub_opt_gap=4.7995e-06, total_loss=5.9574e-06, loss_U=3.0626e-08, loss_P=592.67 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=29, sub_opt_gap=4.7995e-06, total_loss=5.9574e-06, loss_U=3.0626e-08, loss_P=592.67 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0050, Loss U: 0.0050, Loss P: 125.6039, Entropy: 721.5601, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0002, Loss U: 0.0002, Loss P: 744.8124, Entropy: 0.3310, Entropy Lam: 6.024006916124241e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1296, Total Loss: 0.0002, Loss U: 0.0002, Loss P: 745.0153, Entropy: 0.0999, Entropy Lam: 2.2876156239024755e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 47/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=47, sub_opt_gap=1.6787e-05, total_loss=1.7945e-05, loss_U=1.0494e-05, loss_P=745.09 \n",
      "INFO:root:Evaluating with parameters: w=10, rel_alpha=0.9, G_name=LFR, G=Graph with 200 nodes and 634 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=3691.5884, loss_U=3192.6531, loss_P=49.8935\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=2600.8408, loss_U=1644.6238, loss_P=95.6217\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=1632.1581, loss_U=409.3804, loss_P=122.2778\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=1543.2116, loss_U=202.0156, loss_P=134.1196\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=1677.1745, loss_U=136.5425, loss_P=154.0632\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=1712.9513, loss_U=81.6681, loss_P=163.1283\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=1805.5606, loss_U=58.5658, loss_P=174.6995\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=4,  total_loss=1543.2116, loss_U=202.0156, loss_P=134.1196\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=4, sub_opt_gap=1044.3, total_loss=1543.2, loss_U=202.02, loss_P=134.12 \n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=4, sub_opt_gap=1005.9, total_loss=1504.8, loss_U=244.43, loss_P=126.04 \n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=4, sub_opt_gap=982.21, total_loss=1481.1, loss_U=270.35, loss_P=121.08 \n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 3758.3511, Loss U: 3170.5586, Loss P: 58.7792, Entropy: 720.9520, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 1437.2731, Loss U: 219.5947, Loss P: 121.7678, Entropy: 325.6120, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 1437.0317, Loss U: 218.9785, Loss P: 121.8053, Entropy: 237.5099, Entropy Lam: 1.4641000000000003e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 1436.9733, Loss U: 219.0225, Loss P: 121.7951, Entropy: 203.5035, Entropy Lam: 1.3109994191499957e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 1436.8864, Loss U: 219.0381, Loss P: 121.7847, Entropy: 197.4132, Entropy Lam: 8.017953205361366e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 1436.8560, Loss U: 219.0703, Loss P: 121.7766, Entropy: 183.0657, Entropy Lam: 0.00010511531995000595\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 1436.9976, Loss U: 219.1982, Loss P: 121.7619, Entropy: 158.0522, Entropy Lam: 0.0011388935818035023\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 7000, Total Loss: 1438.1505, Loss U: 219.0879, Loss P: 121.7772, Entropy: 126.5205, Entropy Lam: 0.010197997569961401\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 8000, Total Loss: 1440.0087, Loss U: 217.0498, Loss P: 122.0672, Entropy: 78.6018, Entropy Lam: 0.029096077235726082\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 9000, Total Loss: 1443.5314, Loss U: 215.8408, Loss P: 122.2809, Entropy: 48.6026, Entropy Lam: 0.10044754989265563\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 10000, Total Loss: 1458.2037, Loss U: 211.9248, Loss P: 123.0565, Entropy: 34.0453, Entropy Lam: 0.461553778302522\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 11000, Total Loss: 1489.5201, Loss U: 206.0698, Loss P: 125.4641, Entropy: 12.3489, Entropy Lam: 2.332909857356319\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 12000, Total Loss: 1515.1140, Loss U: 198.5513, Loss P: 129.8524, Entropy: 1.6827, Entropy Lam: 10.71965777415877\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 12765, Total Loss: 1538.8860, Loss U: 213.1221, Loss P: 132.2518, Entropy: 0.0725, Entropy Lam: 44.77867081386703\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 6/50\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=6, sub_opt_gap=1038.7, total_loss=1537.6, loss_U=213.6, loss_P=132.4 \n",
      "INFO:root:Evaluating with parameters: w=1, rel_alpha=0.9, G_name=LFR, G=Graph with 200 nodes and 634 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=3242.5466, loss_U=3192.6531, loss_P=49.8935\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=1740.2455, loss_U=1644.6238, loss_P=95.6217\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=531.6582, loss_U=409.3804, loss_P=122.2778\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=336.1352, loss_U=202.0156, loss_P=134.1196\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=290.6057, loss_U=136.5425, loss_P=154.0632\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=244.7964, loss_U=81.6681, loss_P=163.1283\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=233.2653, loss_U=58.5658, loss_P=174.6995\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=227.1341, loss_U=42.8260, loss_P=184.3081\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=221.2739, loss_U=29.7404, loss_P=191.5336\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=226.4844, loss_U=21.2873, loss_P=205.1971\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=233.5393, loss_U=16.7266, loss_P=216.8127\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=246.4877, loss_U=13.3031, loss_P=233.1846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=252.9099, loss_U=10.4174, loss_P=242.4924\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=259.5960, loss_U=8.4916, loss_P=251.1044\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=9,  total_loss=221.2739, loss_U=29.7404, loss_P=191.5336\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=9, sub_opt_gap=171.38, total_loss=221.27, loss_U=29.74, loss_P=191.53 \n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=9, sub_opt_gap=165.95, total_loss=215.84, loss_U=33.268, loss_P=182.57 \n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=9, sub_opt_gap=165.95, total_loss=215.84, loss_U=33.268, loss_P=182.57 \n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 3229.3379, Loss U: 3170.5586, Loss P: 58.7792, Entropy: 720.9520, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 208.6448, Loss U: 34.7002, Loss P: 173.9445, Entropy: 164.5673, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 208.3878, Loss U: 34.2070, Loss P: 174.1807, Entropy: 153.2092, Entropy Lam: 1.3310000000000002e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 208.3026, Loss U: 34.0996, Loss P: 174.2028, Entropy: 147.5229, Entropy Lam: 1.0834705943388394e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 208.1752, Loss U: 34.1641, Loss P: 174.0092, Entropy: 135.2688, Entropy Lam: 1.4204293198443194e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 208.2137, Loss U: 34.1289, Loss P: 174.0337, Entropy: 127.9500, Entropy Lam: 0.00039917525258063944\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 208.4810, Loss U: 33.8716, Loss P: 174.0345, Entropy: 99.8749, Entropy Lam: 0.005756503766994982\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 7000, Total Loss: 211.7265, Loss U: 34.2085, Loss P: 174.3617, Entropy: 67.3562, Entropy Lam: 0.046859523348909235\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 8000, Total Loss: 217.5638, Loss U: 33.6035, Loss P: 178.9725, Entropy: 25.4809, Entropy Lam: 0.19574385812892134\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 9000, Total Loss: 223.6747, Loss U: 33.0039, Loss P: 186.7080, Entropy: 5.3311, Entropy Lam: 0.743336975493995\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 9849, Total Loss: 228.2606, Loss U: 29.5713, Loss P: 198.2212, Entropy: 0.0774, Entropy Lam: 6.050967352610925\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 13/50\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=13, sub_opt_gap=178.19, total_loss=228.08, loss_U=29.451, loss_P=198.63 \n",
      "INFO:root:Evaluating with parameters: w=0.1, rel_alpha=0.9, G_name=LFR, G=Graph with 200 nodes and 634 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=3197.6424, loss_U=3192.6531, loss_P=49.8935\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=1654.1860, loss_U=1644.6238, loss_P=95.6217\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=421.6082, loss_U=409.3804, loss_P=122.2778\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=215.4275, loss_U=202.0156, loss_P=134.1196\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=151.9488, loss_U=136.5425, loss_P=154.0632\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=97.9809, loss_U=81.6681, loss_P=163.1283\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=76.0357, loss_U=58.5658, loss_P=174.6995\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=61.2568, loss_U=42.8260, loss_P=184.3081\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=48.8937, loss_U=29.7404, loss_P=191.5336\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=41.8070, loss_U=21.2873, loss_P=205.1971\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=38.4078, loss_U=16.7266, loss_P=216.8127\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=36.6215, loss_U=13.3031, loss_P=233.1846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=34.6667, loss_U=10.4174, loss_P=242.4924\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=33.6020, loss_U=8.4916, loss_P=251.1044\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=33.1720, loss_U=7.2222, loss_P=259.4980\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=33.3099, loss_U=6.1887, loss_P=271.2118\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=33.1173, loss_U=5.1857, loss_P=279.3161\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=33.4937, loss_U=4.5370, loss_P=289.5669\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=34.3206, loss_U=3.9608, loss_P=303.5982\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=20,  total_loss=34.5440, loss_U=3.4222, loss_P=311.2173\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=21,  total_loss=35.6741, loss_U=3.0358, loss_P=326.3830\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=22,  total_loss=36.6035, loss_U=2.6561, loss_P=339.4741\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=23,  total_loss=37.1369, loss_U=2.3503, loss_P=347.8658\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=24,  total_loss=38.2077, loss_U=2.1054, loss_P=361.0230\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=25,  total_loss=38.4337, loss_U=1.9314, loss_P=365.0230\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=26,  total_loss=38.9919, loss_U=1.7851, loss_P=372.0685\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=17,  total_loss=33.1173, loss_U=5.1857, loss_P=279.3161\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=17, sub_opt_gap=28.128, total_loss=33.117, loss_U=5.1857, loss_P=279.32 \n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=16, sub_opt_gap=27.529, total_loss=32.518, loss_U=6.5867, loss_P=259.32 \n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=16, sub_opt_gap=27.529, total_loss=32.518, loss_U=6.5867, loss_P=259.32 \n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 3176.4365, Loss U: 3170.5586, Loss P: 58.7792, Entropy: 720.9520, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 31.5587, Loss U: 6.1226, Loss P: 254.3610, Entropy: 78.4501, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 31.2536, Loss U: 6.1880, Loss P: 250.6556, Entropy: 83.2785, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 31.1183, Loss U: 6.1665, Loss P: 249.5179, Entropy: 84.0407, Entropy Lam: 5.054470284992945e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 31.0972, Loss U: 6.1646, Loss P: 249.3163, Entropy: 81.3034, Entropy Lam: 1.2912993816766538e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 31.1032, Loss U: 6.1562, Loss P: 249.1778, Entropy: 80.4475, Entropy Lam: 0.00036288659325512675\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 31.5630, Loss U: 6.0835, Loss P: 250.0969, Entropy: 55.7487, Entropy Lam: 0.008428097165257355\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 7000, Total Loss: 32.7569, Loss U: 5.6431, Loss P: 263.8662, Entropy: 12.8264, Entropy Lam: 0.05670002325218018\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 7800, Total Loss: 33.3521, Loss U: 5.3208, Loss P: 279.9463, Entropy: 0.0961, Entropy Lam: 0.3814494035558032\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 20/50\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=20, sub_opt_gap=28.35, total_loss=33.34, loss_U=5.3199, loss_P=280.2 \n",
      "INFO:root:Evaluating with parameters: w=0.01, rel_alpha=0.9, G_name=LFR, G=Graph with 200 nodes and 634 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=3193.1520, loss_U=3192.6531, loss_P=49.8935\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=1645.5801, loss_U=1644.6238, loss_P=95.6217\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=410.6032, loss_U=409.3804, loss_P=122.2778\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=203.3568, loss_U=202.0156, loss_P=134.1196\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=138.0831, loss_U=136.5425, loss_P=154.0632\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=83.2994, loss_U=81.6681, loss_P=163.1283\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=60.3128, loss_U=58.5658, loss_P=174.6995\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=44.6691, loss_U=42.8260, loss_P=184.3081\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=31.6557, loss_U=29.7404, loss_P=191.5336\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=23.3393, loss_U=21.2873, loss_P=205.1971\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=18.8947, loss_U=16.7266, loss_P=216.8127\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=15.6349, loss_U=13.3031, loss_P=233.1846\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=12.8424, loss_U=10.4174, loss_P=242.4924\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=11.0027, loss_U=8.4916, loss_P=251.1044\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=9.8172, loss_U=7.2222, loss_P=259.4980\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=8.9008, loss_U=6.1887, loss_P=271.2118\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=7.9789, loss_U=5.1857, loss_P=279.3161\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=7.4327, loss_U=4.5370, loss_P=289.5669\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=6.9968, loss_U=3.9608, loss_P=303.5982\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=20,  total_loss=6.5344, loss_U=3.4222, loss_P=311.2173\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=21,  total_loss=6.2996, loss_U=3.0358, loss_P=326.3830\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=22,  total_loss=6.0508, loss_U=2.6561, loss_P=339.4741\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=23,  total_loss=5.8290, loss_U=2.3503, loss_P=347.8658\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=24,  total_loss=5.7157, loss_U=2.1054, loss_P=361.0230\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=25,  total_loss=5.5816, loss_U=1.9314, loss_P=365.0230\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=26,  total_loss=5.5058, loss_U=1.7851, loss_P=372.0685\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=27,  total_loss=5.4878, loss_U=1.6404, loss_P=384.7346\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=28,  total_loss=5.4248, loss_U=1.4998, loss_P=392.5013\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=29,  total_loss=5.3455, loss_U=1.3787, loss_P=396.6791\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=30,  total_loss=5.4025, loss_U=1.2710, loss_P=413.1489\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=31,  total_loss=5.3709, loss_U=1.1759, loss_P=419.4939\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=32,  total_loss=5.3702, loss_U=1.0853, loss_P=428.4939\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=33,  total_loss=5.3198, loss_U=0.9955, loss_P=432.4273\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=34,  total_loss=5.2560, loss_U=0.9118, loss_P=434.4273\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=35,  total_loss=5.2872, loss_U=0.8289, loss_P=445.8285\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=36,  total_loss=5.2938, loss_U=0.7685, loss_P=452.5234\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=37,  total_loss=5.4074, loss_U=0.7170, loss_P=469.0405\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=38,  total_loss=5.4038, loss_U=0.6684, loss_P=473.5405\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=39,  total_loss=5.4114, loss_U=0.6227, loss_P=478.8738\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=40,  total_loss=5.4313, loss_U=0.5792, loss_P=485.2071\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=41,  total_loss=5.4191, loss_U=0.5429, loss_P=487.6238\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=42,  total_loss=5.4039, loss_U=0.5094, loss_P=489.4571\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=43,  total_loss=5.4676, loss_U=0.4772, loss_P=499.0381\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=44,  total_loss=5.4609, loss_U=0.4530, loss_P=500.7881\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=45,  total_loss=5.4612, loss_U=0.4299, loss_P=503.1214\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=46,  total_loss=5.4732, loss_U=0.4086, loss_P=506.4548\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=47,  total_loss=5.4558, loss_U=0.3879, loss_P=506.7881\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=34,  total_loss=5.2560, loss_U=0.9118, loss_P=434.4273\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=34, sub_opt_gap=4.7571, total_loss=5.256, loss_U=0.91175, loss_P=434.43 \n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=49, sub_opt_gap=4.4626, total_loss=4.9616, loss_U=0.69879, loss_P=426.28 \n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=50, sub_opt_gap=4.4618, total_loss=4.9607, loss_U=0.69796, loss_P=426.28 \n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 3171.1465, Loss U: 3170.5586, Loss P: 58.7792, Entropy: 720.9520, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 7.3419, Loss U: 3.9448, Loss P: 339.7078, Entropy: 12.7275, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 6.3685, Loss U: 2.9292, Loss P: 343.9336, Entropy: 12.0966, Entropy Lam: 1.1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 3000, Total Loss: 6.2781, Loss U: 2.7656, Loss P: 351.2474, Entropy: 12.0572, Entropy Lam: 6.727499949325611e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 4000, Total Loss: 6.2395, Loss U: 2.7188, Loss P: 352.0540, Entropy: 12.3650, Entropy Lam: 1.5624722518287514e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 5000, Total Loss: 6.2321, Loss U: 2.6938, Loss P: 353.0071, Entropy: 11.5709, Entropy Lam: 0.0007071633096370106\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6000, Total Loss: 6.2937, Loss U: 2.6230, Loss P: 366.0106, Entropy: 0.4389, Entropy Lam: 0.024046344822914113\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 6014, Total Loss: 6.2918, Loss U: 2.6230, Loss P: 366.6402, Entropy: 0.0974, Entropy Lam: 0.024046344822914113\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 28/50\n",
      "INFO:anonymigraph.utils:Katz converged after 283 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=28, sub_opt_gap=5.7846, total_loss=6.2836, loss_U=2.6168, loss_P=366.67 \n",
      "INFO:root:Evaluating with parameters: w=0.0001, rel_alpha=0.01, G_name=LFR, G=Graph with 200 nodes and 634 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0099, loss_U=0.0049, loss_P=49.8935\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0096, loss_U=0.0016, loss_P=80.4315\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0098, loss_U=0.0009, loss_P=89.4529\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=2,  total_loss=0.0096, loss_U=0.0016, loss_P=80.4315\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=2, sub_opt_gap=0.0046327, total_loss=0.009622, loss_U=0.0015789, loss_P=80.432 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=3, sub_opt_gap=0.0043831, total_loss=0.0093725, loss_U=0.0010463, loss_P=83.261 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=3, sub_opt_gap=0.0043868, total_loss=0.0093761, loss_U=0.0010505, loss_P=83.256 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0108, Loss U: 0.0049, Loss P: 58.7792, Entropy: 720.9520, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0097, Loss U: 0.0042, Loss P: 53.8436, Entropy: 20.4617, Entropy Lam: 7.289048368510332e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2000, Total Loss: 0.0102, Loss U: 0.0041, Loss P: 57.3429, Entropy: 0.6655, Entropy Lam: 0.0005313022611848312\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 2100, Total Loss: 0.0103, Loss U: 0.0039, Loss P: 63.7370, Entropy: 0.0677, Entropy Lam: 0.0008556676046607829\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 4/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=4, sub_opt_gap=0.0051953, total_loss=0.010185, loss_U=0.0037778, loss_P=64.069 \n",
      "INFO:root:Evaluating with parameters: w=5.62e-06, rel_alpha=0.01, G_name=LFR, G=Graph with 200 nodes and 634 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0052, loss_U=0.0049, loss_P=49.8935\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0020, loss_U=0.0016, loss_P=80.4315\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0014, loss_U=0.0009, loss_P=89.4529\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=0.0012, loss_U=0.0005, loss_P=127.6407\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=0.0010, loss_U=0.0002, loss_P=144.6035\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=0.0010, loss_U=0.0001, loss_P=152.5724\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=0.0010, loss_U=0.0001, loss_P=169.2925\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=0.0011, loss_U=0.0001, loss_P=178.7367\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=0.0011, loss_U=0.0000, loss_P=185.4059\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=0.0011, loss_U=0.0000, loss_P=193.2172\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=6,  total_loss=0.0010, loss_U=0.0001, loss_P=152.5724\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=6, sub_opt_gap=0.00070828, total_loss=0.00098886, loss_U=0.00013088, loss_P=152.57 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=7, sub_opt_gap=0.00069387, total_loss=0.00097444, loss_U=0.00010837, loss_P=154.01 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=7, sub_opt_gap=0.00069268, total_loss=0.00097325, loss_U=0.00010457, loss_P=154.48 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0053, Loss U: 0.0049, Loss P: 58.7792, Entropy: 720.9520, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0011, Loss U: 0.0002, Loss P: 160.7480, Entropy: 0.2345, Entropy Lam: 6.626407607736665e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1190, Total Loss: 0.0011, Loss U: 0.0002, Loss P: 160.7565, Entropy: 0.0995, Entropy Lam: 1.4204293198443194e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 9/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=9, sub_opt_gap=0.00073345, total_loss=0.001014, loss_U=0.00010991, loss_P=160.78 \n",
      "INFO:root:Evaluating with parameters: w=3.16e-07, rel_alpha=0.01, G_name=LFR, G=Graph with 200 nodes and 634 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0049, loss_U=0.0049, loss_P=49.8935\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0016, loss_U=0.0016, loss_P=80.4315\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0009, loss_U=0.0009, loss_P=89.4529\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=0.0005, loss_U=0.0005, loss_P=127.6407\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=0.0003, loss_U=0.0002, loss_P=144.6035\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=0.0002, loss_U=0.0001, loss_P=152.5724\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=0.0001, loss_U=0.0001, loss_P=169.2925\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=0.0001, loss_U=0.0001, loss_P=178.7367\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=0.0001, loss_U=0.0000, loss_P=185.4059\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=0.0001, loss_U=0.0000, loss_P=193.2172\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=0.0001, loss_U=0.0000, loss_P=209.6524\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=0.0001, loss_U=0.0000, loss_P=220.7232\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=0.0001, loss_U=0.0000, loss_P=236.2624\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=0.0001, loss_U=0.0000, loss_P=250.2495\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=0.0001, loss_U=0.0000, loss_P=252.3337\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=0.0001, loss_U=0.0000, loss_P=260.6995\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=0.0001, loss_U=0.0000, loss_P=268.8661\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=0.0001, loss_U=0.0000, loss_P=279.0661\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=0.0001, loss_U=0.0000, loss_P=290.1337\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=12,  total_loss=0.0001, loss_U=0.0000, loss_P=220.7232\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=12, sub_opt_gap=6.3104e-05, total_loss=7.8881e-05, loss_U=9.0827e-06, loss_P=220.72 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=13, sub_opt_gap=6.2954e-05, total_loss=7.8732e-05, loss_U=8.2743e-06, loss_P=222.81 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=13, sub_opt_gap=6.2954e-05, total_loss=7.8732e-05, loss_U=8.2743e-06, loss_P=222.81 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0049, Loss U: 0.0049, Loss P: 58.7792, Entropy: 720.9520, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0003, Loss U: 0.0002, Loss P: 305.7925, Entropy: 0.2067, Entropy Lam: 6.024006916124241e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1179, Total Loss: 0.0003, Loss U: 0.0002, Loss P: 305.8106, Entropy: 0.0998, Entropy Lam: 1.4204293198443194e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 24/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=24, sub_opt_gap=9.8655e-05, total_loss=0.00011443, loss_U=1.7718e-05, loss_P=305.84 \n",
      "INFO:root:Evaluating with parameters: w=1.78e-08, rel_alpha=0.01, G_name=LFR, G=Graph with 200 nodes and 634 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0049, loss_U=0.0049, loss_P=49.8935\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0016, loss_U=0.0016, loss_P=80.4315\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0009, loss_U=0.0009, loss_P=89.4529\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=0.0005, loss_U=0.0005, loss_P=127.6407\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=0.0002, loss_U=0.0002, loss_P=144.6035\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=0.0001, loss_U=0.0001, loss_P=152.5724\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=0.0001, loss_U=0.0001, loss_P=169.2925\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=0.0001, loss_U=0.0001, loss_P=178.7367\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=0.0000, loss_U=0.0000, loss_P=185.4059\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=0.0000, loss_U=0.0000, loss_P=193.2172\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=0.0000, loss_U=0.0000, loss_P=209.6524\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=0.0000, loss_U=0.0000, loss_P=220.7232\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=0.0000, loss_U=0.0000, loss_P=236.2624\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=0.0000, loss_U=0.0000, loss_P=250.2495\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=0.0000, loss_U=0.0000, loss_P=252.3337\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=0.0000, loss_U=0.0000, loss_P=260.6995\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=0.0000, loss_U=0.0000, loss_P=268.8661\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=0.0000, loss_U=0.0000, loss_P=279.0661\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=0.0000, loss_U=0.0000, loss_P=290.1337\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=20,  total_loss=0.0000, loss_U=0.0000, loss_P=301.7382\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=21,  total_loss=0.0000, loss_U=0.0000, loss_P=316.4953\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=22,  total_loss=0.0000, loss_U=0.0000, loss_P=330.8514\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=23,  total_loss=0.0000, loss_U=0.0000, loss_P=337.0070\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=24,  total_loss=0.0000, loss_U=0.0000, loss_P=351.4098\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=25,  total_loss=0.0000, loss_U=0.0000, loss_P=361.7666\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=26,  total_loss=0.0000, loss_U=0.0000, loss_P=371.3008\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=18,  total_loss=0.0000, loss_U=0.0000, loss_P=279.0661\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=18, sub_opt_gap=4.1859e-06, total_loss=5.0732e-06, loss_U=1.106e-07, loss_P=279.07 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=18, sub_opt_gap=4.1859e-06, total_loss=5.0732e-06, loss_U=1.106e-07, loss_P=279.07 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=18, sub_opt_gap=4.1859e-06, total_loss=5.0732e-06, loss_U=1.106e-07, loss_P=279.07 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0049, Loss U: 0.0049, Loss P: 58.7792, Entropy: 720.9520, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0002, Loss U: 0.0002, Loss P: 527.3619, Entropy: 0.2401, Entropy Lam: 6.024006916124241e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1205, Total Loss: 0.0002, Loss U: 0.0002, Loss P: 527.4299, Entropy: 0.0997, Entropy Lam: 1.5624722518287514e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 49/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=49, sub_opt_gap=2.1828e-05, total_loss=2.2716e-05, loss_U=1.3336e-05, loss_P=527.48 \n",
      "INFO:root:Evaluating with parameters: w=1e-09, rel_alpha=0.01, G_name=LFR, G=Graph with 200 nodes and 634 edges, seed=42\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=1,  total_loss=0.0049, loss_U=0.0049, loss_P=49.8935\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=2,  total_loss=0.0016, loss_U=0.0016, loss_P=80.4315\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=3,  total_loss=0.0009, loss_U=0.0009, loss_P=89.4529\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=4,  total_loss=0.0005, loss_U=0.0005, loss_P=127.6407\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=5,  total_loss=0.0002, loss_U=0.0002, loss_P=144.6035\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=6,  total_loss=0.0001, loss_U=0.0001, loss_P=152.5724\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=7,  total_loss=0.0001, loss_U=0.0001, loss_P=169.2925\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=8,  total_loss=0.0001, loss_U=0.0001, loss_P=178.7367\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=9,  total_loss=0.0000, loss_U=0.0000, loss_P=185.4059\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=10,  total_loss=0.0000, loss_U=0.0000, loss_P=193.2172\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=11,  total_loss=0.0000, loss_U=0.0000, loss_P=209.6524\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=12,  total_loss=0.0000, loss_U=0.0000, loss_P=220.7232\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=13,  total_loss=0.0000, loss_U=0.0000, loss_P=236.2624\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=14,  total_loss=0.0000, loss_U=0.0000, loss_P=250.2495\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=15,  total_loss=0.0000, loss_U=0.0000, loss_P=252.3337\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=16,  total_loss=0.0000, loss_U=0.0000, loss_P=260.6995\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=17,  total_loss=0.0000, loss_U=0.0000, loss_P=268.8661\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=18,  total_loss=0.0000, loss_U=0.0000, loss_P=279.0661\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=19,  total_loss=0.0000, loss_U=0.0000, loss_P=290.1337\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=20,  total_loss=0.0000, loss_U=0.0000, loss_P=301.7382\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=21,  total_loss=0.0000, loss_U=0.0000, loss_P=316.4953\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=22,  total_loss=0.0000, loss_U=0.0000, loss_P=330.8514\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=23,  total_loss=0.0000, loss_U=0.0000, loss_P=337.0070\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=24,  total_loss=0.0000, loss_U=0.0000, loss_P=351.4098\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=25,  total_loss=0.0000, loss_U=0.0000, loss_P=361.7666\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=26,  total_loss=0.0000, loss_U=0.0000, loss_P=371.3008\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=27,  total_loss=0.0000, loss_U=0.0000, loss_P=377.4675\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=28,  total_loss=0.0000, loss_U=0.0000, loss_P=389.7442\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=29,  total_loss=0.0000, loss_U=0.0000, loss_P=395.5109\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=30,  total_loss=0.0000, loss_U=0.0000, loss_P=407.5061\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=31,  total_loss=0.0000, loss_U=0.0000, loss_P=420.9409\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=32,  total_loss=0.0000, loss_U=0.0000, loss_P=429.5163\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=33,  total_loss=0.0000, loss_U=0.0000, loss_P=431.5163\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=34,  total_loss=0.0000, loss_U=0.0000, loss_P=432.5663\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k=35,  total_loss=0.0000, loss_U=0.0000, loss_P=433.8997\n",
      "INFO:anonymigraph.anonymization._method_private_colors.optimizers:Optimal 1d: k_best=21,  total_loss=0.0000, loss_U=0.0000, loss_P=316.4953\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Optimal 1D         : num_clusters=21, sub_opt_gap=3.22e-07, total_loss=3.7189e-07, loss_U=5.5399e-08, loss_P=316.5 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Eager Local Search : num_clusters=22, sub_opt_gap=3.1762e-07, total_loss=3.6751e-07, loss_U=4.6567e-08, loss_P=320.94 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Greedy Local Search: num_clusters=22, sub_opt_gap=3.1762e-07, total_loss=3.6751e-07, loss_U=4.6567e-08, loss_P=320.94 \n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 0, Total Loss: 0.0049, Loss U: 0.0049, Loss P: 58.7792, Entropy: 720.9520, Entropy Lam: 1e-07\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1000, Total Loss: 0.0001, Loss U: 0.0001, Loss P: 527.8300, Entropy: 0.2334, Entropy Lam: 6.024006916124241e-06\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:Epoch 1209, Total Loss: 0.0002, Loss U: 0.0002, Loss P: 527.8939, Entropy: 0.0997, Entropy Lam: 1.5624722518287514e-05\n",
      "INFO:anonymigraph.anonymization.method_private_colors_soft_assignment:used clusters / max_clusters: 47/50\n",
      "INFO:anonymigraph.utils:Katz converged after 7 iterations.\n",
      "INFO:root:Soft Assign w/ Entropy Reg.: num_clusters=47, sub_opt_gap=1.5363e-05, total_loss=1.5413e-05, loss_U=1.4885e-05, loss_P=527.94 \n"
     ]
    }
   ],
   "source": [
    "def evaluate(G, alpha, w, seed, beta=1):\n",
    "    eager_local_search = LocalSearchColorOptimizer(G, w=w, alpha=alpha, beta=beta, is_eager=True)\n",
    "    eager_local_search.fit(seed = seed)\n",
    "\n",
    "    greedy_local_search = LocalSearchColorOptimizer(G, w=w, alpha=alpha, beta=beta, is_eager=False)\n",
    "    greedy_local_search.fit()\n",
    "\n",
    "    opti1d_coloropti = Optimal1dColorOptimizer(G, w=w, alpha=alpha, beta=beta)\n",
    "    opti1d_coloropti.fit()\n",
    "\n",
    "    sf_eval_opti1d = SamplingFreeEvaluator(G, opti1d_coloropti.colors, w=w, alpha=alpha, beta=beta, use_katz_utility=True)\n",
    "    eval_opti1d = sf_eval_opti1d.get_results()\n",
    "    logging.info(f\"Optimal 1D         : \"\n",
    "                 f\"num_clusters={eval_opti1d['num_clusters']}, \"\n",
    "                 f\"sub_opt_gap={eval_opti1d['sub_opt_gap']:.5g}, \"\n",
    "                 f\"total_loss={eval_opti1d['total_loss']:.5g}, \"\n",
    "                 f\"loss_U={eval_opti1d['loss_U']:.5g}, \"\n",
    "                 f\"loss_P={eval_opti1d['loss_P']:.5g} \")\n",
    "\n",
    "    sf_eval_eager_local_search = SamplingFreeEvaluator(G, eager_local_search.colors, w=w, alpha=alpha, beta=beta, use_katz_utility=True)\n",
    "    eval_eager_local_search = sf_eval_eager_local_search.get_results()\n",
    "    logging.info(f\"Eager Local Search : \"\n",
    "                f\"num_clusters={eval_eager_local_search['num_clusters']}, \"\n",
    "                f\"sub_opt_gap={eval_eager_local_search['sub_opt_gap']:.5g}, \"\n",
    "                f\"total_loss={eval_eager_local_search['total_loss']:.5g}, \"\n",
    "                f\"loss_U={eval_eager_local_search['loss_U']:.5g}, \"\n",
    "                f\"loss_P={eval_eager_local_search['loss_P']:.5g} \")\n",
    "\n",
    "    sf_eval_greedy_local_search = SamplingFreeEvaluator(G, greedy_local_search.colors, w=w, alpha=alpha, beta=beta, use_katz_utility=True)\n",
    "    eval_greedy_local_search = sf_eval_greedy_local_search.get_results()\n",
    "    logging.info(f\"Greedy Local Search: \"\n",
    "                f\"num_clusters={eval_greedy_local_search['num_clusters']}, \"\n",
    "                f\"sub_opt_gap={eval_greedy_local_search['sub_opt_gap']:.5g}, \"\n",
    "                f\"total_loss={eval_greedy_local_search['total_loss']:.5g}, \"\n",
    "                f\"loss_U={eval_greedy_local_search['loss_U']:.5g}, \"\n",
    "                f\"loss_P={eval_greedy_local_search['loss_P']:.5g} \")\n",
    "\n",
    "    soft_optim_katz = SoftColorOptimizer(\n",
    "        G, k_max=50, w=w,\n",
    "        use_katz_utility = True,\n",
    "        use_entropy_reg = True,\n",
    "        alpha=alpha, beta=beta,\n",
    "        eps_utility = 1e-9, eps_privacy = 1e-9,\n",
    "        lr=0.1, patience=20, threshold=1e-3, initial_lam=1e-7, factor=1.1,\n",
    "        device='cpu', seed=seed\n",
    "    )\n",
    "\n",
    "    soft_optim_katz.fit(max_epochs=int(1e8), epoch_report_frequency=1000)\n",
    "    sf_eval_soft_assign_katz = SamplingFreeEvaluator(G, soft_optim_katz.colors, w=w, alpha=alpha, beta=beta, use_katz_utility=True)\n",
    "    eval_soft_assign_katz = sf_eval_soft_assign_katz.get_results()\n",
    "    logging.info(f\"Soft Assign w/ Entropy Reg.: \"\n",
    "                f\"num_clusters={eval_soft_assign_katz['num_clusters']}, \"\n",
    "                f\"sub_opt_gap={eval_soft_assign_katz['sub_opt_gap']:.5g}, \"\n",
    "                f\"total_loss={eval_soft_assign_katz['total_loss']:.5g}, \"\n",
    "                f\"loss_U={eval_soft_assign_katz['loss_U']:.5g}, \"\n",
    "                f\"loss_P={eval_soft_assign_katz['loss_P']:.5g} \")\n",
    "\n",
    "    return {\"Optimal 1D\": eval_opti1d, \"First-Improv.\": eval_eager_local_search, \"Best-Improv.\": eval_greedy_local_search, \"Soft Assign\": eval_soft_assign_katz}\n",
    "\n",
    "\n",
    "def no_self_loops_LFR(*args, **kwargs):\n",
    "    G = nx.generators.community.LFR_benchmark_graph(*args, **kwargs)\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    return G\n",
    "\n",
    "\n",
    "graph_types = {\n",
    "    \"Erdos-Renyi\":  nx.erdos_renyi_graph,\n",
    "    \"Barabasi-Albert\": nx.barabasi_albert_graph,\n",
    "    \"LFR\": no_self_loops_LFR\n",
    "}\n",
    "\n",
    "graph_params = {\n",
    "    \"Erdos-Renyi\": {\"n\": 200, \"p\": 0.05, \"seed\": 1, \"g_type\": \"Erdos-Renyi\"}, # Erdos-Renyi - No High Degree Nodes\n",
    "    \"Barabasi-Albert\": {\"n\": 200, \"m\": 5, \"seed\": 1, \"g_type\": \"Barabasi-Albert\"},\n",
    "    \"LFR\": {\"n\": 200, \"tau1\": 3, \"tau2\": 1.5, \"mu\": 0.1, \"average_degree\": 7, \"seed\": 42, \"min_community\": 60, \"g_type\": \"LFR\"} # Default LFR - PowerLaw + Communities\n",
    "    #\"Barabasi-Albert (Dense)\": {\"n\": 200, \"m\": 10, \"seed\": 42, \"g_type\": \"Barabasi-Albert\"},\n",
    "}\n",
    "\n",
    "\n",
    "# Encodes grid of Graph x Relative Alpha x w (dicts are nested in this order)\n",
    "g_rel_alpha_w_grid = {\n",
    "    \"Erdos-Renyi\": {\n",
    "        0.9:  10**(-np.linspace(-2, 2, 4 + 1)),\n",
    "        0.01: 10**(-np.linspace(4, 8, 4 + 1)),\n",
    "    },\n",
    "    \"Barabasi-Albert\": {\n",
    "        0.9: 10**(-np.linspace(-2, 2, 4 + 1)),\n",
    "        0.01: 10**(-np.linspace(4, 8, 4 + 1)),\n",
    "    },\n",
    "    \"LFR\": {\n",
    "        0.9:  10**(-np.linspace(-1, 2, 3 + 1)),\n",
    "        0.01: 10**(-np.linspace(4, 9, 4 + 1)),\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "graphs = {}\n",
    "\n",
    "for graph_name in g_rel_alpha_w_grid.keys():\n",
    "    params = graph_params[graph_name].copy()\n",
    "    g_type = params.pop(\"g_type\")\n",
    "    graphs[graph_name] = graph_types[g_type](**params)\n",
    "\n",
    "results = []\n",
    "\n",
    "for graph_name in g_rel_alpha_w_grid.keys():\n",
    "    G = graphs[graph_name]\n",
    "\n",
    "    eigenvalues, _ = eigs(nx.adjacency_matrix(G).astype(np.float64), k=1, which='LM')\n",
    "    max_alpha = 1 / np.abs(eigenvalues).max()\n",
    "\n",
    "    for rel_alpha in g_rel_alpha_w_grid[graph_name]:\n",
    "        for w in g_rel_alpha_w_grid[graph_name][rel_alpha]:\n",
    "            logging.info(f\"Evaluating with parameters: w={w:.3g}, rel_alpha={rel_alpha:.3g}, G_name={graph_name}, G={str(G)}, seed={params[\"seed\"]}\")\n",
    "\n",
    "            algo_results = evaluate(G, rel_alpha * max_alpha, w, graph_params[graph_name][\"seed\"])\n",
    "            for algo_name in algo_results:\n",
    "                row = {\n",
    "                    \"graph\": graph_name,\n",
    "                    \"algo\": algo_name,\n",
    "                    \"rel_alpha\": rel_alpha,\n",
    "                    \"w\": w,\n",
    "                    **algo_results[algo_name]\n",
    "                }\n",
    "                results.append(row)\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "algo\n",
       "Greedy                1.913793\n",
       "Eager                 1.965517\n",
       "num_clusters_opt1d    3.482759\n",
       "Optimal 1D            3.500000\n",
       "Soft Assign           4.137931\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "rank_counts = Counter()\n",
    "rows = []\n",
    "\n",
    "for graph_name in g_rel_alpha_w_grid.keys():\n",
    "    for rel_alpha in g_rel_alpha_w_grid[graph_name]:\n",
    "        for w in g_rel_alpha_w_grid[graph_name][rel_alpha]:\n",
    "            df_graph = df_results[(df_results['graph'] == graph_name) & (df_results['rel_alpha'] == rel_alpha) & (df_results['w'] == w)]\n",
    "            num_clusters_opt1d = df_graph[df_graph[\"algo\"] == \"Optimal 1D\"][\"num_clusters\"].item()\n",
    "            gap_opt1d = df_graph[df_graph[\"algo\"] == \"Optimal 1D\"][\"sub_opt_gap\"].item()\n",
    "            gap_eager = df_graph[df_graph[\"algo\"] == \"First-Improv.\"][\"sub_opt_gap\"].item()\n",
    "            gap_greedy = df_graph[df_graph[\"algo\"] == \"Best-Improv.\"][\"sub_opt_gap\"].item()\n",
    "            gap_soft = df_graph[df_graph[\"algo\"] == \"Soft Assign\"][\"sub_opt_gap\"].item()\n",
    "\n",
    "            gaps = {\"Soft Assign\": gap_soft, \"Optimal 1D\": gap_opt1d,\"Eager\": gap_eager,\"Greedy\": gap_greedy, \"num_clusters_opt1d\":num_clusters_opt1d}\n",
    "            rows.append(gaps)\n",
    "\n",
    "            # for ranking\n",
    "            df = pd.DataFrame(list(gaps.items()), columns=['algo', 'gap'])\n",
    "            df['rank'] = df['gap'].rank(method='average', ascending=True)\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                rank_counts[(row['algo'], row['rank'])] += 1\n",
    "\n",
    "average_ranks = (\n",
    "    pd.DataFrame([(algo, rank * freq, freq) for (algo, rank), freq in rank_counts.items()],\n",
    "                 columns=[\"algo\", \"rank_sum\", \"count\"])\n",
    "    .groupby(\"algo\")\n",
    "    .apply(lambda x: x[\"rank_sum\"].sum() / x[\"count\"].sum(), include_groups=False)\n",
    "    .sort_values()\n",
    ")\n",
    "display(average_ranks)\n",
    "\n",
    "\n",
    "# Normalize\n",
    "df = pd.DataFrame(rows)\n",
    "df[\"Soft Assign\"] = df[\"Soft Assign\"] / df[\"Greedy\"]\n",
    "df[\"Optimal 1D\"] = df[\"Optimal 1D\"] / df[\"Greedy\"]\n",
    "df[\"Eager\"] = df[\"Eager\"] / df[\"Greedy\"]\n",
    "df[\"Greedy\"] = df[\"Greedy\"] / df[\"Greedy\"]\n",
    "\n",
    "# Hue Choice\n",
    "df.rename(columns={\"num_clusters_opt1d\": \"hue\"}, inplace=True)\n",
    "#df['hue'] = df[\"Optimal 1D\"]\n",
    "df['hue'] = df['hue'] + np.random.normal(0, 1e-7, size=len(df)) # need unique values here, otherwise we plot areas in parallel coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Descriptive statistics for the normalized suboptimality gaps for the proposed algorithms across various graphs and hyperparameter configurations.}\n",
      "\\label{tab:exp1_descriptive}\n",
      "\\begin{tabular}{lcccc}\n",
      "\\toprule\n",
      " & Soft Assign & Optimal 1D & Eager & Greedy \\\\\n",
      "\\midrule\n",
      "\\textbf{mean} & 1.16 & 1.05 & 1.00 & 1.00 \\\\\n",
      "\\textbf{min} & 1.00 & 1.00 & 0.99 & 1.00 \\\\\n",
      "\\textbf{50%} & 1.08 & 1.02 & 1.00 & 1.00 \\\\\n",
      "\\textbf{max} & 1.76 & 1.33 & 1.02 & 1.00 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stats = df.where(df <= 2, np.nan)\n",
    "df_stats = df_stats.drop(\"hue\", axis=1)\n",
    "stats = df_stats.describe()\n",
    "stats = stats.drop([\"count\", \"25%\", \"75%\", \"std\"])\n",
    "latex_code = stats.to_latex(index=True,\n",
    "                         caption=\"Descriptive statistics for the normalized suboptimality gaps for the proposed algorithms across various graphs and hyperparameter configurations.\",\n",
    "                         label=\"tab:exp1_descriptive\",\n",
    "                         bold_rows=True,\n",
    "                         column_format=\"lcccc\",\n",
    "                         float_format=\"%.2f\"\n",
    ")\n",
    "\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Violin plot\n",
    "df_long_no_outlier = df.melt(id_vars='hue', var_name='algorithm', value_name='gap')\n",
    "df_long_no_outlier[\"gap\"] = df_long_no_outlier[\"gap\"].where(df_long_no_outlier[\"gap\"] <= 2, np.nan)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\", palette=\"pastel\", font_scale=1.5)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='algorithm', y='gap', data=df_long_no_outlier, inner=None, linewidth=1, density_norm=\"width\", bw_method=0.4)\n",
    "sns.stripplot(x='algorithm', y='gap', data=df_long_no_outlier, jitter=True, edgecolor=\"black\", linewidth=0.5, alpha=0.8)\n",
    "\n",
    "plt.ylabel('Normalized Suboptimality Gap')\n",
    "plt.xlabel('Algorithm')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"figures/suboptimality_gap_improvements.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Name: Erdos-Renyi, rel_alpha: 0.9, Graph with 200 nodes and 1025 edges\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cd78d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cd78d_level0_col0\" class=\"col_heading level0 col0\" colspan=\"4\">sub_opt_gap</th>\n",
       "      <th id=\"T_cd78d_level0_col4\" class=\"col_heading level0 col4\" colspan=\"4\">num_clusters</th>\n",
       "      <th id=\"T_cd78d_level0_col8\" class=\"col_heading level0 col8\" colspan=\"4\">total_loss</th>\n",
       "      <th id=\"T_cd78d_level0_col12\" class=\"col_heading level0 col12\" colspan=\"4\">loss_U</th>\n",
       "      <th id=\"T_cd78d_level0_col16\" class=\"col_heading level0 col16\" colspan=\"4\">loss_P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level1\" >algo</th>\n",
       "      <th id=\"T_cd78d_level1_col0\" class=\"col_heading level1 col0\" >First-Improv.</th>\n",
       "      <th id=\"T_cd78d_level1_col1\" class=\"col_heading level1 col1\" >Best-Improv.</th>\n",
       "      <th id=\"T_cd78d_level1_col2\" class=\"col_heading level1 col2\" >Optimal 1D</th>\n",
       "      <th id=\"T_cd78d_level1_col3\" class=\"col_heading level1 col3\" >Soft Assign</th>\n",
       "      <th id=\"T_cd78d_level1_col4\" class=\"col_heading level1 col4\" >First-Improv.</th>\n",
       "      <th id=\"T_cd78d_level1_col5\" class=\"col_heading level1 col5\" >Best-Improv.</th>\n",
       "      <th id=\"T_cd78d_level1_col6\" class=\"col_heading level1 col6\" >Optimal 1D</th>\n",
       "      <th id=\"T_cd78d_level1_col7\" class=\"col_heading level1 col7\" >Soft Assign</th>\n",
       "      <th id=\"T_cd78d_level1_col8\" class=\"col_heading level1 col8\" >First-Improv.</th>\n",
       "      <th id=\"T_cd78d_level1_col9\" class=\"col_heading level1 col9\" >Best-Improv.</th>\n",
       "      <th id=\"T_cd78d_level1_col10\" class=\"col_heading level1 col10\" >Optimal 1D</th>\n",
       "      <th id=\"T_cd78d_level1_col11\" class=\"col_heading level1 col11\" >Soft Assign</th>\n",
       "      <th id=\"T_cd78d_level1_col12\" class=\"col_heading level1 col12\" >First-Improv.</th>\n",
       "      <th id=\"T_cd78d_level1_col13\" class=\"col_heading level1 col13\" >Best-Improv.</th>\n",
       "      <th id=\"T_cd78d_level1_col14\" class=\"col_heading level1 col14\" >Optimal 1D</th>\n",
       "      <th id=\"T_cd78d_level1_col15\" class=\"col_heading level1 col15\" >Soft Assign</th>\n",
       "      <th id=\"T_cd78d_level1_col16\" class=\"col_heading level1 col16\" >First-Improv.</th>\n",
       "      <th id=\"T_cd78d_level1_col17\" class=\"col_heading level1 col17\" >Best-Improv.</th>\n",
       "      <th id=\"T_cd78d_level1_col18\" class=\"col_heading level1 col18\" >Optimal 1D</th>\n",
       "      <th id=\"T_cd78d_level1_col19\" class=\"col_heading level1 col19\" >Soft Assign</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >w</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "      <th class=\"blank col11\" >&nbsp;</th>\n",
       "      <th class=\"blank col12\" >&nbsp;</th>\n",
       "      <th class=\"blank col13\" >&nbsp;</th>\n",
       "      <th class=\"blank col14\" >&nbsp;</th>\n",
       "      <th class=\"blank col15\" >&nbsp;</th>\n",
       "      <th class=\"blank col16\" >&nbsp;</th>\n",
       "      <th class=\"blank col17\" >&nbsp;</th>\n",
       "      <th class=\"blank col18\" >&nbsp;</th>\n",
       "      <th class=\"blank col19\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cd78d_level0_row0\" class=\"row_heading level0 row0\" >100</th>\n",
       "      <td id=\"T_cd78d_row0_col0\" class=\"data row0 col0\" >1252</td>\n",
       "      <td id=\"T_cd78d_row0_col1\" class=\"data row0 col1\" >1256</td>\n",
       "      <td id=\"T_cd78d_row0_col2\" class=\"data row0 col2\" >1666</td>\n",
       "      <td id=\"T_cd78d_row0_col3\" class=\"data row0 col3\" >1851</td>\n",
       "      <td id=\"T_cd78d_row0_col4\" class=\"data row0 col4\" >2</td>\n",
       "      <td id=\"T_cd78d_row0_col5\" class=\"data row0 col5\" >2</td>\n",
       "      <td id=\"T_cd78d_row0_col6\" class=\"data row0 col6\" >1</td>\n",
       "      <td id=\"T_cd78d_row0_col7\" class=\"data row0 col7\" >3</td>\n",
       "      <td id=\"T_cd78d_row0_col8\" class=\"data row0 col8\" >7713</td>\n",
       "      <td id=\"T_cd78d_row0_col9\" class=\"data row0 col9\" >7717</td>\n",
       "      <td id=\"T_cd78d_row0_col10\" class=\"data row0 col10\" >8128</td>\n",
       "      <td id=\"T_cd78d_row0_col11\" class=\"data row0 col11\" >8312</td>\n",
       "      <td id=\"T_cd78d_row0_col12\" class=\"data row0 col12\" >824.9</td>\n",
       "      <td id=\"T_cd78d_row0_col13\" class=\"data row0 col13\" >782.7</td>\n",
       "      <td id=\"T_cd78d_row0_col14\" class=\"data row0 col14\" >1666</td>\n",
       "      <td id=\"T_cd78d_row0_col15\" class=\"data row0 col15\" >818.6</td>\n",
       "      <td id=\"T_cd78d_row0_col16\" class=\"data row0 col16\" >68.89</td>\n",
       "      <td id=\"T_cd78d_row0_col17\" class=\"data row0 col17\" >69.35</td>\n",
       "      <td id=\"T_cd78d_row0_col18\" class=\"data row0 col18\" >64.62</td>\n",
       "      <td id=\"T_cd78d_row0_col19\" class=\"data row0 col19\" >74.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd78d_level0_row1\" class=\"row_heading level0 row1\" >10</th>\n",
       "      <td id=\"T_cd78d_row1_col0\" class=\"data row1 col0\" >473.5</td>\n",
       "      <td id=\"T_cd78d_row1_col1\" class=\"data row1 col1\" >472.2</td>\n",
       "      <td id=\"T_cd78d_row1_col2\" class=\"data row1 col2\" >522.5</td>\n",
       "      <td id=\"T_cd78d_row1_col3\" class=\"data row1 col3\" >475.1</td>\n",
       "      <td id=\"T_cd78d_row1_col4\" class=\"data row1 col4\" >4</td>\n",
       "      <td id=\"T_cd78d_row1_col5\" class=\"data row1 col5\" >4</td>\n",
       "      <td id=\"T_cd78d_row1_col6\" class=\"data row1 col6\" >4</td>\n",
       "      <td id=\"T_cd78d_row1_col7\" class=\"data row1 col7\" >5</td>\n",
       "      <td id=\"T_cd78d_row1_col8\" class=\"data row1 col8\" >1120</td>\n",
       "      <td id=\"T_cd78d_row1_col9\" class=\"data row1 col9\" >1118</td>\n",
       "      <td id=\"T_cd78d_row1_col10\" class=\"data row1 col10\" >1169</td>\n",
       "      <td id=\"T_cd78d_row1_col11\" class=\"data row1 col11\" >1121</td>\n",
       "      <td id=\"T_cd78d_row1_col12\" class=\"data row1 col12\" >219.8</td>\n",
       "      <td id=\"T_cd78d_row1_col13\" class=\"data row1 col13\" >214.2</td>\n",
       "      <td id=\"T_cd78d_row1_col14\" class=\"data row1 col14\" >168.4</td>\n",
       "      <td id=\"T_cd78d_row1_col15\" class=\"data row1 col15\" >161.5</td>\n",
       "      <td id=\"T_cd78d_row1_col16\" class=\"data row1 col16\" >89.99</td>\n",
       "      <td id=\"T_cd78d_row1_col17\" class=\"data row1 col17\" >90.42</td>\n",
       "      <td id=\"T_cd78d_row1_col18\" class=\"data row1 col18\" >100</td>\n",
       "      <td id=\"T_cd78d_row1_col19\" class=\"data row1 col19\" >95.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd78d_level0_row2\" class=\"row_heading level0 row2\" >1</th>\n",
       "      <td id=\"T_cd78d_row2_col0\" class=\"data row2 col0\" >125.4</td>\n",
       "      <td id=\"T_cd78d_row2_col1\" class=\"data row2 col1\" >124.4</td>\n",
       "      <td id=\"T_cd78d_row2_col2\" class=\"data row2 col2\" >125.6</td>\n",
       "      <td id=\"T_cd78d_row2_col3\" class=\"data row2 col3\" >126</td>\n",
       "      <td id=\"T_cd78d_row2_col4\" class=\"data row2 col4\" >8</td>\n",
       "      <td id=\"T_cd78d_row2_col5\" class=\"data row2 col5\" >8</td>\n",
       "      <td id=\"T_cd78d_row2_col6\" class=\"data row2 col6\" >7</td>\n",
       "      <td id=\"T_cd78d_row2_col7\" class=\"data row2 col7\" >9</td>\n",
       "      <td id=\"T_cd78d_row2_col8\" class=\"data row2 col8\" >190</td>\n",
       "      <td id=\"T_cd78d_row2_col9\" class=\"data row2 col9\" >189.1</td>\n",
       "      <td id=\"T_cd78d_row2_col10\" class=\"data row2 col10\" >190.2</td>\n",
       "      <td id=\"T_cd78d_row2_col11\" class=\"data row2 col11\" >190.6</td>\n",
       "      <td id=\"T_cd78d_row2_col12\" class=\"data row2 col12\" >47.95</td>\n",
       "      <td id=\"T_cd78d_row2_col13\" class=\"data row2 col13\" >47.75</td>\n",
       "      <td id=\"T_cd78d_row2_col14\" class=\"data row2 col14\" >55.41</td>\n",
       "      <td id=\"T_cd78d_row2_col15\" class=\"data row2 col15\" >44.54</td>\n",
       "      <td id=\"T_cd78d_row2_col16\" class=\"data row2 col16\" >142</td>\n",
       "      <td id=\"T_cd78d_row2_col17\" class=\"data row2 col17\" >141.3</td>\n",
       "      <td id=\"T_cd78d_row2_col18\" class=\"data row2 col18\" >134.8</td>\n",
       "      <td id=\"T_cd78d_row2_col19\" class=\"data row2 col19\" >146.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd78d_level0_row3\" class=\"row_heading level0 row3\" >0.1</th>\n",
       "      <td id=\"T_cd78d_row3_col0\" class=\"data row3 col0\" >29.88</td>\n",
       "      <td id=\"T_cd78d_row3_col1\" class=\"data row3 col1\" >29.88</td>\n",
       "      <td id=\"T_cd78d_row3_col2\" class=\"data row3 col2\" >30.36</td>\n",
       "      <td id=\"T_cd78d_row3_col3\" class=\"data row3 col3\" >30.26</td>\n",
       "      <td id=\"T_cd78d_row3_col4\" class=\"data row3 col4\" >15</td>\n",
       "      <td id=\"T_cd78d_row3_col5\" class=\"data row3 col5\" >15</td>\n",
       "      <td id=\"T_cd78d_row3_col6\" class=\"data row3 col6\" >13</td>\n",
       "      <td id=\"T_cd78d_row3_col7\" class=\"data row3 col7\" >18</td>\n",
       "      <td id=\"T_cd78d_row3_col8\" class=\"data row3 col8\" >36.34</td>\n",
       "      <td id=\"T_cd78d_row3_col9\" class=\"data row3 col9\" >36.34</td>\n",
       "      <td id=\"T_cd78d_row3_col10\" class=\"data row3 col10\" >36.82</td>\n",
       "      <td id=\"T_cd78d_row3_col11\" class=\"data row3 col11\" >36.72</td>\n",
       "      <td id=\"T_cd78d_row3_col12\" class=\"data row3 col12\" >11.7</td>\n",
       "      <td id=\"T_cd78d_row3_col13\" class=\"data row3 col13\" >11.7</td>\n",
       "      <td id=\"T_cd78d_row3_col14\" class=\"data row3 col14\" >14.02</td>\n",
       "      <td id=\"T_cd78d_row3_col15\" class=\"data row3 col15\" >8.688</td>\n",
       "      <td id=\"T_cd78d_row3_col16\" class=\"data row3 col16\" >246.4</td>\n",
       "      <td id=\"T_cd78d_row3_col17\" class=\"data row3 col17\" >246.4</td>\n",
       "      <td id=\"T_cd78d_row3_col18\" class=\"data row3 col18\" >228</td>\n",
       "      <td id=\"T_cd78d_row3_col19\" class=\"data row3 col19\" >280.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cd78d_level0_row4\" class=\"row_heading level0 row4\" >0.01</th>\n",
       "      <td id=\"T_cd78d_row4_col0\" class=\"data row4 col0\" >6.17</td>\n",
       "      <td id=\"T_cd78d_row4_col1\" class=\"data row4 col1\" >6.17</td>\n",
       "      <td id=\"T_cd78d_row4_col2\" class=\"data row4 col2\" >6.418</td>\n",
       "      <td id=\"T_cd78d_row4_col3\" class=\"data row4 col3\" >6.753</td>\n",
       "      <td id=\"T_cd78d_row4_col4\" class=\"data row4 col4\" >34</td>\n",
       "      <td id=\"T_cd78d_row4_col5\" class=\"data row4 col5\" >34</td>\n",
       "      <td id=\"T_cd78d_row4_col6\" class=\"data row4 col6\" >30</td>\n",
       "      <td id=\"T_cd78d_row4_col7\" class=\"data row4 col7\" >31</td>\n",
       "      <td id=\"T_cd78d_row4_col8\" class=\"data row4 col8\" >6.816</td>\n",
       "      <td id=\"T_cd78d_row4_col9\" class=\"data row4 col9\" >6.816</td>\n",
       "      <td id=\"T_cd78d_row4_col10\" class=\"data row4 col10\" >7.064</td>\n",
       "      <td id=\"T_cd78d_row4_col11\" class=\"data row4 col11\" >7.399</td>\n",
       "      <td id=\"T_cd78d_row4_col12\" class=\"data row4 col12\" >1.841</td>\n",
       "      <td id=\"T_cd78d_row4_col13\" class=\"data row4 col13\" >1.841</td>\n",
       "      <td id=\"T_cd78d_row4_col14\" class=\"data row4 col14\" >1.841</td>\n",
       "      <td id=\"T_cd78d_row4_col15\" class=\"data row4 col15\" >2.343</td>\n",
       "      <td id=\"T_cd78d_row4_col16\" class=\"data row4 col16\" >497.5</td>\n",
       "      <td id=\"T_cd78d_row4_col17\" class=\"data row4 col17\" >497.5</td>\n",
       "      <td id=\"T_cd78d_row4_col18\" class=\"data row4 col18\" >522.3</td>\n",
       "      <td id=\"T_cd78d_row4_col19\" class=\"data row4 col19\" >505.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2791e7feba0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Name: Erdos-Renyi, rel_alpha: 0.01, Graph with 200 nodes and 1025 edges\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6bcfe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6bcfe_level0_col0\" class=\"col_heading level0 col0\" colspan=\"4\">sub_opt_gap</th>\n",
       "      <th id=\"T_6bcfe_level0_col4\" class=\"col_heading level0 col4\" colspan=\"4\">num_clusters</th>\n",
       "      <th id=\"T_6bcfe_level0_col8\" class=\"col_heading level0 col8\" colspan=\"4\">total_loss</th>\n",
       "      <th id=\"T_6bcfe_level0_col12\" class=\"col_heading level0 col12\" colspan=\"4\">loss_U</th>\n",
       "      <th id=\"T_6bcfe_level0_col16\" class=\"col_heading level0 col16\" colspan=\"4\">loss_P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level1\" >algo</th>\n",
       "      <th id=\"T_6bcfe_level1_col0\" class=\"col_heading level1 col0\" >First-Improv.</th>\n",
       "      <th id=\"T_6bcfe_level1_col1\" class=\"col_heading level1 col1\" >Best-Improv.</th>\n",
       "      <th id=\"T_6bcfe_level1_col2\" class=\"col_heading level1 col2\" >Optimal 1D</th>\n",
       "      <th id=\"T_6bcfe_level1_col3\" class=\"col_heading level1 col3\" >Soft Assign</th>\n",
       "      <th id=\"T_6bcfe_level1_col4\" class=\"col_heading level1 col4\" >First-Improv.</th>\n",
       "      <th id=\"T_6bcfe_level1_col5\" class=\"col_heading level1 col5\" >Best-Improv.</th>\n",
       "      <th id=\"T_6bcfe_level1_col6\" class=\"col_heading level1 col6\" >Optimal 1D</th>\n",
       "      <th id=\"T_6bcfe_level1_col7\" class=\"col_heading level1 col7\" >Soft Assign</th>\n",
       "      <th id=\"T_6bcfe_level1_col8\" class=\"col_heading level1 col8\" >First-Improv.</th>\n",
       "      <th id=\"T_6bcfe_level1_col9\" class=\"col_heading level1 col9\" >Best-Improv.</th>\n",
       "      <th id=\"T_6bcfe_level1_col10\" class=\"col_heading level1 col10\" >Optimal 1D</th>\n",
       "      <th id=\"T_6bcfe_level1_col11\" class=\"col_heading level1 col11\" >Soft Assign</th>\n",
       "      <th id=\"T_6bcfe_level1_col12\" class=\"col_heading level1 col12\" >First-Improv.</th>\n",
       "      <th id=\"T_6bcfe_level1_col13\" class=\"col_heading level1 col13\" >Best-Improv.</th>\n",
       "      <th id=\"T_6bcfe_level1_col14\" class=\"col_heading level1 col14\" >Optimal 1D</th>\n",
       "      <th id=\"T_6bcfe_level1_col15\" class=\"col_heading level1 col15\" >Soft Assign</th>\n",
       "      <th id=\"T_6bcfe_level1_col16\" class=\"col_heading level1 col16\" >First-Improv.</th>\n",
       "      <th id=\"T_6bcfe_level1_col17\" class=\"col_heading level1 col17\" >Best-Improv.</th>\n",
       "      <th id=\"T_6bcfe_level1_col18\" class=\"col_heading level1 col18\" >Optimal 1D</th>\n",
       "      <th id=\"T_6bcfe_level1_col19\" class=\"col_heading level1 col19\" >Soft Assign</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >w</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "      <th class=\"blank col11\" >&nbsp;</th>\n",
       "      <th class=\"blank col12\" >&nbsp;</th>\n",
       "      <th class=\"blank col13\" >&nbsp;</th>\n",
       "      <th class=\"blank col14\" >&nbsp;</th>\n",
       "      <th class=\"blank col15\" >&nbsp;</th>\n",
       "      <th class=\"blank col16\" >&nbsp;</th>\n",
       "      <th class=\"blank col17\" >&nbsp;</th>\n",
       "      <th class=\"blank col18\" >&nbsp;</th>\n",
       "      <th class=\"blank col19\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6bcfe_level0_row0\" class=\"row_heading level0 row0\" >0.0001</th>\n",
       "      <td id=\"T_6bcfe_row0_col0\" class=\"data row0 col0\" >0.001249</td>\n",
       "      <td id=\"T_6bcfe_row0_col1\" class=\"data row0 col1\" >0.001265</td>\n",
       "      <td id=\"T_6bcfe_row0_col2\" class=\"data row0 col2\" >0.001533</td>\n",
       "      <td id=\"T_6bcfe_row0_col3\" class=\"data row0 col3\" >0.00145</td>\n",
       "      <td id=\"T_6bcfe_row0_col4\" class=\"data row0 col4\" >2</td>\n",
       "      <td id=\"T_6bcfe_row0_col5\" class=\"data row0 col5\" >2</td>\n",
       "      <td id=\"T_6bcfe_row0_col6\" class=\"data row0 col6\" >2</td>\n",
       "      <td id=\"T_6bcfe_row0_col7\" class=\"data row0 col7\" >3</td>\n",
       "      <td id=\"T_6bcfe_row0_col8\" class=\"data row0 col8\" >0.007711</td>\n",
       "      <td id=\"T_6bcfe_row0_col9\" class=\"data row0 col9\" >0.007727</td>\n",
       "      <td id=\"T_6bcfe_row0_col10\" class=\"data row0 col10\" >0.007995</td>\n",
       "      <td id=\"T_6bcfe_row0_col11\" class=\"data row0 col11\" >0.007912</td>\n",
       "      <td id=\"T_6bcfe_row0_col12\" class=\"data row0 col12\" >0.0008074</td>\n",
       "      <td id=\"T_6bcfe_row0_col13\" class=\"data row0 col13\" >0.0008367</td>\n",
       "      <td id=\"T_6bcfe_row0_col14\" class=\"data row0 col14\" >0.0006224</td>\n",
       "      <td id=\"T_6bcfe_row0_col15\" class=\"data row0 col15\" >0.0006859</td>\n",
       "      <td id=\"T_6bcfe_row0_col16\" class=\"data row0 col16\" >69.03</td>\n",
       "      <td id=\"T_6bcfe_row0_col17\" class=\"data row0 col17\" >68.9</td>\n",
       "      <td id=\"T_6bcfe_row0_col18\" class=\"data row0 col18\" >73.72</td>\n",
       "      <td id=\"T_6bcfe_row0_col19\" class=\"data row0 col19\" >72.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6bcfe_level0_row1\" class=\"row_heading level0 row1\" >1e-05</th>\n",
       "      <td id=\"T_6bcfe_row1_col0\" class=\"data row1 col0\" >0.0004752</td>\n",
       "      <td id=\"T_6bcfe_row1_col1\" class=\"data row1 col1\" >0.0004779</td>\n",
       "      <td id=\"T_6bcfe_row1_col2\" class=\"data row1 col2\" >0.0005113</td>\n",
       "      <td id=\"T_6bcfe_row1_col3\" class=\"data row1 col3\" >0.0004977</td>\n",
       "      <td id=\"T_6bcfe_row1_col4\" class=\"data row1 col4\" >4</td>\n",
       "      <td id=\"T_6bcfe_row1_col5\" class=\"data row1 col5\" >4</td>\n",
       "      <td id=\"T_6bcfe_row1_col6\" class=\"data row1 col6\" >4</td>\n",
       "      <td id=\"T_6bcfe_row1_col7\" class=\"data row1 col7\" >5</td>\n",
       "      <td id=\"T_6bcfe_row1_col8\" class=\"data row1 col8\" >0.001121</td>\n",
       "      <td id=\"T_6bcfe_row1_col9\" class=\"data row1 col9\" >0.001124</td>\n",
       "      <td id=\"T_6bcfe_row1_col10\" class=\"data row1 col10\" >0.001157</td>\n",
       "      <td id=\"T_6bcfe_row1_col11\" class=\"data row1 col11\" >0.001144</td>\n",
       "      <td id=\"T_6bcfe_row1_col12\" class=\"data row1 col12\" >0.0002244</td>\n",
       "      <td id=\"T_6bcfe_row1_col13\" class=\"data row1 col13\" >0.0002131</td>\n",
       "      <td id=\"T_6bcfe_row1_col14\" class=\"data row1 col14\" >0.0001848</td>\n",
       "      <td id=\"T_6bcfe_row1_col15\" class=\"data row1 col15\" >0.0002041</td>\n",
       "      <td id=\"T_6bcfe_row1_col16\" class=\"data row1 col16\" >89.7</td>\n",
       "      <td id=\"T_6bcfe_row1_col17\" class=\"data row1 col17\" >91.09</td>\n",
       "      <td id=\"T_6bcfe_row1_col18\" class=\"data row1 col18\" >97.27</td>\n",
       "      <td id=\"T_6bcfe_row1_col19\" class=\"data row1 col19\" >93.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6bcfe_level0_row2\" class=\"row_heading level0 row2\" >1e-06</th>\n",
       "      <td id=\"T_6bcfe_row2_col0\" class=\"data row2 col0\" >0.0001203</td>\n",
       "      <td id=\"T_6bcfe_row2_col1\" class=\"data row2 col1\" >0.0001203</td>\n",
       "      <td id=\"T_6bcfe_row2_col2\" class=\"data row2 col2\" >0.0001223</td>\n",
       "      <td id=\"T_6bcfe_row2_col3\" class=\"data row2 col3\" >0.0001409</td>\n",
       "      <td id=\"T_6bcfe_row2_col4\" class=\"data row2 col4\" >8</td>\n",
       "      <td id=\"T_6bcfe_row2_col5\" class=\"data row2 col5\" >8</td>\n",
       "      <td id=\"T_6bcfe_row2_col6\" class=\"data row2 col6\" >7</td>\n",
       "      <td id=\"T_6bcfe_row2_col7\" class=\"data row2 col7\" >11</td>\n",
       "      <td id=\"T_6bcfe_row2_col8\" class=\"data row2 col8\" >0.000185</td>\n",
       "      <td id=\"T_6bcfe_row2_col9\" class=\"data row2 col9\" >0.000185</td>\n",
       "      <td id=\"T_6bcfe_row2_col10\" class=\"data row2 col10\" >0.000187</td>\n",
       "      <td id=\"T_6bcfe_row2_col11\" class=\"data row2 col11\" >0.0002056</td>\n",
       "      <td id=\"T_6bcfe_row2_col12\" class=\"data row2 col12\" >4.337e-05</td>\n",
       "      <td id=\"T_6bcfe_row2_col13\" class=\"data row2 col13\" >4.337e-05</td>\n",
       "      <td id=\"T_6bcfe_row2_col14\" class=\"data row2 col14\" >5.065e-05</td>\n",
       "      <td id=\"T_6bcfe_row2_col15\" class=\"data row2 col15\" >3.27e-05</td>\n",
       "      <td id=\"T_6bcfe_row2_col16\" class=\"data row2 col16\" >141.6</td>\n",
       "      <td id=\"T_6bcfe_row2_col17\" class=\"data row2 col17\" >141.6</td>\n",
       "      <td id=\"T_6bcfe_row2_col18\" class=\"data row2 col18\" >136.3</td>\n",
       "      <td id=\"T_6bcfe_row2_col19\" class=\"data row2 col19\" >172.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6bcfe_level0_row3\" class=\"row_heading level0 row3\" >1e-07</th>\n",
       "      <td id=\"T_6bcfe_row3_col0\" class=\"data row3 col0\" >2.065e-05</td>\n",
       "      <td id=\"T_6bcfe_row3_col1\" class=\"data row3 col1\" >2.065e-05</td>\n",
       "      <td id=\"T_6bcfe_row3_col2\" class=\"data row3 col2\" >2.065e-05</td>\n",
       "      <td id=\"T_6bcfe_row3_col3\" class=\"data row3 col3\" >5.153e-05</td>\n",
       "      <td id=\"T_6bcfe_row3_col4\" class=\"data row3 col4\" >15</td>\n",
       "      <td id=\"T_6bcfe_row3_col5\" class=\"data row3 col5\" >15</td>\n",
       "      <td id=\"T_6bcfe_row3_col6\" class=\"data row3 col6\" >15</td>\n",
       "      <td id=\"T_6bcfe_row3_col7\" class=\"data row3 col7\" >34</td>\n",
       "      <td id=\"T_6bcfe_row3_col8\" class=\"data row3 col8\" >2.711e-05</td>\n",
       "      <td id=\"T_6bcfe_row3_col9\" class=\"data row3 col9\" >2.711e-05</td>\n",
       "      <td id=\"T_6bcfe_row3_col10\" class=\"data row3 col10\" >2.711e-05</td>\n",
       "      <td id=\"T_6bcfe_row3_col11\" class=\"data row3 col11\" >5.799e-05</td>\n",
       "      <td id=\"T_6bcfe_row3_col12\" class=\"data row3 col12\" >1.542e-06</td>\n",
       "      <td id=\"T_6bcfe_row3_col13\" class=\"data row3 col13\" >1.542e-06</td>\n",
       "      <td id=\"T_6bcfe_row3_col14\" class=\"data row3 col14\" >1.542e-06</td>\n",
       "      <td id=\"T_6bcfe_row3_col15\" class=\"data row3 col15\" >8.1e-06</td>\n",
       "      <td id=\"T_6bcfe_row3_col16\" class=\"data row3 col16\" >255.7</td>\n",
       "      <td id=\"T_6bcfe_row3_col17\" class=\"data row3 col17\" >255.7</td>\n",
       "      <td id=\"T_6bcfe_row3_col18\" class=\"data row3 col18\" >255.7</td>\n",
       "      <td id=\"T_6bcfe_row3_col19\" class=\"data row3 col19\" >498.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6bcfe_level0_row4\" class=\"row_heading level0 row4\" >1e-08</th>\n",
       "      <td id=\"T_6bcfe_row4_col0\" class=\"data row4 col0\" >2.289e-06</td>\n",
       "      <td id=\"T_6bcfe_row4_col1\" class=\"data row4 col1\" >2.289e-06</td>\n",
       "      <td id=\"T_6bcfe_row4_col2\" class=\"data row4 col2\" >2.289e-06</td>\n",
       "      <td id=\"T_6bcfe_row4_col3\" class=\"data row4 col3\" >2.428e-05</td>\n",
       "      <td id=\"T_6bcfe_row4_col4\" class=\"data row4 col4\" >17</td>\n",
       "      <td id=\"T_6bcfe_row4_col5\" class=\"data row4 col5\" >17</td>\n",
       "      <td id=\"T_6bcfe_row4_col6\" class=\"data row4 col6\" >17</td>\n",
       "      <td id=\"T_6bcfe_row4_col7\" class=\"data row4 col7\" >49</td>\n",
       "      <td id=\"T_6bcfe_row4_col8\" class=\"data row4 col8\" >2.935e-06</td>\n",
       "      <td id=\"T_6bcfe_row4_col9\" class=\"data row4 col9\" >2.935e-06</td>\n",
       "      <td id=\"T_6bcfe_row4_col10\" class=\"data row4 col10\" >2.935e-06</td>\n",
       "      <td id=\"T_6bcfe_row4_col11\" class=\"data row4 col11\" >2.493e-05</td>\n",
       "      <td id=\"T_6bcfe_row4_col12\" class=\"data row4 col12\" >1.177e-08</td>\n",
       "      <td id=\"T_6bcfe_row4_col13\" class=\"data row4 col13\" >1.177e-08</td>\n",
       "      <td id=\"T_6bcfe_row4_col14\" class=\"data row4 col14\" >1.177e-08</td>\n",
       "      <td id=\"T_6bcfe_row4_col15\" class=\"data row4 col15\" >1.754e-05</td>\n",
       "      <td id=\"T_6bcfe_row4_col16\" class=\"data row4 col16\" >292.3</td>\n",
       "      <td id=\"T_6bcfe_row4_col17\" class=\"data row4 col17\" >292.3</td>\n",
       "      <td id=\"T_6bcfe_row4_col18\" class=\"data row4 col18\" >292.3</td>\n",
       "      <td id=\"T_6bcfe_row4_col19\" class=\"data row4 col19\" >739.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x27917f9e0c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Name: Barabasi-Albert, rel_alpha: 0.9, Graph with 200 nodes and 975 edges\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_7b26c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7b26c_level0_col0\" class=\"col_heading level0 col0\" colspan=\"4\">sub_opt_gap</th>\n",
       "      <th id=\"T_7b26c_level0_col4\" class=\"col_heading level0 col4\" colspan=\"4\">num_clusters</th>\n",
       "      <th id=\"T_7b26c_level0_col8\" class=\"col_heading level0 col8\" colspan=\"4\">total_loss</th>\n",
       "      <th id=\"T_7b26c_level0_col12\" class=\"col_heading level0 col12\" colspan=\"4\">loss_U</th>\n",
       "      <th id=\"T_7b26c_level0_col16\" class=\"col_heading level0 col16\" colspan=\"4\">loss_P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level1\" >algo</th>\n",
       "      <th id=\"T_7b26c_level1_col0\" class=\"col_heading level1 col0\" >First-Improv.</th>\n",
       "      <th id=\"T_7b26c_level1_col1\" class=\"col_heading level1 col1\" >Best-Improv.</th>\n",
       "      <th id=\"T_7b26c_level1_col2\" class=\"col_heading level1 col2\" >Optimal 1D</th>\n",
       "      <th id=\"T_7b26c_level1_col3\" class=\"col_heading level1 col3\" >Soft Assign</th>\n",
       "      <th id=\"T_7b26c_level1_col4\" class=\"col_heading level1 col4\" >First-Improv.</th>\n",
       "      <th id=\"T_7b26c_level1_col5\" class=\"col_heading level1 col5\" >Best-Improv.</th>\n",
       "      <th id=\"T_7b26c_level1_col6\" class=\"col_heading level1 col6\" >Optimal 1D</th>\n",
       "      <th id=\"T_7b26c_level1_col7\" class=\"col_heading level1 col7\" >Soft Assign</th>\n",
       "      <th id=\"T_7b26c_level1_col8\" class=\"col_heading level1 col8\" >First-Improv.</th>\n",
       "      <th id=\"T_7b26c_level1_col9\" class=\"col_heading level1 col9\" >Best-Improv.</th>\n",
       "      <th id=\"T_7b26c_level1_col10\" class=\"col_heading level1 col10\" >Optimal 1D</th>\n",
       "      <th id=\"T_7b26c_level1_col11\" class=\"col_heading level1 col11\" >Soft Assign</th>\n",
       "      <th id=\"T_7b26c_level1_col12\" class=\"col_heading level1 col12\" >First-Improv.</th>\n",
       "      <th id=\"T_7b26c_level1_col13\" class=\"col_heading level1 col13\" >Best-Improv.</th>\n",
       "      <th id=\"T_7b26c_level1_col14\" class=\"col_heading level1 col14\" >Optimal 1D</th>\n",
       "      <th id=\"T_7b26c_level1_col15\" class=\"col_heading level1 col15\" >Soft Assign</th>\n",
       "      <th id=\"T_7b26c_level1_col16\" class=\"col_heading level1 col16\" >First-Improv.</th>\n",
       "      <th id=\"T_7b26c_level1_col17\" class=\"col_heading level1 col17\" >Best-Improv.</th>\n",
       "      <th id=\"T_7b26c_level1_col18\" class=\"col_heading level1 col18\" >Optimal 1D</th>\n",
       "      <th id=\"T_7b26c_level1_col19\" class=\"col_heading level1 col19\" >Soft Assign</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >w</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "      <th class=\"blank col11\" >&nbsp;</th>\n",
       "      <th class=\"blank col12\" >&nbsp;</th>\n",
       "      <th class=\"blank col13\" >&nbsp;</th>\n",
       "      <th class=\"blank col14\" >&nbsp;</th>\n",
       "      <th class=\"blank col15\" >&nbsp;</th>\n",
       "      <th class=\"blank col16\" >&nbsp;</th>\n",
       "      <th class=\"blank col17\" >&nbsp;</th>\n",
       "      <th class=\"blank col18\" >&nbsp;</th>\n",
       "      <th class=\"blank col19\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7b26c_level0_row0\" class=\"row_heading level0 row0\" >100</th>\n",
       "      <td id=\"T_7b26c_row0_col0\" class=\"data row0 col0\" >2445</td>\n",
       "      <td id=\"T_7b26c_row0_col1\" class=\"data row0 col1\" >2445</td>\n",
       "      <td id=\"T_7b26c_row0_col2\" class=\"data row0 col2\" >2787</td>\n",
       "      <td id=\"T_7b26c_row0_col3\" class=\"data row0 col3\" >3329</td>\n",
       "      <td id=\"T_7b26c_row0_col4\" class=\"data row0 col4\" >2</td>\n",
       "      <td id=\"T_7b26c_row0_col5\" class=\"data row0 col5\" >2</td>\n",
       "      <td id=\"T_7b26c_row0_col6\" class=\"data row0 col6\" >2</td>\n",
       "      <td id=\"T_7b26c_row0_col7\" class=\"data row0 col7\" >5</td>\n",
       "      <td id=\"T_7b26c_row0_col8\" class=\"data row0 col8\" >1.402e+04</td>\n",
       "      <td id=\"T_7b26c_row0_col9\" class=\"data row0 col9\" >1.402e+04</td>\n",
       "      <td id=\"T_7b26c_row0_col10\" class=\"data row0 col10\" >1.437e+04</td>\n",
       "      <td id=\"T_7b26c_row0_col11\" class=\"data row0 col11\" >1.491e+04</td>\n",
       "      <td id=\"T_7b26c_row0_col12\" class=\"data row0 col12\" >1488</td>\n",
       "      <td id=\"T_7b26c_row0_col13\" class=\"data row0 col13\" >1488</td>\n",
       "      <td id=\"T_7b26c_row0_col14\" class=\"data row0 col14\" >1077</td>\n",
       "      <td id=\"T_7b26c_row0_col15\" class=\"data row0 col15\" >1350</td>\n",
       "      <td id=\"T_7b26c_row0_col16\" class=\"data row0 col16\" >125.4</td>\n",
       "      <td id=\"T_7b26c_row0_col17\" class=\"data row0 col17\" >125.4</td>\n",
       "      <td id=\"T_7b26c_row0_col18\" class=\"data row0 col18\" >132.9</td>\n",
       "      <td id=\"T_7b26c_row0_col19\" class=\"data row0 col19\" >135.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b26c_level0_row1\" class=\"row_heading level0 row1\" >10</th>\n",
       "      <td id=\"T_7b26c_row1_col0\" class=\"data row1 col0\" >833.9</td>\n",
       "      <td id=\"T_7b26c_row1_col1\" class=\"data row1 col1\" >833.6</td>\n",
       "      <td id=\"T_7b26c_row1_col2\" class=\"data row1 col2\" >861.2</td>\n",
       "      <td id=\"T_7b26c_row1_col3\" class=\"data row1 col3\" >834.8</td>\n",
       "      <td id=\"T_7b26c_row1_col4\" class=\"data row1 col4\" >4</td>\n",
       "      <td id=\"T_7b26c_row1_col5\" class=\"data row1 col5\" >4</td>\n",
       "      <td id=\"T_7b26c_row1_col6\" class=\"data row1 col6\" >4</td>\n",
       "      <td id=\"T_7b26c_row1_col7\" class=\"data row1 col7\" >6</td>\n",
       "      <td id=\"T_7b26c_row1_col8\" class=\"data row1 col8\" >1992</td>\n",
       "      <td id=\"T_7b26c_row1_col9\" class=\"data row1 col9\" >1991</td>\n",
       "      <td id=\"T_7b26c_row1_col10\" class=\"data row1 col10\" >2019</td>\n",
       "      <td id=\"T_7b26c_row1_col11\" class=\"data row1 col11\" >1993</td>\n",
       "      <td id=\"T_7b26c_row1_col12\" class=\"data row1 col12\" >379</td>\n",
       "      <td id=\"T_7b26c_row1_col13\" class=\"data row1 col13\" >375</td>\n",
       "      <td id=\"T_7b26c_row1_col14\" class=\"data row1 col14\" >356.5</td>\n",
       "      <td id=\"T_7b26c_row1_col15\" class=\"data row1 col15\" >308.4</td>\n",
       "      <td id=\"T_7b26c_row1_col16\" class=\"data row1 col16\" >161.3</td>\n",
       "      <td id=\"T_7b26c_row1_col17\" class=\"data row1 col17\" >161.6</td>\n",
       "      <td id=\"T_7b26c_row1_col18\" class=\"data row1 col18\" >166.3</td>\n",
       "      <td id=\"T_7b26c_row1_col19\" class=\"data row1 col19\" >168.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b26c_level0_row2\" class=\"row_heading level0 row2\" >1</th>\n",
       "      <td id=\"T_7b26c_row2_col0\" class=\"data row2 col0\" >184.3</td>\n",
       "      <td id=\"T_7b26c_row2_col1\" class=\"data row2 col1\" >184.2</td>\n",
       "      <td id=\"T_7b26c_row2_col2\" class=\"data row2 col2\" >185.5</td>\n",
       "      <td id=\"T_7b26c_row2_col3\" class=\"data row2 col3\" >187</td>\n",
       "      <td id=\"T_7b26c_row2_col4\" class=\"data row2 col4\" >8</td>\n",
       "      <td id=\"T_7b26c_row2_col5\" class=\"data row2 col5\" >8</td>\n",
       "      <td id=\"T_7b26c_row2_col6\" class=\"data row2 col6\" >8</td>\n",
       "      <td id=\"T_7b26c_row2_col7\" class=\"data row2 col7\" >10</td>\n",
       "      <td id=\"T_7b26c_row2_col8\" class=\"data row2 col8\" >300.1</td>\n",
       "      <td id=\"T_7b26c_row2_col9\" class=\"data row2 col9\" >300</td>\n",
       "      <td id=\"T_7b26c_row2_col10\" class=\"data row2 col10\" >301.3</td>\n",
       "      <td id=\"T_7b26c_row2_col11\" class=\"data row2 col11\" >302.7</td>\n",
       "      <td id=\"T_7b26c_row2_col12\" class=\"data row2 col12\" >67.55</td>\n",
       "      <td id=\"T_7b26c_row2_col13\" class=\"data row2 col13\" >68.51</td>\n",
       "      <td id=\"T_7b26c_row2_col14\" class=\"data row2 col14\" >66.45</td>\n",
       "      <td id=\"T_7b26c_row2_col15\" class=\"data row2 col15\" >58.18</td>\n",
       "      <td id=\"T_7b26c_row2_col16\" class=\"data row2 col16\" >232.5</td>\n",
       "      <td id=\"T_7b26c_row2_col17\" class=\"data row2 col17\" >231.5</td>\n",
       "      <td id=\"T_7b26c_row2_col18\" class=\"data row2 col18\" >234.8</td>\n",
       "      <td id=\"T_7b26c_row2_col19\" class=\"data row2 col19\" >244.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b26c_level0_row3\" class=\"row_heading level0 row3\" >0.1</th>\n",
       "      <td id=\"T_7b26c_row3_col0\" class=\"data row3 col0\" >36.82</td>\n",
       "      <td id=\"T_7b26c_row3_col1\" class=\"data row3 col1\" >36.8</td>\n",
       "      <td id=\"T_7b26c_row3_col2\" class=\"data row3 col2\" >37.04</td>\n",
       "      <td id=\"T_7b26c_row3_col3\" class=\"data row3 col3\" >37.94</td>\n",
       "      <td id=\"T_7b26c_row3_col4\" class=\"data row3 col4\" >16</td>\n",
       "      <td id=\"T_7b26c_row3_col5\" class=\"data row3 col5\" >17</td>\n",
       "      <td id=\"T_7b26c_row3_col6\" class=\"data row3 col6\" >15</td>\n",
       "      <td id=\"T_7b26c_row3_col7\" class=\"data row3 col7\" >20</td>\n",
       "      <td id=\"T_7b26c_row3_col8\" class=\"data row3 col8\" >48.4</td>\n",
       "      <td id=\"T_7b26c_row3_col9\" class=\"data row3 col9\" >48.38</td>\n",
       "      <td id=\"T_7b26c_row3_col10\" class=\"data row3 col10\" >48.62</td>\n",
       "      <td id=\"T_7b26c_row3_col11\" class=\"data row3 col11\" >49.52</td>\n",
       "      <td id=\"T_7b26c_row3_col12\" class=\"data row3 col12\" >13.34</td>\n",
       "      <td id=\"T_7b26c_row3_col13\" class=\"data row3 col13\" >11.34</td>\n",
       "      <td id=\"T_7b26c_row3_col14\" class=\"data row3 col14\" >13.51</td>\n",
       "      <td id=\"T_7b26c_row3_col15\" class=\"data row3 col15\" >9.286</td>\n",
       "      <td id=\"T_7b26c_row3_col16\" class=\"data row3 col16\" >350.6</td>\n",
       "      <td id=\"T_7b26c_row3_col17\" class=\"data row3 col17\" >370.5</td>\n",
       "      <td id=\"T_7b26c_row3_col18\" class=\"data row3 col18\" >351.1</td>\n",
       "      <td id=\"T_7b26c_row3_col19\" class=\"data row3 col19\" >402.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7b26c_level0_row4\" class=\"row_heading level0 row4\" >0.01</th>\n",
       "      <td id=\"T_7b26c_row4_col0\" class=\"data row4 col0\" >6.853</td>\n",
       "      <td id=\"T_7b26c_row4_col1\" class=\"data row4 col1\" >6.853</td>\n",
       "      <td id=\"T_7b26c_row4_col2\" class=\"data row4 col2\" >7.061</td>\n",
       "      <td id=\"T_7b26c_row4_col3\" class=\"data row4 col3\" >8.346</td>\n",
       "      <td id=\"T_7b26c_row4_col4\" class=\"data row4 col4\" >37</td>\n",
       "      <td id=\"T_7b26c_row4_col5\" class=\"data row4 col5\" >37</td>\n",
       "      <td id=\"T_7b26c_row4_col6\" class=\"data row4 col6\" >37</td>\n",
       "      <td id=\"T_7b26c_row4_col7\" class=\"data row4 col7\" >30</td>\n",
       "      <td id=\"T_7b26c_row4_col8\" class=\"data row4 col8\" >8.011</td>\n",
       "      <td id=\"T_7b26c_row4_col9\" class=\"data row4 col9\" >8.011</td>\n",
       "      <td id=\"T_7b26c_row4_col10\" class=\"data row4 col10\" >8.219</td>\n",
       "      <td id=\"T_7b26c_row4_col11\" class=\"data row4 col11\" >9.504</td>\n",
       "      <td id=\"T_7b26c_row4_col12\" class=\"data row4 col12\" >1.564</td>\n",
       "      <td id=\"T_7b26c_row4_col13\" class=\"data row4 col13\" >1.564</td>\n",
       "      <td id=\"T_7b26c_row4_col14\" class=\"data row4 col14\" >1.228</td>\n",
       "      <td id=\"T_7b26c_row4_col15\" class=\"data row4 col15\" >3.651</td>\n",
       "      <td id=\"T_7b26c_row4_col16\" class=\"data row4 col16\" >644.7</td>\n",
       "      <td id=\"T_7b26c_row4_col17\" class=\"data row4 col17\" >644.7</td>\n",
       "      <td id=\"T_7b26c_row4_col18\" class=\"data row4 col18\" >699.1</td>\n",
       "      <td id=\"T_7b26c_row4_col19\" class=\"data row4 col19\" >585.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x27917e058e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Name: Barabasi-Albert, rel_alpha: 0.01, Graph with 200 nodes and 975 edges\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b3545\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b3545_level0_col0\" class=\"col_heading level0 col0\" colspan=\"4\">sub_opt_gap</th>\n",
       "      <th id=\"T_b3545_level0_col4\" class=\"col_heading level0 col4\" colspan=\"4\">num_clusters</th>\n",
       "      <th id=\"T_b3545_level0_col8\" class=\"col_heading level0 col8\" colspan=\"4\">total_loss</th>\n",
       "      <th id=\"T_b3545_level0_col12\" class=\"col_heading level0 col12\" colspan=\"4\">loss_U</th>\n",
       "      <th id=\"T_b3545_level0_col16\" class=\"col_heading level0 col16\" colspan=\"4\">loss_P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level1\" >algo</th>\n",
       "      <th id=\"T_b3545_level1_col0\" class=\"col_heading level1 col0\" >First-Improv.</th>\n",
       "      <th id=\"T_b3545_level1_col1\" class=\"col_heading level1 col1\" >Best-Improv.</th>\n",
       "      <th id=\"T_b3545_level1_col2\" class=\"col_heading level1 col2\" >Optimal 1D</th>\n",
       "      <th id=\"T_b3545_level1_col3\" class=\"col_heading level1 col3\" >Soft Assign</th>\n",
       "      <th id=\"T_b3545_level1_col4\" class=\"col_heading level1 col4\" >First-Improv.</th>\n",
       "      <th id=\"T_b3545_level1_col5\" class=\"col_heading level1 col5\" >Best-Improv.</th>\n",
       "      <th id=\"T_b3545_level1_col6\" class=\"col_heading level1 col6\" >Optimal 1D</th>\n",
       "      <th id=\"T_b3545_level1_col7\" class=\"col_heading level1 col7\" >Soft Assign</th>\n",
       "      <th id=\"T_b3545_level1_col8\" class=\"col_heading level1 col8\" >First-Improv.</th>\n",
       "      <th id=\"T_b3545_level1_col9\" class=\"col_heading level1 col9\" >Best-Improv.</th>\n",
       "      <th id=\"T_b3545_level1_col10\" class=\"col_heading level1 col10\" >Optimal 1D</th>\n",
       "      <th id=\"T_b3545_level1_col11\" class=\"col_heading level1 col11\" >Soft Assign</th>\n",
       "      <th id=\"T_b3545_level1_col12\" class=\"col_heading level1 col12\" >First-Improv.</th>\n",
       "      <th id=\"T_b3545_level1_col13\" class=\"col_heading level1 col13\" >Best-Improv.</th>\n",
       "      <th id=\"T_b3545_level1_col14\" class=\"col_heading level1 col14\" >Optimal 1D</th>\n",
       "      <th id=\"T_b3545_level1_col15\" class=\"col_heading level1 col15\" >Soft Assign</th>\n",
       "      <th id=\"T_b3545_level1_col16\" class=\"col_heading level1 col16\" >First-Improv.</th>\n",
       "      <th id=\"T_b3545_level1_col17\" class=\"col_heading level1 col17\" >Best-Improv.</th>\n",
       "      <th id=\"T_b3545_level1_col18\" class=\"col_heading level1 col18\" >Optimal 1D</th>\n",
       "      <th id=\"T_b3545_level1_col19\" class=\"col_heading level1 col19\" >Soft Assign</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >w</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "      <th class=\"blank col11\" >&nbsp;</th>\n",
       "      <th class=\"blank col12\" >&nbsp;</th>\n",
       "      <th class=\"blank col13\" >&nbsp;</th>\n",
       "      <th class=\"blank col14\" >&nbsp;</th>\n",
       "      <th class=\"blank col15\" >&nbsp;</th>\n",
       "      <th class=\"blank col16\" >&nbsp;</th>\n",
       "      <th class=\"blank col17\" >&nbsp;</th>\n",
       "      <th class=\"blank col18\" >&nbsp;</th>\n",
       "      <th class=\"blank col19\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b3545_level0_row0\" class=\"row_heading level0 row0\" >0.0001</th>\n",
       "      <td id=\"T_b3545_row0_col0\" class=\"data row0 col0\" >0.002804</td>\n",
       "      <td id=\"T_b3545_row0_col1\" class=\"data row0 col1\" >0.002804</td>\n",
       "      <td id=\"T_b3545_row0_col2\" class=\"data row0 col2\" >0.003193</td>\n",
       "      <td id=\"T_b3545_row0_col3\" class=\"data row0 col3\" >0.00286</td>\n",
       "      <td id=\"T_b3545_row0_col4\" class=\"data row0 col4\" >2</td>\n",
       "      <td id=\"T_b3545_row0_col5\" class=\"data row0 col5\" >2</td>\n",
       "      <td id=\"T_b3545_row0_col6\" class=\"data row0 col6\" >2</td>\n",
       "      <td id=\"T_b3545_row0_col7\" class=\"data row0 col7\" >3</td>\n",
       "      <td id=\"T_b3545_row0_col8\" class=\"data row0 col8\" >0.01438</td>\n",
       "      <td id=\"T_b3545_row0_col9\" class=\"data row0 col9\" >0.01438</td>\n",
       "      <td id=\"T_b3545_row0_col10\" class=\"data row0 col10\" >0.01477</td>\n",
       "      <td id=\"T_b3545_row0_col11\" class=\"data row0 col11\" >0.01444</td>\n",
       "      <td id=\"T_b3545_row0_col12\" class=\"data row0 col12\" >0.001562</td>\n",
       "      <td id=\"T_b3545_row0_col13\" class=\"data row0 col13\" >0.001562</td>\n",
       "      <td id=\"T_b3545_row0_col14\" class=\"data row0 col14\" >0.001369</td>\n",
       "      <td id=\"T_b3545_row0_col15\" class=\"data row0 col15\" >0.001274</td>\n",
       "      <td id=\"T_b3545_row0_col16\" class=\"data row0 col16\" >128.2</td>\n",
       "      <td id=\"T_b3545_row0_col17\" class=\"data row0 col17\" >128.2</td>\n",
       "      <td id=\"T_b3545_row0_col18\" class=\"data row0 col18\" >134</td>\n",
       "      <td id=\"T_b3545_row0_col19\" class=\"data row0 col19\" >131.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3545_level0_row1\" class=\"row_heading level0 row1\" >1e-05</th>\n",
       "      <td id=\"T_b3545_row1_col0\" class=\"data row1 col0\" >0.0008716</td>\n",
       "      <td id=\"T_b3545_row1_col1\" class=\"data row1 col1\" >0.0008716</td>\n",
       "      <td id=\"T_b3545_row1_col2\" class=\"data row1 col2\" >0.0008899</td>\n",
       "      <td id=\"T_b3545_row1_col3\" class=\"data row1 col3\" >0.0009784</td>\n",
       "      <td id=\"T_b3545_row1_col4\" class=\"data row1 col4\" >4</td>\n",
       "      <td id=\"T_b3545_row1_col5\" class=\"data row1 col5\" >4</td>\n",
       "      <td id=\"T_b3545_row1_col6\" class=\"data row1 col6\" >4</td>\n",
       "      <td id=\"T_b3545_row1_col7\" class=\"data row1 col7\" >7</td>\n",
       "      <td id=\"T_b3545_row1_col8\" class=\"data row1 col8\" >0.002029</td>\n",
       "      <td id=\"T_b3545_row1_col9\" class=\"data row1 col9\" >0.002029</td>\n",
       "      <td id=\"T_b3545_row1_col10\" class=\"data row1 col10\" >0.002048</td>\n",
       "      <td id=\"T_b3545_row1_col11\" class=\"data row1 col11\" >0.002136</td>\n",
       "      <td id=\"T_b3545_row1_col12\" class=\"data row1 col12\" >0.0004097</td>\n",
       "      <td id=\"T_b3545_row1_col13\" class=\"data row1 col13\" >0.0004097</td>\n",
       "      <td id=\"T_b3545_row1_col14\" class=\"data row1 col14\" >0.0003834</td>\n",
       "      <td id=\"T_b3545_row1_col15\" class=\"data row1 col15\" >0.0003459</td>\n",
       "      <td id=\"T_b3545_row1_col16\" class=\"data row1 col16\" >162</td>\n",
       "      <td id=\"T_b3545_row1_col17\" class=\"data row1 col17\" >162</td>\n",
       "      <td id=\"T_b3545_row1_col18\" class=\"data row1 col18\" >166.4</td>\n",
       "      <td id=\"T_b3545_row1_col19\" class=\"data row1 col19\" >179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3545_level0_row2\" class=\"row_heading level0 row2\" >1e-06</th>\n",
       "      <td id=\"T_b3545_row2_col0\" class=\"data row2 col0\" >0.0001933</td>\n",
       "      <td id=\"T_b3545_row2_col1\" class=\"data row2 col1\" >0.0001933</td>\n",
       "      <td id=\"T_b3545_row2_col2\" class=\"data row2 col2\" >0.0001968</td>\n",
       "      <td id=\"T_b3545_row2_col3\" class=\"data row2 col3\" >0.0002201</td>\n",
       "      <td id=\"T_b3545_row2_col4\" class=\"data row2 col4\" >10</td>\n",
       "      <td id=\"T_b3545_row2_col5\" class=\"data row2 col5\" >10</td>\n",
       "      <td id=\"T_b3545_row2_col6\" class=\"data row2 col6\" >9</td>\n",
       "      <td id=\"T_b3545_row2_col7\" class=\"data row2 col7\" >14</td>\n",
       "      <td id=\"T_b3545_row2_col8\" class=\"data row2 col8\" >0.0003091</td>\n",
       "      <td id=\"T_b3545_row2_col9\" class=\"data row2 col9\" >0.0003091</td>\n",
       "      <td id=\"T_b3545_row2_col10\" class=\"data row2 col10\" >0.0003126</td>\n",
       "      <td id=\"T_b3545_row2_col11\" class=\"data row2 col11\" >0.0003359</td>\n",
       "      <td id=\"T_b3545_row2_col12\" class=\"data row2 col12\" >4.008e-05</td>\n",
       "      <td id=\"T_b3545_row2_col13\" class=\"data row2 col13\" >4.008e-05</td>\n",
       "      <td id=\"T_b3545_row2_col14\" class=\"data row2 col14\" >4.398e-05</td>\n",
       "      <td id=\"T_b3545_row2_col15\" class=\"data row2 col15\" >4.115e-05</td>\n",
       "      <td id=\"T_b3545_row2_col16\" class=\"data row2 col16\" >269</td>\n",
       "      <td id=\"T_b3545_row2_col17\" class=\"data row2 col17\" >269</td>\n",
       "      <td id=\"T_b3545_row2_col18\" class=\"data row2 col18\" >268.6</td>\n",
       "      <td id=\"T_b3545_row2_col19\" class=\"data row2 col19\" >294.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3545_level0_row3\" class=\"row_heading level0 row3\" >1e-07</th>\n",
       "      <td id=\"T_b3545_row3_col0\" class=\"data row3 col0\" >3.385e-05</td>\n",
       "      <td id=\"T_b3545_row3_col1\" class=\"data row3 col1\" >3.385e-05</td>\n",
       "      <td id=\"T_b3545_row3_col2\" class=\"data row3 col2\" >3.385e-05</td>\n",
       "      <td id=\"T_b3545_row3_col3\" class=\"data row3 col3\" >5.948e-05</td>\n",
       "      <td id=\"T_b3545_row3_col4\" class=\"data row3 col4\" >15</td>\n",
       "      <td id=\"T_b3545_row3_col5\" class=\"data row3 col5\" >15</td>\n",
       "      <td id=\"T_b3545_row3_col6\" class=\"data row3 col6\" >15</td>\n",
       "      <td id=\"T_b3545_row3_col7\" class=\"data row3 col7\" >40</td>\n",
       "      <td id=\"T_b3545_row3_col8\" class=\"data row3 col8\" >4.543e-05</td>\n",
       "      <td id=\"T_b3545_row3_col9\" class=\"data row3 col9\" >4.543e-05</td>\n",
       "      <td id=\"T_b3545_row3_col10\" class=\"data row3 col10\" >4.543e-05</td>\n",
       "      <td id=\"T_b3545_row3_col11\" class=\"data row3 col11\" >7.106e-05</td>\n",
       "      <td id=\"T_b3545_row3_col12\" class=\"data row3 col12\" >9.382e-06</td>\n",
       "      <td id=\"T_b3545_row3_col13\" class=\"data row3 col13\" >9.382e-06</td>\n",
       "      <td id=\"T_b3545_row3_col14\" class=\"data row3 col14\" >9.382e-06</td>\n",
       "      <td id=\"T_b3545_row3_col15\" class=\"data row3 col15\" >1.023e-05</td>\n",
       "      <td id=\"T_b3545_row3_col16\" class=\"data row3 col16\" >360.5</td>\n",
       "      <td id=\"T_b3545_row3_col17\" class=\"data row3 col17\" >360.5</td>\n",
       "      <td id=\"T_b3545_row3_col18\" class=\"data row3 col18\" >360.5</td>\n",
       "      <td id=\"T_b3545_row3_col19\" class=\"data row3 col19\" >608.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b3545_level0_row4\" class=\"row_heading level0 row4\" >1e-08</th>\n",
       "      <td id=\"T_b3545_row4_col0\" class=\"data row4 col0\" >4.8e-06</td>\n",
       "      <td id=\"T_b3545_row4_col1\" class=\"data row4 col1\" >4.8e-06</td>\n",
       "      <td id=\"T_b3545_row4_col2\" class=\"data row4 col2\" >4.8e-06</td>\n",
       "      <td id=\"T_b3545_row4_col3\" class=\"data row4 col3\" >1.679e-05</td>\n",
       "      <td id=\"T_b3545_row4_col4\" class=\"data row4 col4\" >29</td>\n",
       "      <td id=\"T_b3545_row4_col5\" class=\"data row4 col5\" >29</td>\n",
       "      <td id=\"T_b3545_row4_col6\" class=\"data row4 col6\" >29</td>\n",
       "      <td id=\"T_b3545_row4_col7\" class=\"data row4 col7\" >47</td>\n",
       "      <td id=\"T_b3545_row4_col8\" class=\"data row4 col8\" >5.957e-06</td>\n",
       "      <td id=\"T_b3545_row4_col9\" class=\"data row4 col9\" >5.957e-06</td>\n",
       "      <td id=\"T_b3545_row4_col10\" class=\"data row4 col10\" >5.957e-06</td>\n",
       "      <td id=\"T_b3545_row4_col11\" class=\"data row4 col11\" >1.794e-05</td>\n",
       "      <td id=\"T_b3545_row4_col12\" class=\"data row4 col12\" >3.063e-08</td>\n",
       "      <td id=\"T_b3545_row4_col13\" class=\"data row4 col13\" >3.063e-08</td>\n",
       "      <td id=\"T_b3545_row4_col14\" class=\"data row4 col14\" >3.063e-08</td>\n",
       "      <td id=\"T_b3545_row4_col15\" class=\"data row4 col15\" >1.049e-05</td>\n",
       "      <td id=\"T_b3545_row4_col16\" class=\"data row4 col16\" >592.7</td>\n",
       "      <td id=\"T_b3545_row4_col17\" class=\"data row4 col17\" >592.7</td>\n",
       "      <td id=\"T_b3545_row4_col18\" class=\"data row4 col18\" >592.7</td>\n",
       "      <td id=\"T_b3545_row4_col19\" class=\"data row4 col19\" >745.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x27918153890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Name: LFR, rel_alpha: 0.9, Graph with 200 nodes and 634 edges\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_99e43\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_99e43_level0_col0\" class=\"col_heading level0 col0\" colspan=\"4\">sub_opt_gap</th>\n",
       "      <th id=\"T_99e43_level0_col4\" class=\"col_heading level0 col4\" colspan=\"4\">num_clusters</th>\n",
       "      <th id=\"T_99e43_level0_col8\" class=\"col_heading level0 col8\" colspan=\"4\">total_loss</th>\n",
       "      <th id=\"T_99e43_level0_col12\" class=\"col_heading level0 col12\" colspan=\"4\">loss_U</th>\n",
       "      <th id=\"T_99e43_level0_col16\" class=\"col_heading level0 col16\" colspan=\"4\">loss_P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level1\" >algo</th>\n",
       "      <th id=\"T_99e43_level1_col0\" class=\"col_heading level1 col0\" >First-Improv.</th>\n",
       "      <th id=\"T_99e43_level1_col1\" class=\"col_heading level1 col1\" >Best-Improv.</th>\n",
       "      <th id=\"T_99e43_level1_col2\" class=\"col_heading level1 col2\" >Optimal 1D</th>\n",
       "      <th id=\"T_99e43_level1_col3\" class=\"col_heading level1 col3\" >Soft Assign</th>\n",
       "      <th id=\"T_99e43_level1_col4\" class=\"col_heading level1 col4\" >First-Improv.</th>\n",
       "      <th id=\"T_99e43_level1_col5\" class=\"col_heading level1 col5\" >Best-Improv.</th>\n",
       "      <th id=\"T_99e43_level1_col6\" class=\"col_heading level1 col6\" >Optimal 1D</th>\n",
       "      <th id=\"T_99e43_level1_col7\" class=\"col_heading level1 col7\" >Soft Assign</th>\n",
       "      <th id=\"T_99e43_level1_col8\" class=\"col_heading level1 col8\" >First-Improv.</th>\n",
       "      <th id=\"T_99e43_level1_col9\" class=\"col_heading level1 col9\" >Best-Improv.</th>\n",
       "      <th id=\"T_99e43_level1_col10\" class=\"col_heading level1 col10\" >Optimal 1D</th>\n",
       "      <th id=\"T_99e43_level1_col11\" class=\"col_heading level1 col11\" >Soft Assign</th>\n",
       "      <th id=\"T_99e43_level1_col12\" class=\"col_heading level1 col12\" >First-Improv.</th>\n",
       "      <th id=\"T_99e43_level1_col13\" class=\"col_heading level1 col13\" >Best-Improv.</th>\n",
       "      <th id=\"T_99e43_level1_col14\" class=\"col_heading level1 col14\" >Optimal 1D</th>\n",
       "      <th id=\"T_99e43_level1_col15\" class=\"col_heading level1 col15\" >Soft Assign</th>\n",
       "      <th id=\"T_99e43_level1_col16\" class=\"col_heading level1 col16\" >First-Improv.</th>\n",
       "      <th id=\"T_99e43_level1_col17\" class=\"col_heading level1 col17\" >Best-Improv.</th>\n",
       "      <th id=\"T_99e43_level1_col18\" class=\"col_heading level1 col18\" >Optimal 1D</th>\n",
       "      <th id=\"T_99e43_level1_col19\" class=\"col_heading level1 col19\" >Soft Assign</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >w</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "      <th class=\"blank col11\" >&nbsp;</th>\n",
       "      <th class=\"blank col12\" >&nbsp;</th>\n",
       "      <th class=\"blank col13\" >&nbsp;</th>\n",
       "      <th class=\"blank col14\" >&nbsp;</th>\n",
       "      <th class=\"blank col15\" >&nbsp;</th>\n",
       "      <th class=\"blank col16\" >&nbsp;</th>\n",
       "      <th class=\"blank col17\" >&nbsp;</th>\n",
       "      <th class=\"blank col18\" >&nbsp;</th>\n",
       "      <th class=\"blank col19\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_99e43_level0_row0\" class=\"row_heading level0 row0\" >10</th>\n",
       "      <td id=\"T_99e43_row0_col0\" class=\"data row0 col0\" >1006</td>\n",
       "      <td id=\"T_99e43_row0_col1\" class=\"data row0 col1\" >982.2</td>\n",
       "      <td id=\"T_99e43_row0_col2\" class=\"data row0 col2\" >1044</td>\n",
       "      <td id=\"T_99e43_row0_col3\" class=\"data row0 col3\" >1039</td>\n",
       "      <td id=\"T_99e43_row0_col4\" class=\"data row0 col4\" >4</td>\n",
       "      <td id=\"T_99e43_row0_col5\" class=\"data row0 col5\" >4</td>\n",
       "      <td id=\"T_99e43_row0_col6\" class=\"data row0 col6\" >4</td>\n",
       "      <td id=\"T_99e43_row0_col7\" class=\"data row0 col7\" >6</td>\n",
       "      <td id=\"T_99e43_row0_col8\" class=\"data row0 col8\" >1505</td>\n",
       "      <td id=\"T_99e43_row0_col9\" class=\"data row0 col9\" >1481</td>\n",
       "      <td id=\"T_99e43_row0_col10\" class=\"data row0 col10\" >1543</td>\n",
       "      <td id=\"T_99e43_row0_col11\" class=\"data row0 col11\" >1538</td>\n",
       "      <td id=\"T_99e43_row0_col12\" class=\"data row0 col12\" >244.4</td>\n",
       "      <td id=\"T_99e43_row0_col13\" class=\"data row0 col13\" >270.3</td>\n",
       "      <td id=\"T_99e43_row0_col14\" class=\"data row0 col14\" >202</td>\n",
       "      <td id=\"T_99e43_row0_col15\" class=\"data row0 col15\" >213.6</td>\n",
       "      <td id=\"T_99e43_row0_col16\" class=\"data row0 col16\" >126</td>\n",
       "      <td id=\"T_99e43_row0_col17\" class=\"data row0 col17\" >121.1</td>\n",
       "      <td id=\"T_99e43_row0_col18\" class=\"data row0 col18\" >134.1</td>\n",
       "      <td id=\"T_99e43_row0_col19\" class=\"data row0 col19\" >132.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_99e43_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_99e43_row1_col0\" class=\"data row1 col0\" >165.9</td>\n",
       "      <td id=\"T_99e43_row1_col1\" class=\"data row1 col1\" >165.9</td>\n",
       "      <td id=\"T_99e43_row1_col2\" class=\"data row1 col2\" >171.4</td>\n",
       "      <td id=\"T_99e43_row1_col3\" class=\"data row1 col3\" >178.2</td>\n",
       "      <td id=\"T_99e43_row1_col4\" class=\"data row1 col4\" >9</td>\n",
       "      <td id=\"T_99e43_row1_col5\" class=\"data row1 col5\" >9</td>\n",
       "      <td id=\"T_99e43_row1_col6\" class=\"data row1 col6\" >9</td>\n",
       "      <td id=\"T_99e43_row1_col7\" class=\"data row1 col7\" >13</td>\n",
       "      <td id=\"T_99e43_row1_col8\" class=\"data row1 col8\" >215.8</td>\n",
       "      <td id=\"T_99e43_row1_col9\" class=\"data row1 col9\" >215.8</td>\n",
       "      <td id=\"T_99e43_row1_col10\" class=\"data row1 col10\" >221.3</td>\n",
       "      <td id=\"T_99e43_row1_col11\" class=\"data row1 col11\" >228.1</td>\n",
       "      <td id=\"T_99e43_row1_col12\" class=\"data row1 col12\" >33.27</td>\n",
       "      <td id=\"T_99e43_row1_col13\" class=\"data row1 col13\" >33.27</td>\n",
       "      <td id=\"T_99e43_row1_col14\" class=\"data row1 col14\" >29.74</td>\n",
       "      <td id=\"T_99e43_row1_col15\" class=\"data row1 col15\" >29.45</td>\n",
       "      <td id=\"T_99e43_row1_col16\" class=\"data row1 col16\" >182.6</td>\n",
       "      <td id=\"T_99e43_row1_col17\" class=\"data row1 col17\" >182.6</td>\n",
       "      <td id=\"T_99e43_row1_col18\" class=\"data row1 col18\" >191.5</td>\n",
       "      <td id=\"T_99e43_row1_col19\" class=\"data row1 col19\" >198.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_99e43_level0_row2\" class=\"row_heading level0 row2\" >0.1</th>\n",
       "      <td id=\"T_99e43_row2_col0\" class=\"data row2 col0\" >27.53</td>\n",
       "      <td id=\"T_99e43_row2_col1\" class=\"data row2 col1\" >27.53</td>\n",
       "      <td id=\"T_99e43_row2_col2\" class=\"data row2 col2\" >28.13</td>\n",
       "      <td id=\"T_99e43_row2_col3\" class=\"data row2 col3\" >28.35</td>\n",
       "      <td id=\"T_99e43_row2_col4\" class=\"data row2 col4\" >16</td>\n",
       "      <td id=\"T_99e43_row2_col5\" class=\"data row2 col5\" >16</td>\n",
       "      <td id=\"T_99e43_row2_col6\" class=\"data row2 col6\" >17</td>\n",
       "      <td id=\"T_99e43_row2_col7\" class=\"data row2 col7\" >20</td>\n",
       "      <td id=\"T_99e43_row2_col8\" class=\"data row2 col8\" >32.52</td>\n",
       "      <td id=\"T_99e43_row2_col9\" class=\"data row2 col9\" >32.52</td>\n",
       "      <td id=\"T_99e43_row2_col10\" class=\"data row2 col10\" >33.12</td>\n",
       "      <td id=\"T_99e43_row2_col11\" class=\"data row2 col11\" >33.34</td>\n",
       "      <td id=\"T_99e43_row2_col12\" class=\"data row2 col12\" >6.587</td>\n",
       "      <td id=\"T_99e43_row2_col13\" class=\"data row2 col13\" >6.587</td>\n",
       "      <td id=\"T_99e43_row2_col14\" class=\"data row2 col14\" >5.186</td>\n",
       "      <td id=\"T_99e43_row2_col15\" class=\"data row2 col15\" >5.32</td>\n",
       "      <td id=\"T_99e43_row2_col16\" class=\"data row2 col16\" >259.3</td>\n",
       "      <td id=\"T_99e43_row2_col17\" class=\"data row2 col17\" >259.3</td>\n",
       "      <td id=\"T_99e43_row2_col18\" class=\"data row2 col18\" >279.3</td>\n",
       "      <td id=\"T_99e43_row2_col19\" class=\"data row2 col19\" >280.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_99e43_level0_row3\" class=\"row_heading level0 row3\" >0.01</th>\n",
       "      <td id=\"T_99e43_row3_col0\" class=\"data row3 col0\" >4.463</td>\n",
       "      <td id=\"T_99e43_row3_col1\" class=\"data row3 col1\" >4.462</td>\n",
       "      <td id=\"T_99e43_row3_col2\" class=\"data row3 col2\" >4.757</td>\n",
       "      <td id=\"T_99e43_row3_col3\" class=\"data row3 col3\" >5.785</td>\n",
       "      <td id=\"T_99e43_row3_col4\" class=\"data row3 col4\" >49</td>\n",
       "      <td id=\"T_99e43_row3_col5\" class=\"data row3 col5\" >50</td>\n",
       "      <td id=\"T_99e43_row3_col6\" class=\"data row3 col6\" >34</td>\n",
       "      <td id=\"T_99e43_row3_col7\" class=\"data row3 col7\" >28</td>\n",
       "      <td id=\"T_99e43_row3_col8\" class=\"data row3 col8\" >4.962</td>\n",
       "      <td id=\"T_99e43_row3_col9\" class=\"data row3 col9\" >4.961</td>\n",
       "      <td id=\"T_99e43_row3_col10\" class=\"data row3 col10\" >5.256</td>\n",
       "      <td id=\"T_99e43_row3_col11\" class=\"data row3 col11\" >6.284</td>\n",
       "      <td id=\"T_99e43_row3_col12\" class=\"data row3 col12\" >0.6988</td>\n",
       "      <td id=\"T_99e43_row3_col13\" class=\"data row3 col13\" >0.698</td>\n",
       "      <td id=\"T_99e43_row3_col14\" class=\"data row3 col14\" >0.9118</td>\n",
       "      <td id=\"T_99e43_row3_col15\" class=\"data row3 col15\" >2.617</td>\n",
       "      <td id=\"T_99e43_row3_col16\" class=\"data row3 col16\" >426.3</td>\n",
       "      <td id=\"T_99e43_row3_col17\" class=\"data row3 col17\" >426.3</td>\n",
       "      <td id=\"T_99e43_row3_col18\" class=\"data row3 col18\" >434.4</td>\n",
       "      <td id=\"T_99e43_row3_col19\" class=\"data row3 col19\" >366.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x27913c8c230>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Name: LFR, rel_alpha: 0.01, Graph with 200 nodes and 634 edges\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1de70\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_1de70_level0_col0\" class=\"col_heading level0 col0\" colspan=\"4\">sub_opt_gap</th>\n",
       "      <th id=\"T_1de70_level0_col4\" class=\"col_heading level0 col4\" colspan=\"4\">num_clusters</th>\n",
       "      <th id=\"T_1de70_level0_col8\" class=\"col_heading level0 col8\" colspan=\"4\">total_loss</th>\n",
       "      <th id=\"T_1de70_level0_col12\" class=\"col_heading level0 col12\" colspan=\"4\">loss_U</th>\n",
       "      <th id=\"T_1de70_level0_col16\" class=\"col_heading level0 col16\" colspan=\"4\">loss_P</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level1\" >algo</th>\n",
       "      <th id=\"T_1de70_level1_col0\" class=\"col_heading level1 col0\" >First-Improv.</th>\n",
       "      <th id=\"T_1de70_level1_col1\" class=\"col_heading level1 col1\" >Best-Improv.</th>\n",
       "      <th id=\"T_1de70_level1_col2\" class=\"col_heading level1 col2\" >Optimal 1D</th>\n",
       "      <th id=\"T_1de70_level1_col3\" class=\"col_heading level1 col3\" >Soft Assign</th>\n",
       "      <th id=\"T_1de70_level1_col4\" class=\"col_heading level1 col4\" >First-Improv.</th>\n",
       "      <th id=\"T_1de70_level1_col5\" class=\"col_heading level1 col5\" >Best-Improv.</th>\n",
       "      <th id=\"T_1de70_level1_col6\" class=\"col_heading level1 col6\" >Optimal 1D</th>\n",
       "      <th id=\"T_1de70_level1_col7\" class=\"col_heading level1 col7\" >Soft Assign</th>\n",
       "      <th id=\"T_1de70_level1_col8\" class=\"col_heading level1 col8\" >First-Improv.</th>\n",
       "      <th id=\"T_1de70_level1_col9\" class=\"col_heading level1 col9\" >Best-Improv.</th>\n",
       "      <th id=\"T_1de70_level1_col10\" class=\"col_heading level1 col10\" >Optimal 1D</th>\n",
       "      <th id=\"T_1de70_level1_col11\" class=\"col_heading level1 col11\" >Soft Assign</th>\n",
       "      <th id=\"T_1de70_level1_col12\" class=\"col_heading level1 col12\" >First-Improv.</th>\n",
       "      <th id=\"T_1de70_level1_col13\" class=\"col_heading level1 col13\" >Best-Improv.</th>\n",
       "      <th id=\"T_1de70_level1_col14\" class=\"col_heading level1 col14\" >Optimal 1D</th>\n",
       "      <th id=\"T_1de70_level1_col15\" class=\"col_heading level1 col15\" >Soft Assign</th>\n",
       "      <th id=\"T_1de70_level1_col16\" class=\"col_heading level1 col16\" >First-Improv.</th>\n",
       "      <th id=\"T_1de70_level1_col17\" class=\"col_heading level1 col17\" >Best-Improv.</th>\n",
       "      <th id=\"T_1de70_level1_col18\" class=\"col_heading level1 col18\" >Optimal 1D</th>\n",
       "      <th id=\"T_1de70_level1_col19\" class=\"col_heading level1 col19\" >Soft Assign</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >w</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "      <th class=\"blank col7\" >&nbsp;</th>\n",
       "      <th class=\"blank col8\" >&nbsp;</th>\n",
       "      <th class=\"blank col9\" >&nbsp;</th>\n",
       "      <th class=\"blank col10\" >&nbsp;</th>\n",
       "      <th class=\"blank col11\" >&nbsp;</th>\n",
       "      <th class=\"blank col12\" >&nbsp;</th>\n",
       "      <th class=\"blank col13\" >&nbsp;</th>\n",
       "      <th class=\"blank col14\" >&nbsp;</th>\n",
       "      <th class=\"blank col15\" >&nbsp;</th>\n",
       "      <th class=\"blank col16\" >&nbsp;</th>\n",
       "      <th class=\"blank col17\" >&nbsp;</th>\n",
       "      <th class=\"blank col18\" >&nbsp;</th>\n",
       "      <th class=\"blank col19\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1de70_level0_row0\" class=\"row_heading level0 row0\" >0.0001</th>\n",
       "      <td id=\"T_1de70_row0_col0\" class=\"data row0 col0\" >0.004383</td>\n",
       "      <td id=\"T_1de70_row0_col1\" class=\"data row0 col1\" >0.004387</td>\n",
       "      <td id=\"T_1de70_row0_col2\" class=\"data row0 col2\" >0.004633</td>\n",
       "      <td id=\"T_1de70_row0_col3\" class=\"data row0 col3\" >0.005195</td>\n",
       "      <td id=\"T_1de70_row0_col4\" class=\"data row0 col4\" >3</td>\n",
       "      <td id=\"T_1de70_row0_col5\" class=\"data row0 col5\" >3</td>\n",
       "      <td id=\"T_1de70_row0_col6\" class=\"data row0 col6\" >2</td>\n",
       "      <td id=\"T_1de70_row0_col7\" class=\"data row0 col7\" >4</td>\n",
       "      <td id=\"T_1de70_row0_col8\" class=\"data row0 col8\" >0.009372</td>\n",
       "      <td id=\"T_1de70_row0_col9\" class=\"data row0 col9\" >0.009376</td>\n",
       "      <td id=\"T_1de70_row0_col10\" class=\"data row0 col10\" >0.009622</td>\n",
       "      <td id=\"T_1de70_row0_col11\" class=\"data row0 col11\" >0.01018</td>\n",
       "      <td id=\"T_1de70_row0_col12\" class=\"data row0 col12\" >0.001046</td>\n",
       "      <td id=\"T_1de70_row0_col13\" class=\"data row0 col13\" >0.001051</td>\n",
       "      <td id=\"T_1de70_row0_col14\" class=\"data row0 col14\" >0.001579</td>\n",
       "      <td id=\"T_1de70_row0_col15\" class=\"data row0 col15\" >0.003778</td>\n",
       "      <td id=\"T_1de70_row0_col16\" class=\"data row0 col16\" >83.26</td>\n",
       "      <td id=\"T_1de70_row0_col17\" class=\"data row0 col17\" >83.26</td>\n",
       "      <td id=\"T_1de70_row0_col18\" class=\"data row0 col18\" >80.43</td>\n",
       "      <td id=\"T_1de70_row0_col19\" class=\"data row0 col19\" >64.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1de70_level0_row1\" class=\"row_heading level0 row1\" >5.62e-06</th>\n",
       "      <td id=\"T_1de70_row1_col0\" class=\"data row1 col0\" >0.0006939</td>\n",
       "      <td id=\"T_1de70_row1_col1\" class=\"data row1 col1\" >0.0006927</td>\n",
       "      <td id=\"T_1de70_row1_col2\" class=\"data row1 col2\" >0.0007083</td>\n",
       "      <td id=\"T_1de70_row1_col3\" class=\"data row1 col3\" >0.0007335</td>\n",
       "      <td id=\"T_1de70_row1_col4\" class=\"data row1 col4\" >7</td>\n",
       "      <td id=\"T_1de70_row1_col5\" class=\"data row1 col5\" >7</td>\n",
       "      <td id=\"T_1de70_row1_col6\" class=\"data row1 col6\" >6</td>\n",
       "      <td id=\"T_1de70_row1_col7\" class=\"data row1 col7\" >9</td>\n",
       "      <td id=\"T_1de70_row1_col8\" class=\"data row1 col8\" >0.0009744</td>\n",
       "      <td id=\"T_1de70_row1_col9\" class=\"data row1 col9\" >0.0009733</td>\n",
       "      <td id=\"T_1de70_row1_col10\" class=\"data row1 col10\" >0.0009889</td>\n",
       "      <td id=\"T_1de70_row1_col11\" class=\"data row1 col11\" >0.001014</td>\n",
       "      <td id=\"T_1de70_row1_col12\" class=\"data row1 col12\" >0.0001084</td>\n",
       "      <td id=\"T_1de70_row1_col13\" class=\"data row1 col13\" >0.0001046</td>\n",
       "      <td id=\"T_1de70_row1_col14\" class=\"data row1 col14\" >0.0001309</td>\n",
       "      <td id=\"T_1de70_row1_col15\" class=\"data row1 col15\" >0.0001099</td>\n",
       "      <td id=\"T_1de70_row1_col16\" class=\"data row1 col16\" >154</td>\n",
       "      <td id=\"T_1de70_row1_col17\" class=\"data row1 col17\" >154.5</td>\n",
       "      <td id=\"T_1de70_row1_col18\" class=\"data row1 col18\" >152.6</td>\n",
       "      <td id=\"T_1de70_row1_col19\" class=\"data row1 col19\" >160.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1de70_level0_row2\" class=\"row_heading level0 row2\" >3.16e-07</th>\n",
       "      <td id=\"T_1de70_row2_col0\" class=\"data row2 col0\" >6.295e-05</td>\n",
       "      <td id=\"T_1de70_row2_col1\" class=\"data row2 col1\" >6.295e-05</td>\n",
       "      <td id=\"T_1de70_row2_col2\" class=\"data row2 col2\" >6.31e-05</td>\n",
       "      <td id=\"T_1de70_row2_col3\" class=\"data row2 col3\" >9.865e-05</td>\n",
       "      <td id=\"T_1de70_row2_col4\" class=\"data row2 col4\" >13</td>\n",
       "      <td id=\"T_1de70_row2_col5\" class=\"data row2 col5\" >13</td>\n",
       "      <td id=\"T_1de70_row2_col6\" class=\"data row2 col6\" >12</td>\n",
       "      <td id=\"T_1de70_row2_col7\" class=\"data row2 col7\" >24</td>\n",
       "      <td id=\"T_1de70_row2_col8\" class=\"data row2 col8\" >7.873e-05</td>\n",
       "      <td id=\"T_1de70_row2_col9\" class=\"data row2 col9\" >7.873e-05</td>\n",
       "      <td id=\"T_1de70_row2_col10\" class=\"data row2 col10\" >7.888e-05</td>\n",
       "      <td id=\"T_1de70_row2_col11\" class=\"data row2 col11\" >0.0001144</td>\n",
       "      <td id=\"T_1de70_row2_col12\" class=\"data row2 col12\" >8.274e-06</td>\n",
       "      <td id=\"T_1de70_row2_col13\" class=\"data row2 col13\" >8.274e-06</td>\n",
       "      <td id=\"T_1de70_row2_col14\" class=\"data row2 col14\" >9.083e-06</td>\n",
       "      <td id=\"T_1de70_row2_col15\" class=\"data row2 col15\" >1.772e-05</td>\n",
       "      <td id=\"T_1de70_row2_col16\" class=\"data row2 col16\" >222.8</td>\n",
       "      <td id=\"T_1de70_row2_col17\" class=\"data row2 col17\" >222.8</td>\n",
       "      <td id=\"T_1de70_row2_col18\" class=\"data row2 col18\" >220.7</td>\n",
       "      <td id=\"T_1de70_row2_col19\" class=\"data row2 col19\" >305.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1de70_level0_row3\" class=\"row_heading level0 row3\" >1.78e-08</th>\n",
       "      <td id=\"T_1de70_row3_col0\" class=\"data row3 col0\" >4.186e-06</td>\n",
       "      <td id=\"T_1de70_row3_col1\" class=\"data row3 col1\" >4.186e-06</td>\n",
       "      <td id=\"T_1de70_row3_col2\" class=\"data row3 col2\" >4.186e-06</td>\n",
       "      <td id=\"T_1de70_row3_col3\" class=\"data row3 col3\" >2.183e-05</td>\n",
       "      <td id=\"T_1de70_row3_col4\" class=\"data row3 col4\" >18</td>\n",
       "      <td id=\"T_1de70_row3_col5\" class=\"data row3 col5\" >18</td>\n",
       "      <td id=\"T_1de70_row3_col6\" class=\"data row3 col6\" >18</td>\n",
       "      <td id=\"T_1de70_row3_col7\" class=\"data row3 col7\" >49</td>\n",
       "      <td id=\"T_1de70_row3_col8\" class=\"data row3 col8\" >5.073e-06</td>\n",
       "      <td id=\"T_1de70_row3_col9\" class=\"data row3 col9\" >5.073e-06</td>\n",
       "      <td id=\"T_1de70_row3_col10\" class=\"data row3 col10\" >5.073e-06</td>\n",
       "      <td id=\"T_1de70_row3_col11\" class=\"data row3 col11\" >2.272e-05</td>\n",
       "      <td id=\"T_1de70_row3_col12\" class=\"data row3 col12\" >1.106e-07</td>\n",
       "      <td id=\"T_1de70_row3_col13\" class=\"data row3 col13\" >1.106e-07</td>\n",
       "      <td id=\"T_1de70_row3_col14\" class=\"data row3 col14\" >1.106e-07</td>\n",
       "      <td id=\"T_1de70_row3_col15\" class=\"data row3 col15\" >1.334e-05</td>\n",
       "      <td id=\"T_1de70_row3_col16\" class=\"data row3 col16\" >279.1</td>\n",
       "      <td id=\"T_1de70_row3_col17\" class=\"data row3 col17\" >279.1</td>\n",
       "      <td id=\"T_1de70_row3_col18\" class=\"data row3 col18\" >279.1</td>\n",
       "      <td id=\"T_1de70_row3_col19\" class=\"data row3 col19\" >527.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_1de70_level0_row4\" class=\"row_heading level0 row4\" >1e-09</th>\n",
       "      <td id=\"T_1de70_row4_col0\" class=\"data row4 col0\" >3.176e-07</td>\n",
       "      <td id=\"T_1de70_row4_col1\" class=\"data row4 col1\" >3.176e-07</td>\n",
       "      <td id=\"T_1de70_row4_col2\" class=\"data row4 col2\" >3.22e-07</td>\n",
       "      <td id=\"T_1de70_row4_col3\" class=\"data row4 col3\" >1.536e-05</td>\n",
       "      <td id=\"T_1de70_row4_col4\" class=\"data row4 col4\" >22</td>\n",
       "      <td id=\"T_1de70_row4_col5\" class=\"data row4 col5\" >22</td>\n",
       "      <td id=\"T_1de70_row4_col6\" class=\"data row4 col6\" >21</td>\n",
       "      <td id=\"T_1de70_row4_col7\" class=\"data row4 col7\" >47</td>\n",
       "      <td id=\"T_1de70_row4_col8\" class=\"data row4 col8\" >3.675e-07</td>\n",
       "      <td id=\"T_1de70_row4_col9\" class=\"data row4 col9\" >3.675e-07</td>\n",
       "      <td id=\"T_1de70_row4_col10\" class=\"data row4 col10\" >3.719e-07</td>\n",
       "      <td id=\"T_1de70_row4_col11\" class=\"data row4 col11\" >1.541e-05</td>\n",
       "      <td id=\"T_1de70_row4_col12\" class=\"data row4 col12\" >4.657e-08</td>\n",
       "      <td id=\"T_1de70_row4_col13\" class=\"data row4 col13\" >4.657e-08</td>\n",
       "      <td id=\"T_1de70_row4_col14\" class=\"data row4 col14\" >5.54e-08</td>\n",
       "      <td id=\"T_1de70_row4_col15\" class=\"data row4 col15\" >1.489e-05</td>\n",
       "      <td id=\"T_1de70_row4_col16\" class=\"data row4 col16\" >320.9</td>\n",
       "      <td id=\"T_1de70_row4_col17\" class=\"data row4 col17\" >320.9</td>\n",
       "      <td id=\"T_1de70_row4_col18\" class=\"data row4 col18\" >316.5</td>\n",
       "      <td id=\"T_1de70_row4_col19\" class=\"data row4 col19\" >527.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x27917e070b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pivot_dfs = {}\n",
    "for graph_name in g_rel_alpha_w_grid.keys():\n",
    "    for rel_alpha in g_rel_alpha_w_grid[graph_name]:\n",
    "        df_graph = df_results[(df_results['graph'] == graph_name) & (df_results['rel_alpha'] == rel_alpha)]\n",
    "        pivot_df = df_graph.pivot(index='w', columns=['algo'], values=['sub_opt_gap', 'num_clusters', 'total_loss', 'loss_U', 'loss_P'])\n",
    "        pivot_df.sort_index(axis=0, inplace=True, ascending=False)\n",
    "        pivot_df = pivot_df.reindex(columns=['First-Improv.', 'Best-Improv.', 'Optimal 1D', 'Soft Assign'], level=1)\n",
    "\n",
    "        pivot_dfs[graph_name] = pivot_dfs.get(graph_name, {})\n",
    "        pivot_dfs[graph_name][rel_alpha] = pivot_df\n",
    "\n",
    "        print(f\"Graph Name: {graph_name}, rel_alpha: {rel_alpha}, {str(graphs[graph_name])}\")\n",
    "        display(pivot_df.style.format(formatter=\"{:.4g}\").format_index(\"{:.3g}\", axis=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
